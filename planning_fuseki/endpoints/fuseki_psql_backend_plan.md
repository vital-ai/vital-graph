# Backend Implementation Plan
## Fuseki-PostgreSQL Hybrid Backend

### Overview
The Backend implementation provides the foundational dual-write storage system that combines Apache Fuseki (RDF triple store) with PostgreSQL (relational database) for optimal performance and reliability in the VitalGraph knowledge graph system.

### Implementation Status
- **Current Status**: âœ… CORE functionality complete with critical fixes applied
- **Priority**: Foundation for all endpoints
- **Recent Fixes**: Edge persistence, dual-write consistency - See `../fuseki_psql_sparql_plan.md` for SPARQL details

## Architecture

### Dual-Write System
```
Client Request â†’ Backend Coordinator â†’ PostgreSQL (Primary) + Fuseki (Query Index)
                                   â†“                      â†“
                              Transaction Log         SPARQL Queries
                                   â†“                      â†“
                              Consistency Check â† Query Results
```

### Core Components
- **PostgreSQL Primary Storage**: Authoritative data storage with ACID transactions
- **Fuseki Query Index**: Optimized RDF triple store for SPARQL queries
- **Dual-Write Coordinator**: Ensures consistency between both backends
- **SPARQL Parser**: See `../fuseki_psql_sparql_plan.md` for complete implementation
- **Transaction Manager**: Handles atomic operations across both systems
- **Authentication Manager**: Keycloak JWT authentication for production Fuseki instances

## Implementation Status

### Completed Core Features
- âœ… **Dual-Write Coordination**: Perfect validation with matching PostgreSQL/Fuseki counts
- âœ… **SPARQL Parser**: See `../fuseki_psql_sparql_plan.md` for complete details
- âœ… **Triple Persistence**: All 68 triples persist correctly (was only 22 before fix)
- âœ… **Edge Relationships**: Edge_hasEntityKGFrame objects with hasEdgeSource/hasEdgeDestination
- âœ… **Transaction Support**: Atomic operations across both backends
- âœ… **Connection Management**: PostgreSQL connection pooling and Fuseki HTTP client
- âœ… **Keycloak JWT Authentication**: Production-ready authentication for secured Fuseki instances

### Critical Fixes Implemented
- âœ… **SPARQL Parser**: See `../fuseki_psql_sparql_plan.md` for CompValue detection details
- âœ… **PostgreSQL Term Type Detection**: Proper URI vs Literal detection in quad storage
- âœ… **Admin Table Logic**: Changed from creation to verification-only during initialization
- âœ… **PostgreSQL Signal Manager**: Updated to use correct asyncpg API
- âœ… **Schema Compatibility**: Updated PostgreSQL schema with composite keys and dataset columns
- âœ… **Index Creation**: Added IF NOT EXISTS to prevent duplicate index errors

### Backend Storage Components

#### PostgreSQL Implementation
```python
# Core storage methods
async def store_quads_within_transaction(conn, space_id, quads)
async def delete_quads_within_transaction(conn, space_id, quads)
async def _get_or_create_term_uuid(conn, space_id, term_str, term_type)

# Term type detection (FIXED)
if validate_rfc3986(obj_str, rule='URI'):
    obj_type = 'U'  # URI
elif obj_str.startswith('_:'):
    obj_type = 'B'  # Blank node
else:
    obj_type = 'L'  # Literal
```

#### SPARQL Update Parser
See `../fuseki_psql_sparql_plan.md` for complete SPARQL parser implementation including CompValue operation detection.

#### Dual-Write Coordinator
```python
# SPARQL execution flow - See `../fuseki_psql_sparql_plan.md` for complete implementation
async def execute_sparql_update(space_id: str, sparql_update: str):
    # Complete implementation in SPARQL planning document
```

## Shared Backend Operations

### Quad Management
- **Quad Storage**: Efficient storage of RDF quads in both backends
- **Quad Retrieval**: Fast retrieval with proper indexing
- **Quad Deletion**: Atomic deletion across both systems
- **Batch Operations**: Optimized batch processing for large datasets

### SPARQL Operations
- **INSERT DATA**: Direct triple insertion with proper parsing
- **DELETE DATA**: Atomic triple deletion with validation
- **CONSTRUCT**: Complex query construction and execution
- **SELECT**: Optimized query execution with result formatting

### Transaction Management
- **ACID Compliance**: Full ACID transaction support in PostgreSQL
- **Consistency Validation**: Real-time validation between backends
- **Rollback Support**: Automatic rollback on consistency failures
- **Deadlock Prevention**: Proper locking and transaction ordering

## Authentication System

### Keycloak JWT Authentication
The backend now supports Keycloak JWT authentication for production Fuseki deployments that require secure access control.

#### Configuration
Authentication is configured in `vitalgraphdb-config.yaml`:

```yaml
fuseki_postgresql:
  fuseki:
    server_url: https://graphdb.example.com
    dataset_name: vitalgraph
    
    # Keycloak JWT Authentication
    enable_authentication: true  # Set to true to enable JWT authentication
    keycloak:
      url: https://keycloak.example.com
      realm: vitalgraph
      client_id: fuseki-client
      client_secret: ""  # Optional, only for confidential clients
      username: service-account-username
      password: service-account-password
```

#### Authentication Flow
1. **Token Acquisition**: On connection, obtain JWT token from Keycloak using OAuth 2.0 password grant
2. **Token Management**: Automatically refresh tokens before expiry (with 2-minute buffer)
3. **Request Authentication**: Include JWT token in Authorization header for all Fuseki requests
4. **Token Refresh**: Proactively refresh tokens to maintain uninterrupted service

#### Implementation Components

**FusekiAuthManager** (`fuseki_auth.py`):
- Manages JWT token lifecycle
- Handles token acquisition and refresh
- Provides authentication headers for requests
- Validates configuration on initialization

**FusekiDatasetManager** (updated):
- Detects `enable_authentication` flag in configuration
- Initializes `FusekiAuthManager` when authentication is enabled
- Falls back to basic authentication when disabled
- Automatically includes JWT headers in all SPARQL requests
- Implements `_get_request_headers()` helper for authenticated requests

**FusekiAdminDataset** (updated):
- Supports JWT authentication for admin dataset operations
- Initializes `FusekiAuthManager` when authentication is enabled
- Automatically includes JWT headers in admin SPARQL operations
- Implements `_get_request_headers()` helper for authenticated requests

#### Authentication Methods

```python
# Token acquisition
async def get_token(session: aiohttp.ClientSession) -> Optional[str]

# Token refresh
async def refresh_token_if_needed(session: aiohttp.ClientSession) -> bool

# Get authorization headers
def get_auth_headers() -> Dict[str, str]
```

#### Backward Compatibility
- **Basic Auth**: When `enable_authentication: false`, uses traditional username/password basic auth
- **No Breaking Changes**: Existing configurations continue to work without modification
- **Graceful Fallback**: If JWT authentication fails to initialize, falls back to basic auth with warning

#### Production Deployment
For production Fuseki instances with Keycloak authentication:
1. Set `enable_authentication: true` in configuration
2. Configure Keycloak connection parameters
3. Create service account with appropriate Fuseki access permissions
4. Test authentication using `test_scripts/auth/test_keycloak_fuseki_auth.py`

#### Security Considerations
- Service account credentials stored in configuration file (secure appropriately)
- JWT tokens expire after configured duration (typically 5 minutes)
- Automatic token refresh prevents service interruption
- All Fuseki communication uses HTTPS in production

## Configuration Management

### Environment-Based Configuration System

VitalGraph uses an environment-based configuration system that allows switching between local development and production configurations without code changes.

#### Configuration Files

Two configuration files are maintained (not in git):

**Local Development** - `vitalgraphdb-config-local.yaml`:
- Local PostgreSQL (host.docker.internal)
- Local Fuseki (http://host.docker.internal:3030)
- Basic authentication for Fuseki
- MinIO for file storage
- DEBUG log level

**Production** - `vitalgraphdb-config-production.yaml`:
- Production PostgreSQL endpoints
- Production Fuseki with HTTPS (https://graphdb.cardiffbank.co)
- Keycloak JWT authentication enabled
- AWS S3 for file storage
- INFO log level
- Production credentials and service accounts

#### Environment Variable Control

The `.env` file controls which configuration is used:

```bash
# Set to 'local' or 'production'
VITALGRAPH_ENVIRONMENT=local
```

#### Docker Build Process

**Dockerfile** (lines 54-58):
```dockerfile
# Copy environment-specific configuration file (placed last to avoid cache invalidation)
ARG VITALGRAPH_ENVIRONMENT=local
RUN mkdir -p /app/vitalgraphdb_config
COPY vitalgraphdb_config/vitalgraphdb-config-${VITALGRAPH_ENVIRONMENT}.yaml /app/vitalgraphdb_config/vitalgraphdb-config.yaml
```

**docker-compose.yml**:
```yaml
services:
  vitalgraph:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - VITALGRAPH_ENVIRONMENT=${VITALGRAPH_ENVIRONMENT:-local}
```

#### Switching Environments

**For Local Development:**
```bash
# Set environment to local
echo "VITALGRAPH_ENVIRONMENT=local" >> .env

# Rebuild Docker image
docker-compose build

# Start services
docker-compose up
```

**For Production Deployment:**
```bash
# Set environment to production
echo "VITALGRAPH_ENVIRONMENT=production" >> .env

# Rebuild Docker image with production config
docker-compose build

# Deploy to production
docker-compose up -d
```

#### Cache Optimization

The configuration file copy is placed **last** in the Dockerfile to:
- Avoid invalidating Docker build cache when switching environments
- Allow fast rebuilds when only configuration changes
- Minimize rebuild time for environment switches

#### Production Initialization

**Initialization Script:** `scripts/init_vitalgraph_fuseki_admin.py`

The initialization script supports explicit config file paths for production setup:

```bash
# Initialize production environment with explicit config
python scripts/init_vitalgraph_fuseki_admin.py \
    --config vitalgraphdb_config/vitalgraphdb-config-production.yaml

# Skip test space creation for production
python scripts/init_vitalgraph_fuseki_admin.py \
    --config vitalgraphdb_config/vitalgraphdb-config-production.yaml \
    --skip-test-space

# Validate existing production setup
python scripts/init_vitalgraph_fuseki_admin.py \
    --config vitalgraphdb_config/vitalgraphdb-config-production.yaml \
    --validate-only
```

**Config File Search Order:**
1. `vitalgraphdb-config-local.yaml`
2. `vitalgraphdb-config-production.yaml`
3. `vitalgraphdb-config-fuseki-postgresql.yaml`
4. `vitalgraphdb-config.yaml`

**Initialization Steps:**
1. Validate connectivity to Fuseki and PostgreSQL
2. Initialize PostgreSQL admin schema (install, space, graph, user tables)
3. Create Fuseki admin dataset (`vitalgraph_admin`) with RDF schema
4. Create test space (optional, skip with `--skip-test-space`)
5. Validate complete setup

**Production Workflow:**
```bash
# 1. Initialize production backend
python scripts/init_vitalgraph_fuseki_admin.py \
    --config vitalgraphdb_config/vitalgraphdb-config-production.yaml \
    --skip-test-space

# 2. Set environment to production
echo "VITALGRAPH_ENVIRONMENT=production" >> .env

# 3. Build Docker image with production config
docker-compose build

# 4. Deploy to production
docker-compose up -d
```

#### Future Enhancements

**Environment Variable Override** (planned):
- Move sensitive values to environment variables
- Use AWS Secrets Manager for production credentials
- Support AWS ECS Task Definition environment variables
- Override config file values with env vars at runtime

**Benefits:**
- No credentials in configuration files
- Dynamic configuration via AWS Task Definitions
- Secrets rotation without rebuilding images
- Better security posture for production deployments

## PostgreSQL Database Schema

### Admin Tables
The admin tables manage spaces, graphs, users, and installation metadata following the existing PostgreSQL backend pattern.

#### Install Table
```sql
CREATE TABLE install (
    install_id SERIAL PRIMARY KEY,
    install_datetime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    update_datetime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    active BOOLEAN DEFAULT TRUE
);
```

#### Space Table
```sql
CREATE TABLE space (
    space_id VARCHAR(255) PRIMARY KEY,
    space_name VARCHAR(255) NOT NULL,
    space_description TEXT,
    tenant VARCHAR(255),
    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### Graph Table
```sql
CREATE TABLE graph (
    graph_id VARCHAR(255) PRIMARY KEY,
    space_id VARCHAR(255) REFERENCES space(space_id),
    graph_name VARCHAR(255) NOT NULL,
    graph_description TEXT,
    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Per-Space Data Tables
Each space gets dedicated term and quad tables for RDF storage.

#### Term Table (per space)
```sql
CREATE TABLE {space_id}_term (
    term_uuid UUID PRIMARY KEY,
    term_string TEXT NOT NULL,
    term_type CHAR(1) NOT NULL, -- 'U' for URI, 'L' for Literal, 'B' for Blank
    datatype_uuid UUID,
    language_tag VARCHAR(10)
);

CREATE INDEX idx_{space_id}_term_string ON {space_id}_term(term_string);
CREATE INDEX idx_{space_id}_term_type ON {space_id}_term(term_type);
```

#### RDF Quad Table (per space)
```sql
CREATE TABLE {space_id}_rdf_quad (
    quad_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    subject_uuid UUID NOT NULL REFERENCES {space_id}_term(term_uuid),
    predicate_uuid UUID NOT NULL REFERENCES {space_id}_term(term_uuid),
    object_uuid UUID NOT NULL REFERENCES {space_id}_term(term_uuid),
    graph_uuid UUID REFERENCES {space_id}_term(term_uuid),
    dataset VARCHAR(255) DEFAULT 'default'
);

CREATE INDEX idx_{space_id}_quad_spo ON {space_id}_rdf_quad(subject_uuid, predicate_uuid, object_uuid);
CREATE INDEX idx_{space_id}_quad_pos ON {space_id}_rdf_quad(predicate_uuid, object_uuid, subject_uuid);
CREATE INDEX idx_{space_id}_quad_osp ON {space_id}_rdf_quad(object_uuid, subject_uuid, predicate_uuid);
CREATE INDEX idx_{space_id}_quad_graph ON {space_id}_rdf_quad(graph_uuid);
```

## Dual-Write Coordination Architecture

### DualWriteCoordinator
The `DualWriteCoordinator` manages transaction-safe operations across both PostgreSQL and Fuseki backends.

```python
class DualWriteCoordinator:
    # Complete SPARQL execution implementation - See `../fuseki_psql_sparql_plan.md`
    async def execute_sparql_update(self, space_id: str, sparql_update: str):
        # Full implementation details in SPARQL planning document
        pass
```

### Transaction Management

#### PostgreSQL Transaction Handling
```python
class PostgreSQLDbImpl:
    async def begin_transaction(self):
        conn = await self.get_connection()
        await conn.execute("BEGIN")
        return conn
    
    async def commit_transaction(self, transaction):
        await transaction.execute("COMMIT")
        await transaction.close()
    
    async def rollback_transaction(self, transaction):
        await transaction.execute("ROLLBACK")
        await transaction.close()
```

#### Quad-Based Transaction Management
All VitalSigns objects are converted to RDF quads for data updates. Transaction management for quads is handled in both backends:

- **Process**: Modifications made to PostgreSQL first, then to Fuseki on success
- **Transaction Coverage**: Quad-based transactions implemented in both backends
- **Error Handling**: When Fuseki fails, system will retry, invalidate index, or potentially rollback PostgreSQL changes

## Quad Storage Operations

### Store Quads to PostgreSQL
```python
async def store_quads_within_transaction(self, conn, space_id: str, quads: List[Quad]):
    term_table = f"{space_id}_term"
    quad_table = f"{space_id}_rdf_quad"
    
    # Process each quad
    for quad in quads:
        # Get or create term UUIDs
        subject_uuid = await self._get_or_create_term_uuid(
            conn, space_id, str(quad.subject), self._detect_term_type(quad.subject)
        )
        predicate_uuid = await self._get_or_create_term_uuid(
            conn, space_id, str(quad.predicate), 'U'  # Predicates are always URIs
        )
        object_uuid = await self._get_or_create_term_uuid(
            conn, space_id, str(quad.object), self._detect_term_type(quad.object)
        )
        
        graph_uuid = None
        if quad.graph:
            graph_uuid = await self._get_or_create_term_uuid(
                conn, space_id, str(quad.graph), 'U'
            )
        
        # Insert quad
        await conn.execute(
            f"INSERT INTO {quad_table} (subject_uuid, predicate_uuid, object_uuid, graph_uuid) VALUES ($1, $2, $3, $4)",
            subject_uuid, predicate_uuid, object_uuid, graph_uuid
        )
```

### Term Type Detection
```python
def _detect_term_type(self, term) -> str:
    """Detect RDF term type for PostgreSQL storage."""
    term_str = str(term)
    
    if validate_rfc3986(term_str, rule='URI'):
        return 'U'  # URI
    elif term_str.startswith('_:'):
        return 'B'  # Blank node
    else:
        return 'L'  # Literal
```

### Delete Quads from PostgreSQL
```python
async def delete_quads_within_transaction(self, conn, space_id: str, quads: List[Quad]):
    quad_table = f"{space_id}_rdf_quad"
    
    for quad in quads:
        # Resolve term UUIDs
        subject_uuid = await self._get_term_uuid(conn, space_id, str(quad.subject))
        predicate_uuid = await self._get_term_uuid(conn, space_id, str(quad.predicate))
        object_uuid = await self._get_term_uuid(conn, space_id, str(quad.object))
        
        # Delete matching quads
        await conn.execute(
            f"DELETE FROM {quad_table} WHERE subject_uuid = $1 AND predicate_uuid = $2 AND object_uuid = $3",
            subject_uuid, predicate_uuid, object_uuid
        )
```

## SPARQL Integration

For complete SPARQL implementation details including:
- CompValue operation detection
- Triple extraction from INSERT/DELETE operations  
- DELETE WHERE conversion
- RDFLib integration
- Error handling and logging

See `../fuseki_psql_sparql_plan.md`

## Fuseki Integration

### Dataset Management
```python
class FusekiDatasetManager:
    async def create_dataset(self, space_id: str):
        dataset_name = f"vitalgraph_space_{space_id}"
        
        # Create dataset via Fuseki Admin API
        response = await self.http_client.post(
            f"{self.fuseki_admin_url}/$/datasets",
            json={
                "dbName": dataset_name,
                "dbType": "tdb2"
            }
        )
        
        return response.status == 200
    
    async def delete_dataset(self, space_id: str):
        dataset_name = f"vitalgraph_space_{space_id}"
        
        response = await self.http_client.delete(
            f"{self.fuseki_admin_url}/$/datasets/{dataset_name}"
        )
        
        return response.status == 200
```

### SPARQL Execution
```python
# Fuseki SPARQL execution - See `../fuseki_psql_sparql_plan.md` for complete implementation
async def execute_sparql_update_on_fuseki(self, space_id: str, sparql_update: str):
    # Complete implementation in SPARQL planning document
    pass
```

## Consistency Validation

### Dual-Write Consistency Checks
```python
async def _validate_dual_write_consistency(self, space_id: str, operation):
    """Validate that PostgreSQL and Fuseki have consistent data."""
    
    # Count triples in PostgreSQL
    pg_count = await self._count_postgresql_quads(space_id)
    
    # Count triples in Fuseki
    fuseki_count = await self._count_fuseki_triples(space_id)
    
    if pg_count != fuseki_count:
        logger.error(f"Consistency validation failed: PostgreSQL={pg_count}, Fuseki={fuseki_count}")
        raise ConsistencyError(f"Backend consistency validation failed for space {space_id}")
    
    logger.info(f"âœ… Consistency validated: {pg_count} triples in both backends")
```

## Remaining Work

### High Priority Maintenance
- ðŸ”„ **Code Review for Duplicate Function Definitions**: Review codebase for duplicate function definitions that may be causing parsing or execution problems
- ðŸ”„ **RDFLib Parsing Failure Logging**: See `../fuseki_psql_sparql_plan.md` for logging implementation

### Performance Optimization
- ðŸ”„ **Query Optimization**: Advanced query planning and optimization
- ðŸ”„ **Connection Pooling**: Optimize connection pool sizing and management
- ðŸ”„ **Caching**: Implement intelligent caching for frequently accessed data
- ðŸ”„ **Index Optimization**: Fine-tune indexes for common query patterns

### Monitoring and Observability
- ðŸ”„ **Performance Metrics**: Comprehensive performance monitoring
- ðŸ”„ **Error Tracking**: Advanced error tracking and alerting
- ðŸ”„ **Consistency Monitoring**: Real-time consistency validation
- ðŸ”„ **Resource Usage**: Memory and CPU usage optimization

## Backend Integration Points

### Endpoint Integration
- **Graphs Endpoint**: Basic graph operations and metadata
- **KGEntities Endpoint**: Complex entity-frame-slot operations
- **KGTypes Endpoint**: Type management and validation
- **KGFrames Endpoint**: Frame operations and hierarchical structures
- **KGRelations Endpoint**: Relationship management and traversal
- **KGQueries Endpoint**: Advanced SPARQL query execution

### Cross-Backend Operations
- **Data Migration**: Tools for migrating data between backends
- **Backup and Recovery**: Comprehensive backup strategies
- **Schema Evolution**: Support for schema changes and migrations
- **Performance Tuning**: Backend-specific optimization techniques

## Connection Management

### PostgreSQL Connection Pooling
```python
class PostgreSQLConnectionManager:
    def __init__(self, connection_string: str, pool_size: int = 10):
        self.connection_string = connection_string
        self.pool_size = pool_size
        self.pool = None
    
    async def initialize_pool(self):
        self.pool = await asyncpg.create_pool(
            self.connection_string,
            min_size=1,
            max_size=self.pool_size
        )
    
    async def get_connection(self):
        return await self.pool.acquire()
    
    async def release_connection(self, conn):
        await self.pool.release(conn)
```

### Row Factory Configuration
Critical for dictionary-style row access in PostgreSQL queries:

```python
async def get_db_connection(self):
    conn = await self.pool.acquire()
    conn.row_factory = psycopg.rows.dict_row  # Essential for dict access
    return conn
```

## Performance Optimization

### Index Strategy
- **SPO Index**: Subject-Predicate-Object for standard RDF queries
- **POS Index**: Predicate-Object-Subject for property-based queries  
- **OSP Index**: Object-Subject-Predicate for value-based queries
- **Graph Index**: Graph-based filtering for named graph operations
- **Grouping URI Indexes**: Critical for hasKGGraphURI and hasFrameGraphURI queries

### Batch Operations
```python
async def store_quads_batch(self, space_id: str, quads: List[Quad], batch_size: int = 1000):
    """Store quads in batches for optimal performance."""
    
    for i in range(0, len(quads), batch_size):
        batch = quads[i:i + batch_size]
        
        async with self.get_transaction() as transaction:
            await self.store_quads_within_transaction(transaction, space_id, batch)
```

## Error Handling and Recovery

### Transaction Rollback
```python
async def execute_with_rollback(self, space_id: str, operation):
    pg_transaction = None
    try:
        pg_transaction = await self.postgresql_impl.begin_transaction()
        
        # Execute PostgreSQL operations
        await operation(pg_transaction)
        
        # Commit PostgreSQL
        await self.postgresql_impl.commit_transaction(pg_transaction)
        
        # Execute Fuseki operations
        await self._execute_fuseki_operations(space_id, operation)
        
    except Exception as e:
        if pg_transaction:
            await self.postgresql_impl.rollback_transaction(pg_transaction)
        
        logger.error(f"Operation failed, rolled back: {e}")
        raise
```

### Consistency Recovery
```python
async def recover_consistency(self, space_id: str):
    """Recover consistency by rebuilding Fuseki from PostgreSQL."""
    
    # Clear Fuseki dataset
    await self.fuseki_manager.clear_dataset(space_id)
    
    # Rebuild from PostgreSQL
    quads = await self.postgresql_impl.get_all_quads(space_id)
    
    # Batch insert to Fuseki
    sparql_insert = self._build_insert_data_query(quads)
    await self.fuseki_manager.execute_update(space_id, sparql_insert)
    
    logger.info(f"Consistency recovered for space {space_id}: {len(quads)} quads")
```

## Test Coverage

### Primary Test File
**Test Script**: `/test_scripts/fuseki_postgresql/test_fuseki_postgresql_backend_complete.py`

**Test Description**: Complete backend integration test for Fuseki+PostgreSQL hybrid backend with comprehensive CRUD operations and dual-write validation.

### Additional Backend Test Files
**Atomic Quad Update Test**: `/test_scripts/fuseki_postgresql/test_update_quads.py`

**Test Description**: Comprehensive test for atomic `update_quads` functionality focusing on PostgreSQL transaction management, Fuseki synchronization, and atomic operation validation.

**Test Coverage**:
- **Basic Atomic Updates**: Simple quad replacement operations
- **Edge Cases**: Empty delete/insert sets handling
- **Performance Testing**: Large quad sets (1000+ quads) performance validation
- **Concurrency Testing**: Concurrent update operations and race condition detection
- **Transaction Management**: PostgreSQL transaction rollback on failure
- **Dual-Write Consistency**: Fuseki synchronization failure handling
- **Resource Management**: Aggressive cleanup and resource management testing

**Key Features Tested**:
- Atomic quad replacement with DELETE/INSERT operations
- PostgreSQL transaction atomicity and rollback behavior
- Fuseki synchronization and dual-write consistency
- Performance characteristics with large datasets
- Concurrent operation safety and isolation
- Error handling and recovery mechanisms

## Success Criteria
- âœ… Dual-write consistency maintained across all operations
- âœ… SPARQL parser - See `../fuseki_psql_sparql_plan.md` for complete details
- âœ… Transaction integrity preserved in all scenarios
- âœ… Performance meets production requirements
- âœ… PostgreSQL schema optimized for RDF quad storage
- âœ… Fuseki dataset management working correctly
- ðŸ”„ Comprehensive monitoring and alerting implemented
- ðŸ”„ Code quality maintained with no duplicate functions

## Dependencies and Integration

### External Dependencies
- **PostgreSQL 17.5**: Primary relational database
- **Apache Fuseki**: RDF triple store for SPARQL queries
- **RDFLib**: See `../fuseki_psql_sparql_plan.md` for SPARQL parsing details
- **asyncpg**: Asynchronous PostgreSQL driver
- **aiohttp**: Asynchronous HTTP client for Fuseki

### Integration Requirements
- **VitalSigns**: Native JSON-LD object conversion
- **Pydantic**: Response model validation
- **FastAPI**: REST API framework integration
- **Logging**: Comprehensive logging and monitoring

## Notes
- Backend stability is critical for all endpoint operations
- Dual-write consistency ensures data reliability and performance

## Common Object Query Utilities

### Fuseki SPARQL Query Utilities
```python
# NEW FILE: /vitalgraph/db/fuseki_postgresql/fuseki_query_utils.py
class FusekiQueryUtils:
    """Common utilities for Fuseki SPARQL queries used across all object endpoints."""
    
    @staticmethod
    async def find_subject_uris_by_criteria(fuseki_manager, space_id, graph_id, 
                                          filters=None, page_size=100, offset=0):
        """
        Phase 1: Execute SPARQL SELECT to find subject URIs matching criteria.
        
        Args:
            fuseki_manager: FusekiDatasetManager instance
            space_id: Space identifier
            graph_id: Graph identifier  
            filters: Dict with vitaltype_filter, search_text, subject_uri, etc.
            page_size: Results per page
            offset: Pagination offset
            
        Returns:
            Tuple of (subject_uris: List[str], total_count: int)
        """
        graph_uri = f"http://vital.ai/graph/{space_id}" if graph_id == "main" else f"http://vital.ai/graph/{space_id}/{graph_id}"
        
        # Build filter conditions
        filter_conditions = []
        if filters:
            if filters.get('vitaltype_filter'):
                filter_conditions.append(f"?subject a <{filters['vitaltype_filter']}> .")
            if filters.get('search_text'):
                filter_conditions.append(f"""
                    ?subject ?searchProp ?searchValue .
                    FILTER(CONTAINS(LCASE(STR(?searchValue)), LCASE("{filters['search_text']}")))
                """)
            if filters.get('subject_uri'):
                filter_conditions.append(f"?subject = <{filters['subject_uri']}> .")
        
        filter_clause = "\n            ".join(filter_conditions) if filter_conditions else "?subject a ?type ."
        
        sparql_query = f"""
        SELECT DISTINCT ?subject (COUNT(*) OVER() AS ?total) WHERE {{
            GRAPH <{graph_uri}> {{
                {filter_clause}
            }}
        }}
        ORDER BY ?subject
        LIMIT {page_size} OFFSET {offset}
        """
        
        bindings = await fuseki_manager.query_dataset(space_id, sparql_query)
        subject_uris = [binding['subject']['value'] for binding in bindings]
        total_count = int(bindings[0]['total']['value']) if bindings else 0
        
        return subject_uris, total_count
    
    @staticmethod
    async def get_complete_objects_by_uris(fuseki_manager, space_id, subject_uris, graph_id):
        """
        Phase 2: Execute SPARQL CONSTRUCT to get all triples for specific subject URIs.
        
        Args:
            fuseki_manager: FusekiDatasetManager instance
            space_id: Space identifier
            subject_uris: List of subject URIs to retrieve
            graph_id: Graph identifier
            
        Returns:
            List of (subject, predicate, object) triples
        """
        if not subject_uris:
            return []
            
        graph_uri = f"http://vital.ai/graph/{space_id}" if graph_id == "main" else f"http://vital.ai/graph/{space_id}/{graph_id}"
        
        # Format URIs for VALUES clause
        uri_values = " ".join([f"<{uri}>" for uri in subject_uris])
        
        sparql_query = f"""
        CONSTRUCT {{ ?s ?p ?o }}
        WHERE {{
            GRAPH <{graph_uri}> {{
                ?s ?p ?o .
                VALUES ?s {{ {uri_values} }}
            }}
        }}
        """
        
        # Execute CONSTRUCT query to get triples
        triples = await fuseki_manager.construct_dataset(space_id, sparql_query)
        return triples
    
    @staticmethod
    async def get_objects_batch_processing(fuseki_manager, space_id, subject_uris, graph_id, batch_size=100):
        """
        Process large URI lists in batches to avoid SPARQL query size limits.
        
        Args:
            fuseki_manager: FusekiDatasetManager instance
            space_id: Space identifier
            subject_uris: List of all subject URIs to retrieve
            graph_id: Graph identifier
            batch_size: Number of URIs per batch (default: 100)
            
        Returns:
            List of all triples for the subject URIs
        """
        all_triples = []
        
        for i in range(0, len(subject_uris), batch_size):
            batch_uris = subject_uris[i:i + batch_size]
            batch_triples = await FusekiQueryUtils.get_complete_objects_by_uris(
                fuseki_manager, space_id, batch_uris, graph_id
            )
            all_triples.extend(batch_triples)
        
        return all_triples
    
    @staticmethod
    async def convert_triples_to_jsonld(triples_list, return_format='document'):
        """
        Convert SPARQL triples to VitalSigns objects then to JSON-LD.
        
        Args:
            triples_list: List of (subject, predicate, object) triples
            return_format: 'document' for multiple objects, 'object' for single
            
        Returns:
            JSON-LD document or object
        """
        if not triples_list:
            from vital_ai_vitalsigns.vitalsigns import VitalSigns
            vitalsigns = VitalSigns()
            return vitalsigns.to_jsonld_list([])
        
        # Convert triples to VitalSigns GraphObjects
        from vital_ai_vitalsigns.vitalsigns import VitalSigns
        vitalsigns = VitalSigns()
        graph_objects = vitalsigns.from_triples_list(triples_list)
        
        # Convert to JSON-LD format based on object count and requested format
        if return_format == 'object' and len(graph_objects) == 1:
            return graph_objects[0].to_jsonld()  # Single JSON-LD object
        else:
            return vitalsigns.to_jsonld_list(graph_objects)  # JSON-LD document with @graph
```

### FusekiPostgreSQLDbObjects Implementation
```python
# IMPLEMENTATION: /vitalgraph/db/fuseki_postgresql/fuseki_postgresql_db_objects.py
class FusekiPostgreSQLDbObjects:
    """Database objects layer implementing the two-phase query pattern."""
    
    def __init__(self, space_impl):
        self.space_impl = space_impl
        self.fuseki_manager = space_impl.fuseki_manager
        self.logger = logging.getLogger(f"{__name__}.FusekiPostgreSQLDbObjects")
    
    async def list_objects(self, space_id, graph_id, page_size=100, offset=0, filters=None):
        """
        List objects using two-phase query: find URIs then get complete objects.
        """
        # Phase 1: Find subject URIs matching criteria
        subject_uris, total_count = await FusekiQueryUtils.find_subject_uris_by_criteria(
            self.fuseki_manager, space_id, graph_id, filters, page_size, offset
        )
        
        if not subject_uris:
            return [], total_count
        
        # Phase 2: Get complete objects for found URIs (with batching)
        triples = await FusekiQueryUtils.get_objects_batch_processing(
            self.fuseki_manager, space_id, subject_uris, graph_id
        )
        
        # Phase 3: Convert to VitalSigns GraphObjects
        from vital_ai_vitalsigns.vitalsigns import VitalSigns
        vitalsigns = VitalSigns()
        graph_objects = vitalsigns.from_triples_list(triples)
        
        return graph_objects, total_count
    
    async def get_objects_by_uris(self, space_id, uris, graph_id):
        """Get multiple objects by URI list using batch processing."""
        triples = await FusekiQueryUtils.get_objects_batch_processing(
            self.fuseki_manager, space_id, uris, graph_id
        )
        
        from vital_ai_vitalsigns.vitalsigns import VitalSigns
        vitalsigns = VitalSigns()
        return vitalsigns.from_triples_list(triples)
    
    async def get_objects_by_uris_batch(self, space_id, subject_uris, graph_id):
        """Get objects as raw triples (used by object implementation pattern)."""
        triples = await FusekiQueryUtils.get_objects_batch_processing(
            self.fuseki_manager, space_id, subject_uris, graph_id
        )
        
        # Convert to quad format expected by existing code
        graph_uri = f"http://vital.ai/graph/{space_id}" if graph_id == "main" else f"http://vital.ai/graph/{space_id}/{graph_id}"
        quads = [(s, p, o, graph_uri) for s, p, o in triples]
        return quads
    
    async def get_existing_object_uris(self, space_id, uris):
        """Check which URIs exist by querying for any triple with those subjects."""
        if not uris:
            return []
        
        # Use simple ASK queries to check existence
        existing_uris = []
        for uri in uris:
            sparql_query = f"""
            ASK {{
                GRAPH ?g {{
                    <{uri}> ?p ?o .
                }}
            }}
            """
            exists = await self.fuseki_manager.ask_dataset(space_id, sparql_query)
            if exists:
                existing_uris.append(uri)
        
        return existing_uris
```

### Query Routing Architecture
- **Read Operations** (list, get): 
  - Phase 1 queries route to Fuseki for fast subject URI discovery
  - Phase 2 queries route to Fuseki for complete object retrieval
  - Batch processing for large result sets (100 objects per batch)
- **Write Operations** (create, update, delete): Use dual-write coordinator
- **Consistency Validation**: Cross-check between Fuseki and PostgreSQL

## Hybrid Backend Architecture Implementation

### FusekiPostgreSQLSpaceImpl
```python
# vitalgraph/db/fuseki_postgresql/fuseki_postgresql_space_impl.py
class FusekiPostgreSQLSpaceImpl(SpaceBackendInterface):
    """
    Hybrid backend implementation combining Fuseki and PostgreSQL.
    
    Architecture:
    - Fuseki: Primary for SPARQL queries and graph operations
    - PostgreSQL: Secondary for relational queries and consistency validation
    - Dual-write: All updates go to both systems atomically
    """
    
    def __init__(self, fuseki_config: Dict, postgresql_config: Dict):
        self.logger = logging.getLogger(f"{__name__}.FusekiPostgreSQLSpaceImpl")
        
        # Fuseki components
        self.fuseki_manager = FusekiDatasetManager(fuseki_config)
        
        # PostgreSQL components (no SQLAlchemy)
        self.postgresql_impl = FusekiPostgreSQLDbImpl(postgresql_config)
        self.pg_schema = FusekiPostgreSQLSchema()
        
        # Signal manager (PostgreSQL-based)
        self.signal_manager = PostgreSQLSignalManager(postgresql_config)
        
        # SPARQL UPDATE parser
        self.sparql_parser = SPARQLUpdateParser(self.fuseki_manager)
        
        # Dual-write coordinator
        self.dual_write_coordinator = DualWriteCoordinator(
            self.fuseki_manager, self.postgresql_impl, self.sparql_parser
        )
    
    async def execute_sparql_update(self, space_id: str, sparql_update: str) -> bool:
        """
        Execute SPARQL UPDATE with dual-write to both Fuseki and PostgreSQL.
        
        This is the main entry point for graph updates in the hybrid backend.
        Parses the SPARQL UPDATE to determine affected triples, then coordinates
        the dual-write operation.
        """
        try:
            # Parse SPARQL UPDATE to determine affected triples
            parsed_operation = await self.sparql_parser.parse_update_operation(
                space_id, sparql_update
            )
            
            # Execute dual-write operation
            return await self.dual_write_coordinator.execute_parsed_update(
                space_id, parsed_operation
            )
            
        except Exception as e:
            self.logger.error(f"Error executing SPARQL UPDATE: {e}")
            return False
    
    # Legacy quad-based methods (for compatibility)
    async def add_quads(self, space_id: str, quads: List[Quad]) -> bool:
        """Legacy method: Convert quads to INSERT and execute via SPARQL UPDATE."""
        insert_query = self._quads_to_insert_sparql(quads)
        return await self.execute_sparql_update(space_id, insert_query)
    
    async def remove_quads(self, space_id: str, quads: List[Quad]) -> bool:
        """Legacy method: Convert quads to DELETE and execute via SPARQL UPDATE."""
        delete_query = self._quads_to_delete_sparql(quads)
        return await self.execute_sparql_update(space_id, delete_query)
    
    async def query_quads(self, space_id: str, query: str) -> List[Dict]:
        """Primary read: Query Fuseki dataset directly."""
        return await self.fuseki_manager.query_dataset(
            f"vitalgraph_space_{space_id}", query
        )
    
    # Recovery methods
    async def rebuild_fuseki_from_postgresql(self, space_id: str) -> bool:
        """Rebuild Fuseki dataset from PostgreSQL primary data."""
        # Get all quads from PostgreSQL
        primary_quads = await self.postgresql_impl.get_all_quads(space_id)
        
        # Recreate Fuseki dataset
        await self.fuseki_manager.delete_dataset(f"vitalgraph_space_{space_id}")
        await self.fuseki_manager.create_dataset(f"vitalgraph_space_{space_id}")
        
        # Restore all quads to Fuseki
        return await self.fuseki_manager.add_quads_to_dataset(
            f"vitalgraph_space_{space_id}", primary_quads
        )
```

### Updated Dual-Write Coordinator with SPARQL Parsing
```python
# vitalgraph/db/fuseki_postgresql/dual_write_coordinator.py
class DualWriteCoordinator:
    """Coordinates dual-write operations between Fuseki and PostgreSQL."""
    
    def __init__(self, fuseki_manager, postgresql_impl, sparql_parser):
        self.fuseki_manager = fuseki_manager
        self.postgresql_impl = postgresql_impl
        self.sparql_parser = sparql_parser
    
    async def execute_parsed_update(self, space_id: str, parsed_operation: Dict) -> bool:
        """
        Execute dual-write operation from parsed SPARQL UPDATE.
        
        Args:
            space_id: Target space
            parsed_operation: Result from SPARQLUpdateParser.parse_update_operation()
                Contains: operation_type, insert_triples, delete_triples, raw_update
        """
        operation_type = parsed_operation['operation_type']
        
        try:
            # Begin transaction on PostgreSQL
            pg_transaction = await self.postgresql_impl.begin_transaction()
            
            # Execute on PostgreSQL first (primary)
            if operation_type == 'INSERT':
                pg_success = await self.postgresql_impl.add_triples(
                    space_id, parsed_operation['insert_triples'], pg_transaction
                )
            elif operation_type == 'DELETE':
                pg_success = await self.postgresql_impl.remove_triples(
                    space_id, parsed_operation['delete_triples'], pg_transaction
                )
            elif operation_type == 'INSERT_DELETE':
                # Handle DELETE WHERE operations
                pg_success = await self.postgresql_impl.update_triples(
                    space_id, 
                    parsed_operation['delete_triples'],
                    parsed_operation['insert_triples'],
                    pg_transaction
                )
            
            if not pg_success:
                await self.postgresql_impl.rollback_transaction(pg_transaction)
                return False
            
            # Execute on Fuseki (secondary)
            fuseki_success = await self.fuseki_manager.execute_update(
                f"vitalgraph_space_{space_id}", parsed_operation['raw_update']
            )
            
            if fuseki_success:
                # Both succeeded - commit PostgreSQL transaction
                await self.postgresql_impl.commit_transaction(pg_transaction)
                return True
            else:
                # Fuseki failed - rollback PostgreSQL
                await self.postgresql_impl.rollback_transaction(pg_transaction)
                return False
                
        except Exception as e:
            # Error occurred - rollback PostgreSQL transaction
            await self.postgresql_impl.rollback_transaction(pg_transaction)
            self.logger.error(f"Dual-write operation failed: {e}")
            return False
```

## PostgreSQL Signal Manager Implementation

### Enhanced Signal Management
```python
# vitalgraph/db/fuseki_postgresql/postgresql_signal_manager.py
class PostgreSQLSignalManager(SignalManagerInterface):
    """
    PostgreSQL-based signal implementation using NOTIFY/LISTEN.
    Provides significant enhancement over Fuseki's no-op signal manager.
    """
    
    def __init__(self, postgresql_config: dict):
        self.pg_connection = PostgreSQLConnection(postgresql_config)
        self.listeners = {}
    
    async def emit_signal(self, signal_type: str, data: dict) -> bool:
        """Emit signal using PostgreSQL NOTIFY."""
        payload = json.dumps(data)
        await self.pg_connection.execute(
            f"NOTIFY {signal_type}, '{payload}'"
        )
        return True
    
    async def listen_for_signals(self, signal_type: str, callback) -> bool:
        """Listen for signals using PostgreSQL LISTEN."""
        await self.pg_connection.execute(f"LISTEN {signal_type}")
        self.listeners[signal_type] = callback
        return True
```

## Package Structure Implementation

### Fuseki-PostgreSQL Package Structure
```
vitalgraph/db/fuseki_postgresql/
â”œâ”€â”€ __init__.py                           # Package initialization
â”œâ”€â”€ fuseki_postgresql_space_impl.py      # Main hybrid implementation
â”œâ”€â”€ postgresql_db_impl.py                # DbImplInterface implementation
â”œâ”€â”€ postgresql_schema.py                 # Schema definitions and migrations
â”œâ”€â”€ postgresql_signal_manager.py         # PostgreSQL-based signals
â”œâ”€â”€ fuseki_dataset_manager.py           # Fuseki dataset operations
â”œâ”€â”€ dual_write_coordinator.py           # Synchronization logic
â””â”€â”€ backup_recovery_manager.py          # Disaster recovery operations
```

## Testing Strategy for Hybrid Backend

### Comprehensive Backend Testing
```python
# test_fuseki_postgresql_backend.py
class TestFusekiPostgreSQLBackend:
    """Test hybrid backend implementation independently."""
    
    async def test_dual_write_consistency(self):
        """Verify data consistency between Fuseki and PostgreSQL."""
        
    async def test_disaster_recovery(self):
        """Test rebuilding Fuseki from PostgreSQL backup."""
        
    async def test_performance_comparison(self):
        """Compare performance vs pure Fuseki and pure PostgreSQL."""
        
    async def test_signal_integration(self):
        """Test PostgreSQL-based signal notifications."""
        
    async def test_concurrent_operations(self):
        """Test concurrent reads/writes across both systems."""
```

## PyOxigraph In-Memory Testing Framework

### SPARQL UPDATE Testing Framework
```python
# vitalgraph/db/fuseki_postgresql/test_sparql_update_parser.py
import pyoxigraph
from typing import Dict, List, Any
import pytest
from .sparql_update_parser import SPARQLUpdateParser

class SPARQLUpdateTestFramework:
    """
    Comprehensive testing framework for SPARQL UPDATE operations using pyoxigraph.
    
    Uses in-memory RDF store to:
    1. Insert test data
    2. Execute SPARQL UPDATE operations
    3. Validate before/after triple states
    4. Test all SPARQL UPDATE operation types
    """
    
    def __init__(self):
        self.store = None
        self.parser = None
        
    def setup_test_store(self) -> pyoxigraph.Store:
        """Create fresh in-memory pyoxigraph store for testing."""
        self.store = pyoxigraph.Store()
        return self.store
    
    async def test_insert_operations(self):
        """Test SPARQL INSERT operations."""
        
    async def test_delete_operations(self):
        """Test SPARQL DELETE operations."""
        
    async def test_delete_where_operations(self):
        """Test SPARQL DELETE WHERE operations."""
        
    async def test_complex_update_operations(self):
        """Test complex SPARQL UPDATE operations with multiple clauses."""
    
    def load_test_data(self, test_case: str) -> None:
        """
        Load predefined test data sets for different SPARQL UPDATE scenarios.
        
        Test Cases:
        - 'basic_persons': Person entities with names, ages, emails
        - 'organizations': Company entities with employees, locations
        - 'complex_graph': Multi-graph data with named graphs
        - 'datatypes': Various RDF datatypes (strings, integers, dates, etc.)
        """
        
    def execute_sparql_update(self, update_query: str) -> bool:
        """Execute SPARQL UPDATE operation on test store."""
        try:
            self.store.update(update_query)
            return True
        except Exception as e:
            print(f"SPARQL update failed: {e}")
            return False
    
    def get_all_triples(self) -> List[Dict]:
        """Get all triples from test store as list of dictionaries."""
        try:
            results = []
            for triple in self.store:
                results.append({
                    'subject': str(triple[0]),
                    'predicate': str(triple[1]),
                    'object': str(triple[2])
                })
            return results
        except Exception as e:
            print(f"SPARQL query failed: {e}")
            return []

# Comprehensive test cases covering all SPARQL UPDATE operations
class TestSPARQLUpdateOperations:
    """Test suite covering all SPARQL UPDATE operation types."""
    
    def setup_method(self):
        """Setup fresh test environment for each test."""
        self.framework = SPARQLUpdateTestFramework()
        self.framework.setup_test_store()
    
    def test_simple_insert(self):
        """Test basic INSERT DATA operation."""
        # Setup
        self.framework.load_test_data('basic_persons')
        before_triples = self.framework.get_all_triples()
        
        # Execute INSERT
        insert_query = """
            PREFIX : <http://example.org/>
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            
            INSERT DATA {
                :alice a foaf:Person ;
                    foaf:name "Alice Brown" ;
                    foaf:age 28 .
            }
        """
        
        success = self.framework.execute_sparql_update(insert_query)
        after_triples = self.framework.get_all_triples()
        
        # Validate
        assert success
        assert len(after_triples) == len(before_triples) + 3  # 3 new triples
        
        # Check specific new triples exist
        alice_triples = [t for t in after_triples if ':alice' in t['subject']]
        assert len(alice_triples) == 3
    
    def test_simple_delete(self):
        """Test basic DELETE DATA operation."""
        # Setup
        self.framework.load_test_data('basic_persons')
        before_triples = self.framework.get_all_triples()
        
        # Execute DELETE
        delete_query = """
            PREFIX : <http://example.org/>
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            
            DELETE DATA {
                :bob foaf:email "bob@example.com" .
            }
        """
        
        success = self.framework.execute_sparql_update(delete_query)
        after_triples = self.framework.get_all_triples()
        
        # Validate
        assert success
        # Should be same count since Bob didn't have email in test data
        assert len(after_triples) == len(before_triples)
    
    def test_conditional_delete(self):
        """Test DELETE WHERE operation with query patterns."""
        # Setup
        self.framework.load_test_data('basic_persons')
        before_triples = self.framework.get_all_triples()
        
        # Execute conditional DELETE
        delete_query = """
            PREFIX : <http://example.org/>
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            
            DELETE { 
                ?person foaf:email ?email 
            }
            WHERE { 
                ?person a foaf:Person .
                ?person foaf:email ?email .
                ?person foaf:age ?age .
                FILTER(?age > 28)
            }
        """
        
        success = self.framework.execute_sparql_update(delete_query)
        after_triples = self.framework.get_all_triples()
        
        # Validate - should remove John's email (age 30) but keep Jane's (age 25)
        assert success
        john_email_triples = [t for t in after_triples 
                             if ':john' in t['subject'] and 'email' in t['predicate']]
        jane_email_triples = [t for t in after_triples 
                             if ':jane' in t['subject'] and 'email' in t['predicate']]
        
        assert len(john_email_triples) == 0  # John's email removed
        assert len(jane_email_triples) == 1  # Jane's email preserved

    def test_parser_integration(self):
        """Test integration with SPARQLUpdateParser to validate parsing logic."""
        # This would test the actual parser we're building
        # Mock fuseki_manager for parser testing
        
        class MockFusekiManager:
            def __init__(self, test_store):
                self.test_store = test_store
            
            async def query_dataset(self, space_id, query):
                # Convert pyoxigraph results to expected format
                results = []
                for solution in self.test_store.query(query):
                    result = {}
                    for var_name, value in solution:
                        result[str(var_name)] = {'value': str(value), 'type': 'uri'}
                    results.append(result)
                return results
        
        # Setup
        self.framework.load_test_data('basic_persons')
        mock_fuseki = MockFusekiManager(self.framework.store)
        parser = SPARQLUpdateParser(mock_fuseki)
        
        # Test parsing DELETE operation
        delete_query = """
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            DELETE { ?person foaf:email ?email }
            WHERE { ?person a foaf:Person . ?person foaf:email ?email }
        """
        
        # This would test our actual parser implementation
        # parsed_result = await parser.parse_update_operation('test_space', delete_query)
        # assert parsed_result['operation_type'] == 'delete'
        # assert len(parsed_result['delete_triples']) > 0
```

## Standalone SPARQL Operations Engine

### SPARQL Operations Engine Implementation
```python
# vitalgraph/db/fuseki_postgresql/sparql_operations.py
import pyoxigraph
from rdflib import Graph
from rdflib.plugins.sparql import prepareQuery
from typing import Dict, List, Any, Optional, Tuple
import logging

class SPARQLOperationsEngine:
    """
    Standalone SPARQL operations engine for testing and validation.
    
    This class encapsulates all SPARQL UPDATE parsing and execution logic
    in a way that can be tested independently of the full hybrid backend.
    Uses pyoxigraph as the in-memory RDF store for validation.
    """
    
    def __init__(self, store: Optional[pyoxigraph.Store] = None):
        """
        Initialize SPARQL operations engine.
        
        Args:
            store: Optional pyoxigraph store. If None, creates in-memory store.
        """
        self.store = store or pyoxigraph.Store()
        self.logger = logging.getLogger(__name__)
    
    def load_turtle_data(self, turtle_data: str) -> bool:
        """Load RDF data from Turtle format into the store."""
        try:
            self.store.load(turtle_data.encode(), "text/turtle")
            return True
        except Exception as e:
            self.logger.error(f"Failed to load turtle data: {e}")
            return False
    
    def execute_sparql_update(self, update_query: str) -> bool:
        """Execute SPARQL UPDATE operation."""
        try:
            self.store.update(update_query)
            return True
        except Exception as e:
            self.logger.error(f"SPARQL update failed: {e}")
            return False
    
    def execute_sparql_query(self, query: str) -> List[Dict[str, Any]]:
        """Execute SPARQL SELECT query and return results."""
        try:
            results = []
            for solution in self.store.query(query):
                result = {}
                for var_name, value in solution:
                    result[str(var_name)] = {
                        'value': str(value),
                        'type': 'uri' if value.startswith('http') else 'literal'
                    }
                results.append(result)
            return results
        except Exception as e:
            self.logger.error(f"SPARQL query failed: {e}")
            return []
    
    def get_all_triples(self) -> List[Tuple[str, str, str]]:
        """Get all triples from the store."""
        triples = []
        for triple in self.store:
            triples.append((str(triple[0]), str(triple[1]), str(triple[2])))
        return triples
    
    def clear_store(self) -> None:
        """Clear all data from the store."""
        self.store.clear()
    
    def validate_sparql_syntax(self, query: str) -> bool:
        """Validate SPARQL query syntax without executing."""
        try:
            # Use rdflib for syntax validation
            prepareQuery(query)
            return True
        except Exception as e:
            self.logger.error(f"SPARQL syntax error: {e}")
            return False
```

### Test Coverage Requirements

**Basic Operations**: INSERT DATA, DELETE DATA
**Conditional Operations**: DELETE WHERE, INSERT WHERE  
**Combined Operations**: DELETE/INSERT WHERE
**Graph Operations**: Named graph insertions/deletions
**Filter Operations**: Complex FILTER conditions (STRSTARTS, regex, numeric, date)
**Datatype Operations**: String, integer, float, date, boolean literals
**Variable Binding**: Complex WHERE clauses with multiple variables
**Edge Cases**: Empty results, malformed queries, constraint violations

### Integration with Parser Testing

The framework validates that:
- Parser correctly identifies operation types
- Query-before-delete finds the right triples  
- All SPARQL UPDATE variations are supported
- Arbitrary WHERE clause queries work correctly

## Standalone Test Scripts

### Direct SPARQL Operations Testing
```python
# vitalgraph/db/fuseki_postgresql/test_scripts/test_sparql_operations.py
"""
Standalone test script for SPARQL operations validation.
Run directly with: python test_sparql_operations.py
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sparql_operations import SPARQLOperationsEngine

def test_basic_insert():
    """Test basic INSERT DATA operation."""
    print("=== Testing Basic INSERT ===")
    
    engine = SPARQLOperationsEngine()
    
    # Load initial data
    initial_data = """
        @prefix : <http://example.org/> .
        @prefix foaf: <http://xmlns.com/foaf/0.1/> .
        
        :john a foaf:Person ;
            foaf:name "John Doe" .
    """
    
    engine.load_turtle_data(initial_data)
    before_count = engine.count_triples()
    print(f"Initial triple count: {before_count}")
    
    # Execute INSERT
    insert_query = """
        PREFIX : <http://example.org/>
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        
        INSERT DATA {
            :jane a foaf:Person ;
                foaf:name "Jane Smith" ;
                foaf:age 25 .
        }
    """
    
    success = engine.execute_sparql_update(insert_query)
    after_count = engine.count_triples()
    
    print(f"INSERT success: {success}")
    print(f"Final triple count: {after_count}")
    print(f"Triples added: {after_count - before_count}")
    
    # Validate
    if success and after_count == before_count + 3:
        print("âœ… Basic INSERT test PASSED")
    else:
        print("âŒ Basic INSERT test FAILED")
    
    print()

def test_conditional_delete():
    """Test DELETE WHERE operation."""
    print("=== Testing Conditional DELETE ===")
    
    engine = SPARQLOperationsEngine()
    
    # Load test data
    test_data = """
        @prefix : <http://example.org/> .
        @prefix foaf: <http://xmlns.com/foaf/0.1/> .
        
        :john a foaf:Person ;
            foaf:name "John Doe" ;
            foaf:age 30 ;
            foaf:email "john@example.com" .
        
        :jane a foaf:Person ;
            foaf:name "Jane Smith" ;
            foaf:age 25 ;
            foaf:email "jane@example.com" .
    """
    
    engine.load_turtle_data(test_data)
    
    # Get validation report
    delete_query = """
        PREFIX : <http://example.org/>
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        
        DELETE { 
            ?person foaf:email ?email 
        }
        WHERE { 
            ?person a foaf:Person .
            ?person foaf:email ?email .
            ?person foaf:age ?age .
            FILTER(?age > 28)
        }
    """
    
    validation_report = engine.validate_sparql_update_operation(delete_query)
    
    print(f"Syntax valid: {validation_report['syntax_valid']}")
    print(f"Operation type: {validation_report['operation_details']['operation_type']}")
    print(f"Triples before: {validation_report['triple_count_before']}")
    print(f"Triples after: {validation_report['triple_count_after']}")
    print(f"Execution success: {validation_report['execution_result']}")
    
    # Should remove John's email (age 30) but keep Jane's (age 25)
    if (validation_report['execution_result'] and 
        validation_report['triple_count_after'] == validation_report['triple_count_before'] - 1):
        print("âœ… Conditional DELETE test PASSED")
    else:
        print("âŒ Conditional DELETE test FAILED")
    
    print()

def test_delete_insert_operation():
    """Test combined DELETE/INSERT operation."""
    print("=== Testing DELETE/INSERT ===")
    
    engine = SPARQLOperationsEngine()
    
    # Load test data
    test_data = """
        @prefix : <http://example.org/> .
        @prefix foaf: <http://xmlns.com/foaf/0.1/> .
        
        :john a foaf:Person ;
            foaf:name "John Doe" ;
            foaf:age 30 .
    """
    
    engine.load_turtle_data(test_data)
    
    # Update John's age
    update_query = """
        PREFIX : <http://example.org/>
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        
        DELETE { 
            :john foaf:age ?oldAge 
        }
        INSERT { 
            :john foaf:age 31 
        }
        WHERE { 
            :john foaf:age ?oldAge 
        }
    """
    
    validation_report = engine.validate_sparql_update_operation(update_query)
    
    print(f"Operation type: {validation_report['operation_details']['operation_type']}")
    print(f"Execution success: {validation_report['execution_result']}")
    
    # Verify age was updated
    age_query = """
        PREFIX : <http://example.org/>
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        
        SELECT ?age WHERE {
            :john foaf:age ?age .
        }
    """
    
    results = engine.execute_sparql_query(age_query)
    
    if (validation_report['execution_result'] and 
        len(results) == 1 and 
        results[0]['age']['value'] == '31'):
        print("âœ… DELETE/INSERT test PASSED")
    else:
        print("âŒ DELETE/INSERT test FAILED")
    
    print()

def run_all_tests():
    """Run all SPARQL operation tests."""
    print("ðŸ§ª Running SPARQL Operations Test Suite")
    print("=" * 50)
    
    test_basic_insert()
    test_conditional_delete()
    test_delete_insert_operation()
    
    print("ðŸ Test suite completed")

if __name__ == "__main__":
    run_all_tests()
```

## Atomic Operations Testing

### Update Quads Testing Framework
```python
# vitalgraph/db/fuseki_postgresql/test_scripts/test_update_quads.py
"""
Comprehensive test suite for atomic update_quads functionality.
Tests PostgreSQL transaction management, Fuseki synchronization, and atomic operation validation.
"""

async def test_update_quads_basic_operations():
    """Test basic atomic quad replacement operations."""
    
async def test_update_quads_transaction_rollback():
    """Test PostgreSQL transaction rollback on failure."""
    
async def test_update_quads_fuseki_sync_failure():
    """Test Fuseki synchronization failure handling."""
    
async def test_update_quads_large_operations():
    """Test performance with large quad sets (1000+ quads)."""
    
async def test_update_quads_empty_sets():
    """Test edge cases with empty delete/insert sets."""
    
async def test_update_quads_concurrent_operations():
    """Test concurrent update_quads operations for race conditions."""
```

### Integration with Existing Test Suite
The `test_update_quads.py` module will be integrated into the comprehensive test orchestrator as **Phase 1.6: Atomic Operations Validation**, positioned after frame deletion tests and before any UPDATE/UPSERT frame operations.

## Legacy UPDATE Implementation Revision

### Critical Architectural Impact
The introduction of atomic `update_quads` functionality requires revision of existing UPDATE implementations that currently use separate delete/insert operations. This ensures consistency across the entire VitalGraph architecture.

### KGEntity UPDATE Revision
**Current Issue**: `KGEntityUpdateProcessor.update_entity()` uses high-level `delete_object()` followed by `store_objects()`, which lacks true atomicity.

**Required Changes**:
```python
# Current implementation (lines 55-68 in kgentity_update_impl.py)
delete_result = await backend.delete_object(space_id, graph_id, entity_uri)
store_result = await backend.store_objects(space_id, graph_id, updated_objects)

# New atomic implementation using update_quads
delete_quads = await self._build_entity_delete_quads(space_id, graph_id, entity_uri)
insert_quads = self._build_entity_insert_quads(graph_id, updated_objects)
success = await backend.update_quads(space_id, graph_id, delete_quads, insert_quads)
```

**Benefits**:
- True atomicity for entity updates
- Consistent transaction management across all UPDATE operations
- Better performance through batch quad operations
- Unified error handling and rollback behavior
```
- SPARQL implementation - See `../fuseki_psql_sparql_plan.md` for complete details
- Performance optimization ongoing for production workloads
- Monitoring and observability essential for production deployment

## Critical Architecture Rules

### JsonLdObject vs JsonLdDocument Handling
**ABSOLUTE RULE: Never ever ever convert JsonLdObject to JsonLdDocument**

- JsonLdObject represents single JSON-LD objects and must remain as single objects
- JsonLdDocument represents documents with @graph arrays containing multiple objects  
- The service/endpoint layer must handle JsonLdObject inputs directly without conversion
- Single objects should stay single objects throughout the entire processing pipeline
- Any conversion between these types violates the fundamental distinction between single objects and document collections

This rule applies to:
- All endpoint implementations
- All processor implementations
- All validation logic
- All conversion utilities
- Test code and production code

The service layer must be designed to handle both JsonLdObject and JsonLdDocument as distinct input types without forced conversion between them.

### VitalSigns Data Flow Rule
**ABSOLUTE RULE: Never directly use JSON-LD data in JsonLdObject or JsonLdDocument constructors**

**Forbidden Pattern:**
```python
# WRONG - Direct JSON-LD data usage
jsonld_obj = JsonLdObject(**some_jsonld_dict)
jsonld_doc = JsonLdDocument(**some_jsonld_dict)
```

**Correct Pattern:**
Always use VitalSigns objects as the source, then convert to JSON-LD:
```python
# CORRECT - VitalSigns object as source
frame = KGFrame()  # Create VitalSigns object
frame.URI = "..."
frame.name = "..."

# Convert VitalSigns to JSON-LD, then to Pydantic model
jsonld_dict = entity.to_jsonld()
jsonld_obj = JsonLdObject(**jsonld_dict)
```

**Why This Rule Exists:**
- JsonLdObject/JsonLdDocument are Pydantic models for API contracts
- They should only contain data that originated from VitalSigns objects
- Direct JSON-LD manipulation bypasses VitalSigns validation and type safety
- Maintains clear separation between VitalSigns domain objects and API models

This rule ensures data integrity and maintains the proper VitalSigns â†’ JSON-LD â†’ Pydantic model flow.

### Pydantic Response Model Requirements
**ABSOLUTE RULE: All endpoints must use proper Pydantic models for structured responses**

**Forbidden Pattern:**
```python
# WRONG - Generic dictionary responses
@router.get("/endpoint", response_model=Dict[str, Any])
async def get_data():
    return {"some": "data", "nested": {"structure": "here"}}
```

**Required Pattern:**
```python
# CORRECT - Proper Pydantic model with defined structure
class DataResponse(BaseModel):
    some: str = Field(..., description="Some data field")
    nested: NestedModel = Field(..., description="Nested structured data")

@router.get("/endpoint", response_model=DataResponse)
async def get_data():
    return DataResponse(some="data", nested=NestedModel(structure="here"))
```

**Why This Rule Exists:**
- **API Documentation**: Pydantic models generate proper OpenAPI/Swagger documentation
- **Type Safety**: Provides compile-time and runtime type checking
- **Validation**: Ensures response data matches expected structure
- **Client Generation**: Enables proper client SDK generation
- **Maintainability**: Makes API contracts explicit and discoverable

**Implementation Requirements:**
1. **Define Response Structure**: Create Pydantic models that exactly match response data structure
2. **Use Field Descriptions**: All fields must have clear descriptions for API documentation
3. **Handle Union Types**: Use `Union[ModelA, ModelB]` for endpoints with multiple response formats
4. **Nested Models**: Create separate models for complex nested structures
5. **Error Models**: Define specific models for error responses within successful HTTP responses

This rule applies to all VitalGraph endpoints and ensures consistent, well-documented, and type-safe API responses.

## Multi-Dataset Architecture Requirements

### Current Implementation Status

**âœ… Completed Components**

**1. FUSEKI_POSTGRESQL Hybrid Backend Implementation**
- **âœ… DUAL-WRITE CONSISTENCY: 9/9 tests passed (100%)**

**2. Interim Fuseki Backend Implementation**
- `FusekiSpaceImpl` - HTTP-based space backend (1,101 lines) - **Single dataset approach**
- `FusekiSparqlImpl` - SPARQL implementation details
- `FusekiSignalManager` - No-op notification system (277 lines)
- Backend factory integration with `BackendType.FUSEKI`
- Configuration support in `vitalgraphdb-config.yaml`
- **KG Query Builder Integration** - Working with interim implementation

**3. HTTP Integration**
- aiohttp-based async HTTP client with connection pooling
- Basic authentication support
- Proper error handling and timeout management
- SPARQL endpoint integration
- **Docker host connectivity via `host.docker.internal`**

**4. Test Infrastructure**
- **FUSEKI_POSTGRESQL Backend Test** - **âœ… 9/9 tests passed (100%)**
- **SPARQL Pattern Parsing Test** - **âœ… Pure RDFLib implementation verified**
- **Backend Integration Test** - Validates single-dataset approach
- **REST API Integration Test** - Tests interim implementation
- VitalSigns data conversion and RDF operations
- Complex SPARQL query testing with KG query builder
- Docker Compose setup for local testing
- **Performance validation**: 1,807 triples inserted in ~231ms (single dataset)
- **Dual-write consistency verification**: 9 triples matching in both Fuseki and PostgreSQL

### Critical Requirements - Multi-Dataset Architecture

**âŒ CRITICAL GAPS - MULTI-DATASET ARCHITECTURE REQUIRED**
The current single-dataset implementation is an interim solution with significant limitations. The multi-dataset architecture is not optional - it is a critical requirement for proper space isolation and scalability.

**ðŸŽ¯ REQUIRED: Multi-Dataset Architecture Implementation**
The current single-dataset approach has fundamental limitations that must be addressed:

**Current Interim Architecture (VALIDATED & WORKING):**
```
Fuseki Server
â””â”€â”€ vitalgraph dataset
    â”œâ”€â”€ urn:vitalgraph:spaces (admin metadata graph) - 16 triples
    â”œâ”€â”€ http://vital.ai/graph/test_complex_queries/entities (space data) - 1,740 triples
    â”œâ”€â”€ http://vital.ai/graph/test_complex_queries/connections (space data) - 67 triples
    â””â”€â”€ {complete_graph_uri} (additional spaces - URIs provided by calling layer)
```

**Target Multi-Dataset Architecture:**
- **Current (Interim)**: Single Fuseki dataset (`vitalgraph`) with named graphs per space
- **Target (Required)**: Separate Fuseki dataset per space + admin dataset for metadata
- **Admin Dataset**: Tracks spaces, graphs within spaces, and users (following PostgreSQL schema)
- **Space Datasets**: Individual datasets for each VitalGraph space's RDF data

**CRITICAL: Graph URI Format in Fuseki-PostgreSQL Backend**
In the current fuseki-postgresql backend implementation, the `graph_id` parameter **MUST be a complete URI**, not a plain string. The backend does NOT construct URIs - it uses the provided `graph_id` directly as the graph URI.

**Correct Usage Pattern:**
- `graph_id` parameter: Complete URI (e.g., `"http://vital.ai/graph/my_space/entities"`)
- Backend behavior: Uses `graph_id` directly without modification
- SPARQL queries: `GRAPH <{graph_id}>` where `graph_id` is the complete URI

**Incorrect Usage Pattern (DO NOT USE):**
- `graph_id` parameter: Plain string (e.g., `"my_graph"`)
- This will cause SPARQL syntax errors as plain strings are not valid graph URIs

**REFACTORING NOTE: Remove Misleading build_graph_uri Function**
The current `FusekiQueryUtils.build_graph_uri(space_id, graph_id)` function should be factored out as it is misleading - it simply returns `graph_id` without any building/construction. Since `graph_id` must already be a complete URI, this function adds no value and creates confusion about URI construction responsibilities.

## Implementation Plan

### Phase 1: Architectural Redesign (Multi-Dataset)

#### 1.1 Performance Optimization
**Priority: Medium**
**Estimated Time: 1-2 days**

- Implement connection pooling optimizations
- Add batch operation support for bulk data loading
- Optimize SPARQL query generation for common patterns
- Add query result caching where appropriate

### Phase 2: Admin Space Infrastructure

#### 2.1 Admin Dataset Implementation
**Priority: High**
**Estimated Time: 3-4 days**

Implement the admin dataset following PostgreSQL schema patterns:

```python
class FusekiAdminDataset:
    """
    Admin dataset implementation for VitalGraph space management.
    
    Manages separate Fuseki dataset: vitalgraph_admin
    Replicates PostgreSQL admin tables as RDF:
    - Install: Installation metadata and state
    - Space: Space registry with tenant isolation  
    - User: User management with authentication
    - Graph: Graph tracking within each space
    """
    
    ADMIN_DATASET = "vitalgraph_admin"
    
    # RDF Classes (equivalent to PostgreSQL tables)
    INSTALL_CLASS = "http://vital.ai/admin/Install"
    SPACE_CLASS = "http://vital.ai/admin/Space"
    USER_CLASS = "http://vital.ai/admin/User"
    GRAPH_CLASS = "http://vital.ai/admin/Graph"
    
    async def initialize_admin_dataset(self) -> bool:
        """Create admin dataset and initialize with schema."""
        
    async def create_install_record(self) -> bool:
        """Create initial install record (equivalent to PostgreSQL Install table)."""
        
    async def register_space(self, space_id: str, space_name: str, 
                           space_description: str = None, tenant: str = None) -> bool:
        """Register new space in admin dataset."""
        
    async def unregister_space(self, space_id: str) -> bool:
        """Remove space from admin dataset."""
        
    async def register_graph(self, space_id: str, graph_uri: str, 
                           graph_name: str = None) -> bool:
        """Register graph within a space."""
        
    async def list_spaces(self) -> List[Dict[str, Any]]:
        """List all registered spaces with metadata."""
        
    async def list_graphs_for_space(self, space_id: str) -> List[Dict[str, Any]]:
        """List all graphs within a specific space."""
        
    async def get_space_info(self, space_id: str) -> Optional[Dict[str, Any]]:
        """Get detailed space information from admin dataset."""
```

#### 2.2 Fuseki Dataset Management
**Priority: High**
**Estimated Time: 2 days**

Implement Fuseki dataset lifecycle management via HTTP Admin API:

```python
class FusekiDatasetManager:
    """
    Manages Fuseki dataset creation, deletion, and configuration.
    Uses Fuseki's HTTP Admin API for dataset operations.
    """
    
    def __init__(self, server_url: str, admin_user: str, admin_password: str):
        self.server_url = server_url
        self.admin_url = f"{server_url}/$/datasets"
        self.auth = aiohttp.BasicAuth(admin_user, admin_password)
    
    async def create_dataset(self, dataset_name: str, dataset_type: str = "tdb2") -> bool:
        """Create new Fuseki dataset via Admin API."""
        
    async def delete_dataset(self, dataset_name: str) -> bool:
        """Delete Fuseki dataset via Admin API."""
        
    async def list_datasets(self) -> List[str]:
        """List all datasets on Fuseki server."""
        
    async def dataset_exists(self, dataset_name: str) -> bool:
        """Check if dataset exists on Fuseki server."""
```

#### 2.3 Initialization Scripts
**Priority: High**
**Estimated Time: 1 day**

Create scripts for setting up VitalGraph with multi-dataset Fuseki backend.

## Testing Strategy and Validation

### Integration Testing Results

```python
# âœ… tests/integration/test_fuseki_integration.py - ALL PASSING
class TestFusekiIntegration:
    âœ… test_vitalgraph_service_with_fuseki()  # Full service integration
    âœ… test_kg_endpoints_with_fuseki()  # KG query builder integration
    âœ… test_space_manager_with_fuseki()  # Space management validation
    âœ… test_rest_api_operations()  # Complete REST API validation
    âœ… test_docker_integration()  # Container networking validation
    âœ… test_authentication_flow()  # JWT authentication working
    âœ… test_data_persistence()  # RDF data storage/retrieval
    âœ… test_performance_benchmarks()  # Query performance validation
```

### Performance Test Results - EXCELLENT PERFORMANCE

**âœ… Performance Validation Results:**
- âœ… **Bulk data loading**: 1,807 triples in 231ms (7,830 triples/sec)
- âœ… **Query performance**: 16-27ms for complex multi-criteria queries
- âœ… **Authentication**: 13.77ms for JWT token validation
- âœ… **Space operations**: 138.95ms for space creation via REST API
- âœ… **Memory efficiency**: Excellent resource utilization
- âœ… **Connection pooling**: aiohttp pool working optimally
- âœ… **Docker networking**: host.docker.internal connectivity validated

### Production Deployment Validation

**Infrastructure: DOCKER CONTAINERIZED**

Production deployment infrastructure validated:

- âœ… **Docker Configuration**: VitalGraph service containerized and tested
- âœ… **Fuseki Integration**: External Fuseki server connectivity validated
- âœ… **Network Configuration**: host.docker.internal connectivity working
- âœ… **Authentication**: JWT-based authentication system operational
- âœ… **Configuration Management**: vitalgraphdb-config.yaml working
- âœ… **Health Checks**: Service startup and connectivity validated
- âœ… **Data Persistence**: RDF data storage and retrieval confirmed

### Monitoring & Observability - IMPLEMENTED

**Logging: COMPREHENSIVE**

**âœ… Monitoring Features:**
- âœ… **Fuseki Metrics**: Query performance tracking (16-27ms validated)
- âœ… **VitalGraph Health**: Service startup and connectivity monitoring
- âœ… **Performance Logging**: Detailed timing for all operations
- âœ… **Error Tracking**: Comprehensive error handling and logging
- âœ… **Request Monitoring**: REST API request/response tracking
- âœ… **Data Validation**: Triple count and integrity monitoring
- âœ… **Authentication Logging**: JWT token validation tracking

## Local Development Testing

**1. Docker Compose Environment**
```bash
# Start Fuseki + VitalGraph locally
docker-compose -f docker-compose.vitalgraph-fuseki.yml up -d

# Run initialization
python scripts/init_vitalgraph_fuseki.py

# Run test suite
python -m pytest tests/fuseki/ -v

# Run integration tests
python test_fuseki_backend.py
```

**2. Manual Testing Workflow**
- Space creation and management
- RDF data loading and querying
- KG endpoint operations (entities, frames, types)
- Multi-user concurrent access
- Error scenarios and recovery

## CI/CD Integration

**1. Automated Testing Pipeline**
- Unit tests on every commit
- Integration tests on PR creation
- Performance regression testing
- Docker image building and testing

**2. Deployment Pipeline**
- Staging environment deployment
- Production deployment with rollback capability
- Health check validation
- Monitoring setup verification

## Configuration Management

### Environment Configurations

**Local Development:**
```yaml
# vitalgraphdb-config-local.yaml
fuseki:
  server_url: "http://localhost:3030"
  admin_user: "admin"
  admin_password: "admin"
  dataset_type: "tdb2"
  connection_pool_size: 10
  timeout_seconds: 30

postgresql:
  host: "localhost"
  port: 5432
  database: "vitalgraph"
  user: "vitalgraph"
  password: "vitalgraph"
  pool_size: 20
  
logging:
  level: "DEBUG"
  fuseki_queries: true
  performance_metrics: true
```

**Production:**
```yaml
# vitalgraphdb-config-production.yaml
fuseki:
  server_url: "${FUSEKI_SERVER_URL}"
  admin_user: "${FUSEKI_ADMIN_USER}"
  admin_password: "${FUSEKI_ADMIN_PASSWORD}"
  dataset_type: "tdb2"
  connection_pool_size: 50
  timeout_seconds: 60

postgresql:
  host: "${POSTGRES_HOST}"
  port: "${POSTGRES_PORT}"
  database: "${POSTGRES_DATABASE}"
  user: "${POSTGRES_USER}"
  password: "${POSTGRES_PASSWORD}"
  pool_size: 100
  
logging:
  level: "INFO"
  fuseki_queries: false
  performance_metrics: true
```

## Implementation Timeline and Priorities

### Timeline Overview

**Week 1: Architectural Foundation**
- Multi-dataset architecture design and implementation
- Admin dataset implementation and schema setup
- Dataset management via HTTP Admin API
- Basic space lifecycle operations

**Week 2-3: Core Implementation**
- Complete endpoint implementations
- SPARQL parser integration and optimization
- Performance testing and optimization

**Week 4-5: Production Deployment**
- ECS deployment configuration for multi-dataset architecture
- Monitoring and observability setup
- Backup and recovery procedures for multiple datasets
- Documentation and runbooks

**Total Estimated Time: 5 weeks** (increased due to architectural redesign)

### Immediate Next Steps - Multi-Dataset Architecture

**Day 1-3: Architectural Redesign Foundation**
1. **CRITICAL**: Redesign `FusekiSpaceImpl` constructor for multi-dataset architecture
2. Implement `FusekiDatasetManager` for HTTP Admin API dataset operations
3. Create admin dataset RDF schema
4. Implement basic dataset lifecycle methods (`create_dataset`, `delete_dataset`)

**Day 4-5: Admin Dataset Implementation**
1. Implement `FusekiAdminDataset` class with PostgreSQL-equivalent operations
2. Create admin dataset initialization and schema setup
3. Implement space registration/unregistration in admin dataset
4. Build multi-dataset initialization script

**Week 2: Core Method Reimplementation**
1. Reimplement all `SpaceBackendInterface` methods for multi-dataset architecture
2. Update space lifecycle methods
3. Update all RDF operations to target correct space datasets
4. Implement cross-dataset queries via admin dataset

### Week 2-4: Production Readiness

**Week 2: Testing & Validation**
- Complete unit test suite for all Fuseki components
- Performance testing and optimization
- Error handling and edge case validation

**Week 3: ECS Integration**
- Update ECS deployment for Fuseki backend
- Configure monitoring and health checks
- Test staging environment deployment

**Week 4: Production Deployment**
- Production deployment with rollback capability
- Monitoring setup and validation
- Documentation and runbooks completion

## Implementation Status Update - January 2026

**MAJOR BREAKTHROUGH**: The Fuseki+PostgreSQL hybrid backend implementation has achieved significant success with complete endpoint functionality and perfect dual-write consistency.

### Hybrid Architecture - Core Functionality Implemented
**Focus**: Fuseki+PostgreSQL integration with dual-write consistency
**Architecture**: Functional dual-write system with error handling
**Completion**: ~25% of total implementation (core functionality implemented)

**âœ… COMPLETED CORE FUNCTIONALITY:**
- âœ… **Backend Implementation** - Complete dual-write coordination
- âœ… **Signal Manager** - Fixed asyncpg 0.30.0 compatibility with add_listener/remove_listener API
- âœ… **Triples Endpoint** - Complete implementation with validation
- âœ… **Dual-Write Consistency** - Perfect validation with matching PostgreSQL/Fuseki counts
- âœ… **Error Handling** - Comprehensive error handling and logging throughout
- âœ… **Test Framework** - Complete test suites for spaces, graphs, and triples endpoints

**âœ… CRITICAL FIXES IMPLEMENTED:**
- âœ… **Admin Table Logic** - Changed from creation to verification-only during backend initialization
- âœ… **PostgreSQL Signal Manager** - Updated to use correct asyncpg API (add_listener instead of wait_for_notification)
- âœ… **Dual-Write Validation** - Fixed attribute access (postgresql_impl instead of core) for proper connection handling
- âœ… **Schema Compatibility** - Updated PostgreSQL schema to match existing backend with composite keys and dataset columns
- âœ… **Index Creation** - Added IF NOT EXISTS to prevent duplicate index errors

**ðŸ”„ REMAINING WORK:**
- âœ… **SPARQL Parser Implementation** - COMPLETED
- âœ… **Frame Creation and Deletion** - COMPLETED: Fixed Edge_hasEntityKGFrame object persistence
- ðŸ”„ **Code Review for Duplicate Function Definitions** - HIGH PRIORITY: Review codebase for duplicate function definitions
- ðŸ”„ **Aggressive RDFLib Parsing Failure Logging** - HIGH PRIORITY
- ðŸ”„ **Performance Optimization** - Query optimization and caching
- ðŸ”„ **Production Deployment** - ECS deployment configuration
- ðŸ”„ **Monitoring & Observability** - Monitoring setup
- ðŸ”„ **Documentation** - API documentation and deployment guides

## Test File Organization

### Current Test Files with Planning File References
- `test_spaces_endpoint_fuseki_postgresql.py` â†’ `endpoints/fuseki_psql_spaces_endpoint_plan.md`
- `test_triples_endpoint_fuseki_postgresql.py` â†’ `endpoints/fuseki_psql_triples_endpoint_plan.md`
- `test_graphs_endpoint_fuseki_postgresql.py` â†’ `endpoints/fuseki_psql_graphs_endpoint_plan.md`
- `test_fuseki_postgresql_backend_complete.py` â†’ `endpoints/fuseki_psql_backend_plan.md`
- SPARQL test files â†’ `fuseki_psql_sparql_plan.md`

### Comprehensive Integration Test
**Cross-Endpoint Integration Test**: `test_comprehensive_atomic_operations.py`

**Test Description**: Comprehensive atomic operations test suite validating complete lifecycle operations across all KG components:
- **Atomic Frame Lifecycle**: CREATE â†’ UPDATE â†’ UPSERT operations using KGFrameCreateProcessor
- **Cross-Component Integration**: Entity-Frame-Slot relationship validation
- **Dual-Write Consistency**: PostgreSQL and Fuseki synchronization validation
- **Error Recovery**: Transaction rollback and error handling validation

## Implementation Files and Status

### VitalSigns Integration Patterns

**Critical Implementation Pattern:**
- **Batch Operations** (create multiple, update multiple): Use `objects.to_jsonld_list()` â†’ JSON-LD document with @graph array
- **Individual Operations** (update single, get single): Use `object.to_jsonld()` â†’ JSON-LD object
- **Response Parsing**: Match conversion method to expected response format
- **Critical**: Never use list conversion for single objects or single conversion for multiple objects

### Implemented Files

**âœ… IMPLEMENTED FILES:**
- `/vitalgraph/db/fuseki_postgresql/fuseki_postgresql_space_impl.py` - Hybrid space implementation
- `/vitalgraph/db/fuseki_postgresql/dual_write_coordinator.py` - Transaction-safe dual-write operations
- `/vitalgraph/db/fuseki_postgresql/sparql_update_parser.py` - SPARQL parsing implementation
- `/vitalgraph/db/fuseki_postgresql/postgresql_db_impl.py` - PostgreSQL implementation with admin table verification
- `/vitalgraph/db/fuseki_postgresql/postgresql_schema.py` - Schema with composite keys and dataset columns
- `/vitalgraph/db/fuseki_postgresql/postgresql_signal_manager.py` - Fixed asyncpg 0.30.0 compatibility
- `/vitalgraph/db/fuseki_postgresql/fuseki_postgresql_space_graphs.py` - Graph management
- `/test_scripts/fuseki_postgresql/test_spaces_endpoint_fuseki_postgresql.py` - 7/7 tests passing
- `/test_scripts/fuseki_postgresql/test_graphs_endpoint_fuseki_postgresql.py` - 6/6 tests passing
- `/test_scripts/fuseki_postgresql/test_triples_endpoint_fuseki_postgresql.py` - Complete implementation

### Original Plan Components (Not Yet Implemented)

**Focus**: Multi-dataset Fuseki + PostgreSQL hybrid architecture
**Architecture**: Separate Fuseki datasets per space + PostgreSQL primary data tables
**Status**: Planning phase only - implementation not started

**Components Still Required:**
- âŒ **Multi-dataset Fuseki architecture** - Need to implement separate dataset per space
- âŒ **FusekiDatasetManager** - HTTP Admin API dataset operations not implemented
- âŒ **FusekiAdminDataset** - RDF-based metadata management not implemented  
- âŒ **Direct PostgreSQL connections** - Basic connection exploration only
- âŒ **Per-space primary data tables** - Schema design not finalized
- âŒ **Admin dataset initialization scripts** - Not implemented

**Files Still To Be Created:**
- `/vitalgraph/db/fuseki_postgresql/fuseki_admin_dataset.py` - Not created
- `/scripts/init_vitalgraph_fuseki_admin.py` - Not created
- `/vitalgraph/db/fuseki_postgresql/fuseki_dataset_manager.py` - Not created
- `/vitalgraph/db/fuseki_postgresql/postgresql_schema.py` - Not created
- `/vitalgraph/db/fuseki_postgresql/dual_write_coordinator.py` - Not created

## Realistic Assessment - Implementation Status

**SPARQL UPDATE Implementation**: âœ… COMPLETED
**Hybrid Architecture**: Concept exploration only âŒ

**ACTUAL IMPLEMENTATION STATUS:**
The Fuseki+PostgreSQL hybrid backend is in very early exploration phase:

**Backend Implementation Status:**
- Complete dual-write system architecture
- SPARQL implementation
- Fuseki dataset management
- PostgreSQL schema implementation  
- Error handling and recovery
- Performance optimization
- Security implementation
- Production testing
- Documentation
- Monitoring and observability

This is a very early stage exploration with the vast majority of implementation work still ahead.

## Hybrid Architecture Design (Original Plan)

**Status: PLANNING PHASE - January 2026**
**Estimated Implementation Time: 6-12 months**

The `BackendType.FUSEKI_POSTGRESQL` hybrid backend requires extensive implementation work:

**Architecture Overview:**
```
VitalGraph FUSEKI_POSTGRESQL Backend
â”œâ”€â”€ Fuseki Server (Primary for graph operations)
â”‚   â”œâ”€â”€ vitalgraph_space_space1 dataset (active graph data)
â”‚   â”œâ”€â”€ vitalgraph_space_space2 dataset (active graph data)
â”‚   â””â”€â”€ vitalgraph_space_spaceN dataset (active graph data)
â””â”€â”€ PostgreSQL Server (Primary for relational operations)
    â”œâ”€â”€ Admin tables (spaces, graphs, users, install)
    â”œâ”€â”€ space1 schema (quad tables for space1 RDF data)
    â”œâ”€â”€ space2 schema (quad tables for space2 RDF data)
    â””â”€â”€ spaceN schema (quad tables for spaceN RDF data)
```

## DbImpl Interface Design

**Priority: High**
**Estimated Time: 1 day**

Create a common `DbImplInterface` that both PostgreSQL and FUSEKI_POSTGRESQL backends can implement:

```python
# vitalgraph/db/db_inf.py (existing file)
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List

class DbImplInterface(ABC):
    """
    Common interface for database implementation components.
    Both PostgreSQL and FUSEKI_POSTGRESQL backends will implement this interface.
    """
    
    @abstractmethod
    async def initialize(self) -> bool:
        """Initialize database connection and schema."""
        pass
    
    @abstractmethod
    async def create_space(self, space_id: str, space_name: str) -> bool:
        """Create a new space in the database."""
        pass
    
    @abstractmethod
    async def delete_space(self, space_id: str) -> bool:
        """Delete a space from the database."""
        pass
    
    @abstractmethod
    async def store_objects(self, space_id: str, graph_id: str, objects: List[Any]) -> bool:
        """Store objects in the specified space and graph."""
        pass
    
    @abstractmethod
    async def query_objects(self, space_id: str, query: str) -> List[Dict[str, Any]]:
        """Execute a query and return results."""
        pass
```

## PostgreSQL Schema Implementation

### Schema Design for Hybrid Backend

```python
# vitalgraph/db/fuseki_postgresql/postgresql_schema.py
from typing import Dict, List

class FusekiPostgreSQLSchema:
    """
    PostgreSQL schema for FUSEKI_POSTGRESQL hybrid backend.
    
    Provides admin tables (shared) + per-space primary data tables.
    Admin tables track spaces, graphs, users - matching existing PostgreSQL backend.
    Per-space tables provide archival/backup of RDF data stored primarily in Fuseki.
    """
    
    def get_admin_tables(self) -> Dict[str, str]:
        """Get admin table definitions (shared across all spaces)."""
        return {
            # Install table - matches existing PostgreSQL backend install table
            'install': '''
                CREATE TABLE install (
                    install_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    install_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    version VARCHAR(50),
                    status VARCHAR(50)
                )
            ''',
            
            # Space table - matches existing PostgreSQL backend space table  
            'space': '''
                CREATE TABLE space (
                    space_id VARCHAR(255) PRIMARY KEY,
                    space_name VARCHAR(255) NOT NULL,
                    space_description TEXT,
                    tenant VARCHAR(255),
                    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''',
            
            # Graph table - matches existing PostgreSQL backend graph table
            'graph': '''
                CREATE TABLE graph (
                    space_id VARCHAR(255) NOT NULL,
                    graph_uri VARCHAR(500) NOT NULL,
                    graph_name VARCHAR(255),
                    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (space_id, graph_uri),
                    FOREIGN KEY (space_id) REFERENCES space(space_id) ON DELETE CASCADE
                )
            ''',
            
            # User table - matches existing PostgreSQL backend user table
            'user': '''
                CREATE TABLE "user" (
                    user_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    username VARCHAR(255) UNIQUE NOT NULL,
                    password_hash VARCHAR(255),
                    email VARCHAR(255),
                    tenant VARCHAR(255),
                    update_time TIMESTAMP
                )
            '''
        }
    
    # Per-space primary data tables (matching existing PostgreSQL backend schema)
    def get_space_tables(self, space_id: str) -> Dict[str, str]:
        prefix = f"{space_id}_"
        return {
            # Term table - matches existing PostgreSQL backend term table structure
            'term': f'''
                CREATE TABLE {prefix}term (
                    term_uuid UUID PRIMARY KEY,
                    term_text TEXT NOT NULL,
                    term_type CHAR(1) NOT NULL,
                    term_language VARCHAR(10),
                    term_datatype VARCHAR(500)
                )
            ''',
            
            # RDF Quad table - matches existing PostgreSQL backend rdf_quad table structure
            'rdf_quad': f'''
                CREATE TABLE {prefix}rdf_quad (
                    subject_uuid UUID NOT NULL,
                    predicate_uuid UUID NOT NULL,
                    object_uuid UUID NOT NULL,
                    context_uuid UUID NOT NULL,
                    FOREIGN KEY (subject_uuid) REFERENCES {prefix}term(term_uuid) ON DELETE CASCADE,
                    FOREIGN KEY (predicate_uuid) REFERENCES {prefix}term(term_uuid) ON DELETE CASCADE,
                    FOREIGN KEY (object_uuid) REFERENCES {prefix}term(term_uuid) ON DELETE CASCADE,
                    FOREIGN KEY (context_uuid) REFERENCES {prefix}term(term_uuid) ON DELETE CASCADE,
                    PRIMARY KEY (subject_uuid, predicate_uuid, object_uuid, context_uuid)
                )
            '''
        }
    
    def get_admin_indexes(self) -> Dict[str, List[str]]:
        """Get index definitions for admin tables only (space, graph, user tables)."""
        return {
            'space_indexes': [
                'CREATE INDEX idx_space_tenant ON space(tenant)',
                'CREATE INDEX idx_space_update_time ON space(update_time)'
            ],
            'graph_indexes': [
                'CREATE INDEX idx_graph_space_id ON graph(space_id)',
                'CREATE INDEX idx_graph_uri ON graph(graph_uri)'
            ],
            'user_indexes': [
                'CREATE INDEX idx_user_tenant ON "user"(tenant)',
                'CREATE INDEX idx_user_username ON "user"(username)'
            ]
        }
    
    # NOTE: No indexes needed for per-space term and quad tables
    # These tables are used for archival/backup purposes only, not active querying
    # All active graph queries go directly to Fuseki datasets for optimal performance
```

## PostgreSQL DbImpl Implementations

### Existing PostgreSQL Backend Update

**Priority: High**
**Estimated Time: 1 day**

Update existing PostgreSQL backend to implement the DbImplInterface:

```python
# vitalgraph/db/postgresql/postgresql_db_impl.py (minimal changes)
from ..db_inf import DbImplInterface

class PostgreSQLDbImpl(DbImplInterface):  # Add interface inheritance
    """
    PostgreSQL database implementation.
    Now implements DbImplInterface for consistency with hybrid backend.
    """
    
    # All existing methods remain unchanged
    # Interface methods already exist with compatible signatures:
    # - connect() -> bool (already exists)
    # - disconnect() -> bool (already exists) 
    # - is_connected() -> bool (already exists)
    # - execute_query() -> List[Dict] (already exists)
    # - execute_update() -> bool (already exists)
    # - begin_transaction() -> Any (already exists)
    # - commit_transaction() -> bool (already exists)
    # - rollback_transaction() -> bool (already exists)
    # - get_connection_info() -> Dict (already exists)
    
    # No implementation changes needed - just interface marking
```

### FUSEKI_POSTGRESQL DbImpl Implementation

**Priority: High**
**Estimated Time: 2 days**

Create PostgreSQL component for hybrid backend implementing DbImplInterface:

```python
# vitalgraph/db/fuseki_postgresql/postgresql_db_impl.py
from ..db_inf import DbImplInterface
from .postgresql_schema import FusekiPostgreSQLSchema
import asyncpg
from typing import Dict, Any, Optional, List
import logging

class FusekiPostgreSQLDbImpl(DbImplInterface):
    """
    PostgreSQL database implementation for FUSEKI_POSTGRESQL hybrid backend.
    
    Provides admin tables + per-space archival tables.
    Primary RDF storage is in Fuseki datasets.
    PostgreSQL provides backup/archival and admin metadata.
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.connection_pool: Optional[asyncpg.Pool] = None
        self.schema = FusekiPostgreSQLSchema()
        self.logger = logging.getLogger(__name__)
    
    async def initialize(self) -> bool:
        """Initialize PostgreSQL connection pool and admin schema."""
        try:
            # Create connection pool
            self.connection_pool = await asyncpg.create_pool(
                host=self.config['host'],
                port=self.config['port'],
                database=self.config['database'],
                user=self.config['user'],
                password=self.config['password'],
                min_size=2,
                max_size=10
            )
            
            # Initialize admin schema
            await self._create_admin_tables()
            await self._create_admin_indexes()
            
            self.logger.info("FusekiPostgreSQLDbImpl initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to initialize FusekiPostgreSQLDbImpl: {e}")
            return False
    
    async def create_space(self, space_id: str, space_name: str) -> bool:
        """Create space in admin tables and create per-space archival tables."""
        try:
            async with self.connection_pool.acquire() as conn:
                async with conn.transaction():
                    # Insert space into admin table
                    await conn.execute(
                        "INSERT INTO space (space_id, space_name) VALUES ($1, $2)",
                        space_id, space_name
                    )
                    
                    # Create per-space archival tables
                    space_tables = self.schema.get_space_tables(space_id)
                    for table_name, table_sql in space_tables.items():
                        await conn.execute(table_sql)
                    
                    self.logger.info(f"Created space {space_id} with archival tables")
                    return True
                    
        except Exception as e:
            self.logger.error(f"Failed to create space {space_id}: {e}")
            return False
    
    async def delete_space(self, space_id: str) -> bool:
        """Delete space from admin tables and drop per-space archival tables."""
        try:
            async with self.connection_pool.acquire() as conn:
                async with conn.transaction():
                    # Drop per-space tables
                    await conn.execute(f"DROP TABLE IF EXISTS {space_id}_rdf_quad CASCADE")
                    await conn.execute(f"DROP TABLE IF EXISTS {space_id}_term CASCADE")
                    
                    # Delete from admin tables (cascades to graph table)
                    await conn.execute("DELETE FROM space WHERE space_id = $1", space_id)
                    
                    self.logger.info(f"Deleted space {space_id} and archival tables")
                    return True
                    
        except Exception as e:
            self.logger.error(f"Failed to delete space {space_id}: {e}")
            return False
    
    async def _create_admin_tables(self) -> None:
        """Create admin tables if they don't exist."""
        admin_tables = self.schema.get_admin_tables()
        
        async with self.connection_pool.acquire() as conn:
            for table_name, table_sql in admin_tables.items():
                await conn.execute(table_sql.replace("CREATE TABLE", "CREATE TABLE IF NOT EXISTS"))
    
    async def _create_admin_indexes(self) -> None:
        """Create admin table indexes if they don't exist."""
        admin_indexes = self.schema.get_admin_indexes()
        
        async with self.connection_pool.acquire() as conn:
            for index_group, indexes in admin_indexes.items():
                for index_sql in indexes:
                    try:
                        await conn.execute(index_sql.replace("CREATE INDEX", "CREATE INDEX IF NOT EXISTS"))
                    except Exception as e:
                        # Index might already exist, continue
                        self.logger.debug(f"Index creation skipped: {e}")
```

## Multi-Dataset Fuseki Architecture Implementation

### Core Architecture Changes

```python
class FusekiSpaceImpl(SpaceBackendInterface):
    """
    Fuseki implementation using separate datasets per space.
    
    Architecture:
    - Admin dataset: vitalgraph_admin (spaces, graphs, users metadata)
    - Space datasets: vitalgraph_space_{space_id} (RDF data per space)
    """
    
    def __init__(self, server_url: str, admin_dataset: str = 'vitalgraph_admin', 
                 dataset_prefix: str = 'vitalgraph_space_', **kwargs):
        self.server_url = server_url
        self.admin_dataset = admin_dataset
        self.dataset_prefix = dataset_prefix
        
        # Admin dataset endpoints
        self.admin_query_url = f"{server_url}/{admin_dataset}/sparql"
        self.admin_update_url = f"{server_url}/{admin_dataset}/update"
        
    def _get_space_dataset_name(self, space_id: str) -> str:
        """Get dataset name for a specific space."""
        return f"{self.dataset_prefix}{space_id}"
    
    def _get_space_endpoints(self, space_id: str) -> tuple:
        """Get SPARQL endpoints for a specific space dataset."""
        dataset_name = self._get_space_dataset_name(space_id)
        query_url = f"{self.server_url}/{dataset_name}/sparql"
        update_url = f"{self.server_url}/{dataset_name}/update"
        return query_url, update_url
```

### Admin Dataset Schema (RDF-based)

        Following PostgreSQL pattern for consistency:

        ```python
# Admin dataset ontology (similar to PostgreSQL tables)
ADMIN_ONTOLOGY = {
    'install': 'http://vital.ai/admin/Install',
    'space': 'http://vital.ai/admin/Space', 
    'user': 'http://vital.ai/admin/User',
    'graph': 'http://vital.ai/admin/Graph'
}

# Install metadata (equivalent to PostgreSQL Install table)
class AdminInstall:
    """RDF representation of installation metadata."""
    rdf_type = 'http://vital.ai/admin/Install'
    properties = ['install_datetime', 'update_datetime', 'active']

# Space registry (equivalent to PostgreSQL Space table)  
class AdminSpace:
    """RDF representation of space metadata."""
    rdf_type = 'http://vital.ai/admin/Space'
    properties = ['space_id', 'space_name', 'space_description', 'tenant', 'update_time']

# User management (equivalent to PostgreSQL User table)
class AdminUser:
    """RDF representation of user metadata."""
    rdf_type = 'http://vital.ai/admin/User'
    properties = ['username', 'password', 'email', 'tenant', 'update_time']

# Graph tracking within spaces
class AdminGraph:
    """RDF representation of graph metadata within spaces."""
    rdf_type = 'http://vital.ai/admin/Graph'
    properties = ['space_id', 'graph_uri', 'graph_name', 'created_time', 'triple_count']
```

        ### Backend Initialization Implementation

        Initialize VitalGraph with multi-dataset Fuseki backend:

        ```python
async def initialize_fuseki_backend():
    """
    Initialize VitalGraph with multi-dataset Fuseki backend.
    
    Steps:
    1. Create admin dataset (vitalgraph_admin)
    2. Initialize admin dataset with RDF schema
    3. Create initial install record
    4. Validate connectivity to all endpoints
    5. Set up default admin user
    """
    
    # 1. Create admin dataset
    dataset_manager = FusekiDatasetManager(server_url, admin_user, admin_password)
    await dataset_manager.create_dataset("vitalgraph_admin")
    
    # 2. Initialize admin dataset schema
    admin_dataset = FusekiAdminDataset(server_url)
    await admin_dataset.initialize_admin_dataset()
    
    # 3. Create install record
    await admin_dataset.create_install_record()
    
    # 4. Validate all endpoints
    await validate_fuseki_setup()
```

## Environment Configuration Examples

### Development Configuration
```yaml
# vitalgraphdb-config-local.yaml
backend:
  type: fuseki
fuseki:
  server_url: http://localhost:3030
  dataset_name: vitalgraph_dev
  username: admin
  password: admin
```

### Staging Configuration
```yaml
# vitalgraphdb-config-staging.yaml  
backend:
  type: fuseki
fuseki:
  server_url: http://fuseki-staging.internal:3030
  dataset_name: vitalgraph_staging
  username: ${FUSEKI_USERNAME}
  password: ${FUSEKI_PASSWORD}
```

### Production Configuration
```yaml
# vitalgraphdb-config-production.yaml
backend:
  type: fuseki
fuseki:
  server_url: http://fuseki-prod.internal:3030
  dataset_name: vitalgraph_prod
  username: ${FUSEKI_USERNAME}
  password: ${FUSEKI_PASSWORD}
```

## Risk Assessment & Mitigation

### Technical Risks

**1. Performance at Scale**
- Risk: Fuseki may not perform as well as PostgreSQL for complex queries
- Mitigation: Performance testing, query optimization, caching strategies

**2. Data Consistency**
- Risk: HTTP-based operations may have consistency issues
- Mitigation: Proper transaction handling, validation, retry logic

**3. Backup & Recovery**
- Risk: Fuseki backup procedures differ from PostgreSQL
- Mitigation: Automated backup scripts, recovery testing, documentation

### Operational Risks

**1. Deployment Complexity**
- Risk: ECS deployment may be more complex with Fuseki
- Mitigation: Comprehensive testing, documentation, rollback procedures

**2. Monitoring Gaps**
- Risk: Different monitoring requirements for Fuseki vs PostgreSQL
- Mitigation: Comprehensive monitoring setup, alerting, runbooks

## Success Criteria

### Functional Requirements
- âœ… All SpaceBackendInterface methods implemented and tested
- âœ… Admin space infrastructure working
- âœ… Docker Compose local development environment
- âœ… Comprehensive test suite passing
- âœ… ECS deployment successful

### Performance Requirements
- Query response times within 2x of PostgreSQL backend
- Support for 1000+ concurrent connections
- Bulk data loading at acceptable speeds
- Memory usage within reasonable bounds

### Operational Requirements
- Automated deployment pipeline
- Comprehensive monitoring and alerting
- Backup and recovery procedures tested
- Documentation complete and up-to-date

## Implementation Timeline

### Week 1-2: Architectural Redesign
- Complete multi-dataset architecture implementation
- Redesign FusekiSpaceImpl for separate datasets per space
- Implement admin dataset with PostgreSQL-equivalent schema
- Create Fuseki dataset management via HTTP Admin API

### Week 3: Testing & Integration
- Update all existing tests for multi-dataset architecture
- Comprehensive unit and integration tests
- Docker Compose environment setup for multi-dataset
- Performance testing and optimization

### Week 4-5: Production Deployment
- ECS deployment configuration for multi-dataset architecture
- Monitoring and observability setup
- Backup and recovery procedures for multiple datasets
- Documentation and runbooks

**Total Estimated Time: 5 weeks** (increased due to architectural redesign)
