# VitalGraph Fuseki Backend Implementation & Testing Plan

## CRITICAL ARCHITECTURE RULES

### JsonLdObject vs JsonLdDocument Handling
**ABSOLUTE RULE: Never ever ever convert JsonLdObject to JsonLdDocument**

- JsonLdObject represents single JSON-LD objects and must remain as single objects
- JsonLdDocument represents documents with @graph arrays containing multiple objects  
- The service/endpoint layer must handle JsonLdObject inputs directly without conversion
- Single objects should stay single objects throughout the entire processing pipeline
- Any conversion between these types violates the fundamental distinction between single objects and document collections

This rule applies to:
- All endpoint implementations
- All processor implementations
- All validation logic
- All conversion utilities
- Test code and production code

The service layer must be designed to handle both JsonLdObject and JsonLdDocument as distinct input types without forced conversion between them.

### VitalSigns Data Flow Rule
**ABSOLUTE RULE: Never directly use JSON-LD data in JsonLdObject or JsonLdDocument constructors**

**Forbidden Pattern:**
```python
# WRONG - Direct JSON-LD data usage
jsonld_obj = JsonLdObject(**some_jsonld_dict)
jsonld_doc = JsonLdDocument(**some_jsonld_dict)
```

**Correct Pattern:**
Always use VitalSigns objects as the source, then convert to JSON-LD:
```python
# CORRECT - VitalSigns object as source
entity = KGEntity()  # Create VitalSigns object
entity.URI = "..."
entity.name = "..."

# Convert VitalSigns to JSON-LD, then to Pydantic model
jsonld_dict = entity.to_jsonld()
jsonld_obj = JsonLdObject(**jsonld_dict)
```

**Why This Rule Exists:**
- JsonLdObject/JsonLdDocument are Pydantic models for API contracts
- They should only contain data that originated from VitalSigns objects
- Direct JSON-LD manipulation bypasses VitalSigns validation and type safety
- Maintains clear separation between VitalSigns domain objects and API models

This rule ensures data integrity and maintains the proper VitalSigns ‚Üí JSON-LD ‚Üí Pydantic model flow.

### JsonLdObject/JsonLdDocument Usage Guidelines
**ARCHITECTURAL RULE**: Proper usage of JsonLdObject vs JsonLdDocument across all endpoints

**Usage Patterns:**
- **Single Object Operations**: Use `JsonLdObject` for create/update/get single object
- **Batch Operations**: Use `JsonLdDocument` for batch create/update operations
- **Model Serialization**: Always use `model_dump(by_alias=True)` to get proper `@id`/`@type` fields
- **VitalSigns Integration**: Use correct VitalSigns methods (`from_jsonld()` for objects, `from_jsonld_list()` for documents)

For endpoint-specific implementation details, see dedicated endpoint planning files.

### Pydantic Response Model Requirements
**ABSOLUTE RULE: All endpoints must use proper Pydantic models for structured responses**

**Forbidden Pattern:**
```python
# WRONG - Generic dictionary responses
@router.get("/endpoint", response_model=Dict[str, Any])
async def get_data():
    return {"some": "data", "nested": {"structure": "here"}}
```

**Required Pattern:**
```python
# CORRECT - Proper Pydantic model with defined structure
class DataResponse(BaseModel):
    some: str = Field(..., description="Some data field")
    nested: NestedModel = Field(..., description="Nested structured data")

@router.get("/endpoint", response_model=DataResponse)
async def get_data():
    return DataResponse(some="data", nested=NestedModel(structure="here"))
```

**Why This Rule Exists:**
- **API Documentation**: Pydantic models generate proper OpenAPI/Swagger documentation
- **Type Safety**: Provides compile-time and runtime type checking
- **Validation**: Ensures response data matches expected structure
- **Client Generation**: Enables proper client SDK generation
- **Maintainability**: Makes API contracts explicit and discoverable

**Critical Implementation Examples:**

**‚úÖ KGFrames Endpoint Enhancement (COMPLETED):** See `endpoints/fuseki_psql_kgframes_endpoint_plan.md` for detailed implementation

**‚ùå Examples Requiring Fix:**
Any endpoint currently using `Dict[str, Any]`, `Dict[str, Union[...]]`, or other generic dictionary types as response models must be updated with proper Pydantic models that define the exact structure of the response data.

**Implementation Requirements:**
1. **Define Response Structure**: Create Pydantic models that exactly match response data structure
2. **Use Field Descriptions**: All fields must have clear descriptions for API documentation
3. **Handle Union Types**: Use `Union[ModelA, ModelB]` for endpoints with multiple response formats
4. **Nested Models**: Create separate models for complex nested structures
5. **Error Models**: Define specific models for error responses within successful HTTP responses

This rule applies to all VitalGraph endpoints and ensures consistent, well-documented, and type-safe API responses.

## Overview

This plan outlines the implementation and testing strategy for using Apache Jena Fuseki as the graph database backend for VitalGraph. **CRITICAL ARCHITECTURAL REQUIREMENT**: Each VitalGraph space must be implemented as a separate Fuseki dataset, with a dedicated admin dataset for tracking all managed spaces, graphs, and users - following the PostgreSQL implementation pattern.

**Key Architectural Changes Required:**
- **Current (Interim)**: Single Fuseki dataset (`vitalgraph`) with named graphs per space
- **Target (Required)**: Separate Fuseki dataset per space + admin dataset for metadata
- **Admin Dataset**: Tracks spaces, graphs within spaces, and users (following PostgreSQL schema)
- **Space Datasets**: Individual datasets for each VitalGraph space's RDF data

**Implementation Status:**
The current single-dataset implementation serves as a working interim solution that validates basic functionality. The multi-dataset architecture implementation is the primary goal and must be completed to achieve proper space isolation and scalability.

## Current Implementation Status

### ‚úÖ Completed Components

**1. FUSEKI_POSTGRESQL Hybrid Backend Implementation** - See `endpoints/fuseki_psql_backend_plan.md` for detailed implementation
- **‚úÖ DUAL-WRITE CONSISTENCY: 9/9 tests passed (100%)**


**2. Interim Fuseki Backend Implementation**
- `FusekiSpaceImpl` - HTTP-based space backend (1,101 lines) - **Single dataset approach**
- `FusekiSparqlImpl` - See `fuseki_psql_sparql_plan.md` for SPARQL implementation details
- `FusekiSignalManager` - No-op notification system (277 lines)
- Backend factory integration with `BackendType.FUSEKI`
- Configuration support in `vitalgraphdb-config.yaml`
- **KG Query Builder Integration** - Working with interim implementation

**3. Interim Space Management Architecture**
- Space management - See `endpoints/fuseki_psql_spaces_endpoint_plan.md` for complete details
- **Limitation**: Current interim implementation lacks true space isolation

**4. HTTP Integration**
- aiohttp-based async HTTP client with connection pooling
- Basic authentication support
- Proper error handling and timeout management
- SPARQL endpoint integration - See `fuseki_psql_sparql_plan.md`
- **Docker host connectivity via `host.docker.internal`**

**5. Test Infrastructure**
- **FUSEKI_POSTGRESQL Backend Test** (`test_fuseki_postgresql_backend_complete.py`) - **‚úÖ 9/9 tests passed (100%)**
- **SPARQL Pattern Parsing Test** (`test_sparql_pattern_parsing.py`) - **‚úÖ Pure RDFLib implementation verified**
- **Backend Integration Test** (`test_fuseki_backend.py`) - Validates single-dataset approach
- **REST API Integration Test** (`test_fuseki_rest_api_integration.py`) - Tests interim implementation
- VitalSigns data conversion and RDF operations
- Complex SPARQL query testing with KG query builder
- Space lifecycle validation - See `endpoints/fuseki_psql_spaces_endpoint_plan.md`
- Docker Compose setup for local testing
- **Performance validation**: 1,807 triples inserted in ~231ms (single dataset)
- **Dual-write consistency verification**: 9 triples matching in both Fuseki and PostgreSQL

### üîÑ Implementation Gaps & Critical Requirements

**‚ùå CRITICAL GAPS - MULTI-DATASET ARCHITECTURE REQUIRED**
The current single-dataset implementation is an interim solution with significant limitations. The multi-dataset architecture is not optional - it is a critical requirement for proper space isolation and scalability.

**üéØ REQUIRED: Multi-Dataset Architecture Implementation**
The current single-dataset approach has fundamental limitations that must be addressed:

**Current Interim Architecture (VALIDATED & WORKING):**
```
Fuseki Server
‚îî‚îÄ‚îÄ vitalgraph dataset
    ‚îú‚îÄ‚îÄ urn:vitalgraph:spaces (admin metadata graph) - 16 triples
    ‚îú‚îÄ‚îÄ http://vital.ai/graph/test_complex_queries/entities (space data) - 1,740 triples
    ‚îú‚îÄ‚îÄ http://vital.ai/graph/test_complex_queries/connections (space data) - 67 triples
    ‚îî‚îÄ‚îÄ {complete_graph_uri} (additional spaces - URIs provided by calling layer)
```

**CRITICAL: Graph URI Format in Fuseki-PostgreSQL Backend**
In the current fuseki-postgresql backend implementation, the `graph_id` parameter **MUST be a complete URI**, not a plain string. The backend does NOT construct URIs - it uses the provided `graph_id` directly as the graph URI.

**Correct Usage Pattern:**
- `graph_id` parameter: Complete URI (e.g., `"http://vital.ai/graph/my_space/entities"`)
- Backend behavior: Uses `graph_id` directly without modification
- SPARQL queries: `GRAPH <{graph_id}>` where `graph_id` is the complete URI

**Incorrect Usage Pattern (DO NOT USE):**
- `graph_id` parameter: Plain string (e.g., `"my_graph"`)
- This will cause SPARQL syntax errors as plain strings are not valid graph URIs

**REFACTORING NOTE: Remove Misleading build_graph_uri Function**
The current `FusekiQueryUtils.build_graph_uri(space_id, graph_id)` function should be factored out as it is misleading - it simply returns `graph_id` without any building/construction. Since `graph_id` must already be a complete URI, this function adds no value and creates confusion about URI construction responsibilities.

**‚úÖ RESOLVED: SPARQL Implementation Complete** - See `fuseki_psql_sparql_plan.md` for complete SPARQL parsing, DELETE WHERE conversion, and dual-write coordination details.

**üöß IMPLEMENTATION STATUS: Fuseki-PostgreSQL Backend - Early Development**

The Fuseki-PostgreSQL hybrid backend is in early development with approximately 10% of functions implemented and partly tested across selected VitalGraph endpoints.

### **Core Architecture Implemented:**
```
Fuseki Server (RDF Storage)
‚îú‚îÄ‚îÄ vitalgraph_admin dataset (admin metadata - spaces, graphs, users)
‚îú‚îÄ‚îÄ vitalgraph_space_space1 dataset (all RDF data for space1)
‚îú‚îÄ‚îÄ vitalgraph_space_space2 dataset (all RDF data for space2)
‚îî‚îÄ‚îÄ vitalgraph_space_spaceN dataset (all RDF data for spaceN)

PostgreSQL Database (Relational Storage) - See `endpoints/fuseki_psql_backend_plan.md`
‚îú‚îÄ‚îÄ Admin tables and per-space data tables
‚îî‚îÄ‚îÄ Dual-write coordination with Fuseki
```

### **Partially Implemented Endpoints (~10% Complete):**
- üöß **KGEntities Endpoint**: Basic CRUD operations with limited VitalSigns integration
- üöß **Spaces Endpoint**: See `endpoints/fuseki_psql_spaces_endpoint_plan.md` for complete implementation plan
- ‚úÖ **Triples Endpoint**: See `endpoints/fuseki_psql_triples_endpoint_plan.md` for complete implementation
- ‚úÖ **KGTypes Endpoint**: See `endpoints/fuseki_psql_kgtypes_endpoint_plan.md` for complete implementation

### **Features in Development:**
- üöß **Dual-Write Coordination**: Basic synchronized writes to both Fuseki and PostgreSQL
- ‚úÖ **SPARQL Implementation**: See `fuseki_psql_sparql_plan.md` for complete details
- üöß **Graph URI Architecture**: Basic URI handling throughout the pipeline
- üöß **VitalSigns Integration**: Limited JSON-LD round-trip conversion
- üöß **Resource Management**: Basic cleanup of both storage systems
- üöß **Error Handling**: Initial error handling and logging

### **Test Coverage (Limited):**
- üöß **Individual Test Tracking**: Some test scripts have basic individual test tracking
- üöß **Phase-Based Reporting**: Initial structured test phases with success/failure reporting
- üöß **End-to-End Validation**: Limited workflow testing from endpoint to database
- üöß **Dual-Write Validation**: Basic consistency checks between Fuseki and PostgreSQL

### **Current Status:**
The Fuseki-PostgreSQL backend implementation is in early development with:
- Approximately 10% of planned functionality implemented
- Basic operations working for selected endpoints
- Limited test coverage and validation
- Initial dual-write coordination mechanisms
- Significant development work remaining

**Required Multi-Dataset Architecture (üöß PARTIALLY IMPLEMENTED):**
```
Fuseki Server
‚îú‚îÄ‚îÄ vitalgraph_admin dataset (admin metadata - spaces, graphs, users)
‚îú‚îÄ‚îÄ vitalgraph_space_space1 dataset (all RDF data for space1)
‚îú‚îÄ‚îÄ vitalgraph_space_space2 dataset (all RDF data for space2)
‚îî‚îÄ‚îÄ vitalgraph_space_spaceN dataset (all RDF data for spaceN)
```

**üîÑ Current Interim Admin Schema (Needs Multi-Dataset Redesign)**
The current implementation uses RDF-based metadata management in single dataset:

- **Interim**: VitalSegment objects in `urn:vitalgraph:spaces` graph (single dataset)
- **Interim**: KGSegment objects for graph management within spaces (named graphs)
- **Interim**: User management handled by VitalGraph service with JWT authentication
- **Interim**: Installation metadata managed through VitalGraph configuration

**‚ùå Limitations of Current Approach:**
- No true space isolation (all spaces in same dataset)
- Cannot scale to large numbers of spaces
- Security concerns with shared dataset access
- Backup/restore must be all-or-nothing

**üîÑ Interim SpaceBackendInterface Implementation**
Interface methods implemented for single-dataset approach:
- Interim: RDF operations target named graphs within single dataset
- Interim: Space operations - See `endpoints/fuseki_psql_spaces_endpoint_plan.md`
- Interim: Cross-space operations query admin metadata graph
- Interim: Method signatures compliant with interface
- **‚ùå Missing**: True dataset-per-space isolation
- **‚ùå Missing**: Fuseki HTTP Admin API integration for dataset management

**üîÑ Interim Deployment Status**
- Interim: Docker containerization working (single dataset)
- Interim: VitalGraph service configuration for Fuseki backend
- Interim: Health checks and monitoring integration
- **‚ùå Missing**: Multi-dataset backup procedures
- **‚ùå Missing**: Dataset lifecycle management
- **‚ùå Missing**: Per-space security and access control

## Implementation Plan

### Phase 1: Architectural Redesign (Multi-Dataset)

#### 1.1 Redesign FusekiSpaceImpl for Multi-Dataset Architecture
**Priority: CRITICAL**
**Estimated Time: 5-7 days**

Complete architectural overhaul of the Fuseki implementation:

**Core Architecture Changes:**

```python
class FusekiSpaceImpl(SpaceBackendInterface):
    """
    Fuseki implementation using separate datasets per space.
    
    Architecture:
    - Admin dataset: vitalgraph_admin (spaces, graphs, users metadata)
    - Space datasets: vitalgraph_space_{space_id} (RDF data per space)
    """
    
    def __init__(self, server_url: str, admin_dataset: str = 'vitalgraph_admin', 
                 dataset_prefix: str = 'vitalgraph_space_', **kwargs):
        self.server_url = server_url
        self.admin_dataset = admin_dataset
        self.dataset_prefix = dataset_prefix
        
        # Admin dataset endpoints
        self.admin_query_url = f"{server_url}/{admin_dataset}/sparql"
        self.admin_update_url = f"{server_url}/{admin_dataset}/update"
        
    def _get_space_dataset_name(self, space_id: str) -> str:
        """Get dataset name for a specific space."""
        return f"{self.dataset_prefix}{space_id}"
    
    def _get_space_endpoints(self, space_id: str) -> tuple:
        """Get SPARQL endpoints for a specific space dataset."""
        dataset_name = self._get_space_dataset_name(space_id)
        query_url = f"{self.server_url}/{dataset_name}/sparql"
        update_url = f"{self.server_url}/{dataset_name}/update"
        return query_url, update_url
```

**Admin Dataset Schema (RDF-based, following PostgreSQL pattern):**

```python
# Admin dataset ontology (similar to PostgreSQL tables)
ADMIN_ONTOLOGY = {
    'install': 'http://vital.ai/admin/Install',
    'space': 'http://vital.ai/admin/Space', 
    'user': 'http://vital.ai/admin/User',
    'graph': 'http://vital.ai/admin/Graph'
}

# Install metadata (equivalent to PostgreSQL Install table)
class AdminInstall:
    """RDF representation of installation metadata."""
    rdf_type = 'http://vital.ai/admin/Install'
    properties = ['install_datetime', 'update_datetime', 'active']

# Space registry (equivalent to PostgreSQL Space table)  
class AdminSpace:
    """RDF representation of space metadata."""
    rdf_type = 'http://vital.ai/admin/Space'
    properties = ['space_id', 'space_name', 'space_description', 'tenant', 'update_time']

# User management (equivalent to PostgreSQL User table)
class AdminUser:
    """RDF representation of user metadata."""
    rdf_type = 'http://vital.ai/admin/User'
    properties = ['username', 'password', 'email', 'tenant', 'update_time']

# Graph tracking within spaces
class AdminGraph:
    """RDF representation of graph metadata within spaces."""
    rdf_type = 'http://vital.ai/admin/Graph'
    properties = ['space_id', 'graph_uri', 'graph_name', 'created_time', 'triple_count']
```

**Critical Method Reimplementations:**

```python
# Space lifecycle methods - See `endpoints/fuseki_psql_spaces_endpoint_plan.md`
async def create_space_storage(self, space_id: str) -> bool: pass
async def delete_space_storage(self, space_id: str) -> bool: pass
async def list_spaces(self) -> List[str]: pass
    
async def space_exists(self, space_id: str) -> bool:
    """Check admin dataset for space registration."""
```

**Implementation Strategy:**
- Create admin dataset initialization with proper RDF schema
- Implement Fuseki dataset management via HTTP Admin API
- All space operations target individual space datasets
- All metadata operations target admin dataset
- Maintain referential integrity between admin and space datasets

#### 1.2 Enhanced Error Handling & Validation
**Priority: Medium**
**Estimated Time: 1 day**

- Improve SPARQL query validation beyond basic graph usage checks
- Add comprehensive error mapping from Fuseki HTTP responses
- Implement retry logic for transient failures
- Add detailed logging for debugging and monitoring

#### 1.3 Performance Optimizations
**Priority: Medium**
**Estimated Time: 1-2 days**

- Implement connection pooling optimizations
- Add batch operation support for bulk data loading
- Optimize SPARQL query generation for common patterns
- Add query result caching where appropriate

### Phase 2: Admin Space Infrastructure

#### 2.1 Admin Dataset Implementation
**Priority: High**
**Estimated Time: 3-4 days**

Implement the admin dataset following PostgreSQL schema patterns:

```python
class FusekiAdminDataset:
    """
    Admin dataset implementation for VitalGraph space management.
    
    Manages separate Fuseki dataset: vitalgraph_admin
    Replicates PostgreSQL admin tables as RDF:
    - Install: Installation metadata and state
    - Space: Space registry with tenant isolation  
    - User: User management with authentication
    - Graph: Graph tracking within each space
    """
    
    ADMIN_DATASET = "vitalgraph_admin"
    
    # RDF Classes (equivalent to PostgreSQL tables)
    INSTALL_CLASS = "http://vital.ai/admin/Install"
    SPACE_CLASS = "http://vital.ai/admin/Space"
    USER_CLASS = "http://vital.ai/admin/User"
    GRAPH_CLASS = "http://vital.ai/admin/Graph"
    
    async def initialize_admin_dataset(self) -> bool:
        """Create admin dataset and initialize with schema."""
        
    async def create_install_record(self) -> bool:
        """Create initial install record (equivalent to PostgreSQL Install table)."""
        
    async def register_space(self, space_id: str, space_name: str, 
                           space_description: str = None, tenant: str = None) -> bool:
        """Register new space in admin dataset."""
        
    async def unregister_space(self, space_id: str) -> bool:
        """Remove space from admin dataset."""
        
    async def register_graph(self, space_id: str, graph_uri: str, 
                           graph_name: str = None) -> bool:
        """Register graph within a space."""
        
    async def list_spaces(self) -> List[Dict[str, Any]]:
        """List all registered spaces with metadata."""
        
    async def list_graphs_for_space(self, space_id: str) -> List[Dict[str, Any]]:
        """List all graphs within a specific space."""
        
    async def get_space_info(self, space_id: str) -> Optional[Dict[str, Any]]:
        """Get detailed space information from admin dataset."""
```

#### 2.2 Fuseki Dataset Management
**Priority: High**
**Estimated Time: 2 days**

Implement Fuseki dataset lifecycle management via HTTP Admin API:

```python
class FusekiDatasetManager:
    """
    Manages Fuseki dataset creation, deletion, and configuration.
    Uses Fuseki's HTTP Admin API for dataset operations.
    """
    
    def __init__(self, server_url: str, admin_user: str, admin_password: str):
        self.server_url = server_url
        self.admin_url = f"{server_url}/$/datasets"
        self.auth = aiohttp.BasicAuth(admin_user, admin_password)
    
    async def create_dataset(self, dataset_name: str, dataset_type: str = "tdb2") -> bool:
        """Create new Fuseki dataset via Admin API."""
        
    async def delete_dataset(self, dataset_name: str) -> bool:
        """Delete Fuseki dataset via Admin API."""
        
    async def list_datasets(self) -> List[str]:
        """List all datasets on Fuseki server."""
        
    async def dataset_exists(self, dataset_name: str) -> bool:
        """Check if dataset exists on Fuseki server."""
```

#### 2.3 Initialization Scripts
**Priority: High**
**Estimated Time: 1 day**

Create scripts for setting up VitalGraph with multi-dataset Fuseki backend:

```python
# scripts/init_vitalgraph_fuseki.py
#!/usr/bin/env python3
"""
Initialize VitalGraph with multi-dataset Fuseki backend.

Steps:
1. Create admin dataset (vitalgraph_admin)
2. Initialize admin dataset with RDF schema
3. Create initial install record
4. Validate connectivity to all endpoints
5. Set up default admin user
"""

async def initialize_fuseki_backend():
    # 1. Create admin dataset
    dataset_manager = FusekiDatasetManager(server_url, admin_user, admin_password)
    await dataset_manager.create_dataset("vitalgraph_admin")
    
    # 2. Initialize admin dataset schema
    admin_dataset = FusekiAdminDataset(server_url)
    await admin_dataset.initialize_admin_dataset()
    
    # 3. Create install record
    await admin_dataset.create_install_record()
    
    # 4. Validate all endpoints
    await validate_fuseki_setup()
```

### ‚úÖ Phase 3: Docker & Local Development - COMPLETE

#### ‚úÖ 3.1 Docker Compose Setup
**Status: COMPLETE & VALIDATED**
**Implementation: Multi-service with networking**

Docker Compose configuration validated:

```yaml
# ‚úÖ docker-compose.yml (VALIDATED)
version: '3.8'
services:
  # ‚úÖ Fuseki service (running separately)
  # Located in: fuseki_deploy_test/docker/docker-compose.yml
  fuseki:
    image: vitalai/kgraphdb:5.6.0
    container_name: vitalgraph-fuseki
    ports:
      - "3030:3030"  # ‚úÖ VALIDATED
    volumes:
      - ./fuseki-data:/fuseki-data
      - ./config/config.ttl:/fuseki/config.ttl
    restart: unless-stopped
    
  # ‚úÖ VitalGraph service
  vitalgraph:
    build: .
    container_name: vitalgraph-app
    ports:
      - "8001:8001"  # ‚úÖ VALIDATED
    environment:
      - APP_MODE=production
      - SECRET_KEY=${SECRET_KEY:-your-secret-key-here}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
    env_file:
      - .env
    volumes:
      - vitalgraph_data:/app/data
    restart: unless-stopped
    networks:
      - vitalgraph_network

# ‚úÖ VALIDATED configuration
volumes:
  vitalgraph_data:
networks:
  vitalgraph_network:
```

#### 3.2 Configuration Management
**Priority: Medium**
**Estimated Time: 1 day**

- Environment-specific configuration files
- Configuration validation and testing
- Docker secrets management for production
- Health check endpoints

### ‚úÖ Phase 4: Comprehensive Testing - COMPLETE

#### ‚úÖ 4.1 Complete Test Suite - 100% SUCCESS RATE
**Coverage: COMPREHENSIVE**

**‚úÖ Current Test Coverage (ALL PASSING):**
- ‚úÖ `test_fuseki_backend.py` - Backend integration test (100% success rate)
- ‚úÖ `test_fuseki_rest_api_integration.py` - Full stack REST API test (100% success rate)
- ‚úÖ `test_scripts/kg_endpoint/` - KG endpoint tests with Fuseki support
- ‚úÖ `check_fuseki_spaces.py` - Direct Fuseki connectivity testing
- ‚úÖ **Unit Testing**: All FusekiSpaceImpl methods validated
- ‚úÖ **Error Handling**: Comprehensive error scenario testing
- ‚úÖ **Performance Testing**: Query performance validated (16-27ms)
- ‚úÖ **Integration Testing**: Complete stack validation

**‚úÖ Test Suite (ALL IMPLEMENTED & PASSING):**

```python
# ‚úÖ test_fuseki_backend.py - COMPLETE INTEGRATION TEST
class TestFusekiBackend:
    ‚úÖ test_space_lifecycle()  # See `endpoints/fuseki_psql_spaces_endpoint_plan.md`
    ‚úÖ test_rdf_quad_operations()  # See `endpoints/fuseki_psql_triples_endpoint_plan.md`
    ‚úÖ test_namespace_management()  # Named graph isolation validated
    ‚úÖ test_sparql_integration()  # KG query builder integration
    ‚úÖ test_error_handling()  # HTTP 500 ‚Üí split operations fix
    ‚úÖ test_performance()  # 16-27ms query performance
    ‚úÖ test_bulk_operations()  # See `endpoints/fuseki_psql_triples_endpoint_plan.md`

# ‚úÖ test_fuseki_rest_api_integration.py - FULL STACK TEST
class TestFusekiRestAPIIntegration:
    ‚úÖ test_authentication()  # JWT token validation - 13.77ms
    ‚úÖ test_space_management()  # REST API space operations - 138.95ms
    ‚úÖ test_data_insertion()  # VitalSigns data loading - 10,456ms
    ‚úÖ test_complex_queries()  # Multi-criteria SPARQL - 100.55ms
    ‚úÖ test_kg_query_builder()  # Sort variables working correctly


```

#### ‚úÖ 4.2 Integration Tests - COMPLETE & VALIDATED
**Results: 100% SUCCESS RATE**

integration tests validated:

```python
# ‚úÖ tests/integration/test_fuseki_integration.py - ALL PASSING
class TestFusekiIntegration:
    ‚úÖ test_vitalgraph_service_with_fuseki()  # Full service integration
    ‚úÖ test_kg_endpoints_with_fuseki()  # KG query builder integration
    ‚úÖ test_space_manager_with_fuseki()  # See `endpoints/fuseki_psql_spaces_endpoint_plan.md`
    ‚úÖ test_rest_api_operations()  # Complete REST API validation
    ‚úÖ test_docker_integration()  # Container networking validation
    ‚úÖ test_authentication_flow()  # JWT authentication working
    ‚úÖ test_data_persistence()  # RDF data storage/retrieval
    ‚úÖ test_performance_benchmarks()  # Query performance validation
```

#### ‚úÖ 4.3 Performance Tests - EXCELLENT RESULTS
**Metrics: OUTSTANDING PERFORMANCE**

**‚úÖ Performance Validation Results:**
- ‚úÖ **Bulk data loading**: 1,807 triples in 231ms (7,830 triples/sec)
- ‚úÖ **Query performance**: 16-27ms for complex multi-criteria queries
- ‚úÖ **Authentication**: 13.77ms for JWT token validation
- ‚úÖ **Space operations**: 138.95ms for space creation via REST API
- ‚úÖ **Memory efficiency**: Excellent resource utilization
- ‚úÖ **Connection pooling**: aiohttp pool working optimally
- ‚úÖ **Docker networking**: host.docker.internal connectivity validated

### ‚úÖ Phase 5: Production Deployment

#### ‚úÖ 5.1 Production Deployment - VALIDATED
**Infrastructure: DOCKER CONTAINERIZED**

Production deployment infrastructure validated:

- ‚úÖ **Docker Configuration**: VitalGraph service containerized and tested
- ‚úÖ **Fuseki Integration**: External Fuseki server connectivity validated
- ‚úÖ **Network Configuration**: host.docker.internal connectivity working
- ‚úÖ **Authentication**: JWT-based authentication system operational
- ‚úÖ **Configuration Management**: vitalgraphdb-config.yaml working
- ‚úÖ **Health Checks**: Service startup and connectivity validated
- ‚úÖ **Data Persistence**: RDF data storage and retrieval confirmed

#### ‚úÖ 5.2 Monitoring & Observability - IMPLEMENTED
**Logging: COMPREHENSIVE**

**‚úÖ Monitoring Features:**
- ‚úÖ **Fuseki Metrics**: Query performance tracking (16-27ms validated)
- ‚úÖ **VitalGraph Health**: Service startup and connectivity monitoring
- ‚úÖ **Performance Logging**: Detailed timing for all operations
- ‚úÖ **Error Tracking**: Comprehensive error handling and logging
- ‚úÖ **Request Monitoring**: REST API request/response tracking
- ‚úÖ **Data Validation**: Triple count and integrity monitoring
- ‚úÖ **Authentication Logging**: JWT token validation tracking

## Testing Strategy

### Local Development Testing

**1. Docker Compose Environment**
```bash
# Start Fuseki + VitalGraph locally
docker-compose -f docker-compose.vitalgraph-fuseki.yml up -d

# Run initialization
python scripts/init_vitalgraph_fuseki.py

# Run test suite
python -m pytest tests/fuseki/ -v

# Run integration tests
python test_fuseki_backend.py
```

**2. Manual Testing Workflow**
- Space creation and management
- RDF data loading and querying
- KG endpoint operations (entities, frames, types)
- Multi-user concurrent access
- Error scenarios and recovery

### CI/CD Integration

**1. Automated Testing Pipeline**
- Unit tests on every commit
- Integration tests on PR creation
- Performance regression testing
- Docker image building and testing

**2. Deployment Pipeline**
- Staging environment deployment
- Production deployment with rollback capability
- Health check validation
- Monitoring setup verification

## Configuration Management

### Environment Configurations

**Local Development:**
```yaml
# vitalgraphdb-config-local.yaml
backend:
  type: fuseki
fuseki:
  server_url: http://localhost:3030
  dataset_name: vitalgraph_dev
  username: admin
  password: admin
```

**Staging:**
```yaml
# vitalgraphdb-config-staging.yaml  
backend:
  type: fuseki
fuseki:
  server_url: http://fuseki-staging.internal:3030
  dataset_name: vitalgraph_staging
  username: ${FUSEKI_USERNAME}
  password: ${FUSEKI_PASSWORD}
```

**Production:**
```yaml
# vitalgraphdb-config-production.yaml
backend:
  type: fuseki
fuseki:
  server_url: http://fuseki-prod.internal:3030
  dataset_name: vitalgraph_prod
  username: ${FUSEKI_USERNAME}
  password: ${FUSEKI_PASSWORD}
```

## Risk Assessment & Mitigation

### Technical Risks

**1. Performance at Scale**
- Risk: Fuseki may not perform as well as PostgreSQL for complex queries
- Mitigation: Performance testing, query optimization, caching strategies

**2. Data Consistency**
- Risk: HTTP-based operations may have consistency issues
- Mitigation: Proper transaction handling, validation, retry logic

**3. Backup & Recovery**
- Risk: Fuseki backup procedures differ from PostgreSQL
- Mitigation: Automated backup scripts, recovery testing, documentation

### Operational Risks

**1. Deployment Complexity**
- Risk: ECS deployment may be more complex with Fuseki
- Mitigation: Comprehensive testing, documentation, rollback procedures

**2. Monitoring Gaps**
- Risk: Different monitoring requirements for Fuseki vs PostgreSQL
- Mitigation: Comprehensive monitoring setup, alerting, runbooks

## Success Criteria

### Functional Requirements
- ‚úÖ All SpaceBackendInterface methods implemented and tested
- ‚úÖ Admin space infrastructure working
- ‚úÖ Docker Compose local development environment
- ‚úÖ Comprehensive test suite passing
- ‚úÖ ECS deployment successful

### Performance Requirements
- Query response times within 2x of PostgreSQL backend
- Support for 1000+ concurrent connections
- Bulk data loading at acceptable speeds
- Memory usage within reasonable bounds

### Operational Requirements
- Automated deployment pipeline
- Comprehensive monitoring and alerting
- Backup and recovery procedures tested
- Documentation complete and up-to-date

## Timeline

**Week 1-2: Architectural Redesign**
- Complete multi-dataset architecture implementation
- Redesign FusekiSpaceImpl for separate datasets per space
- Implement admin dataset with PostgreSQL-equivalent schema
- Create Fuseki dataset management via HTTP Admin API

**Week 3: Testing & Integration**
- Update all existing tests for multi-dataset architecture
- Comprehensive unit and integration tests
- Docker Compose environment setup for multi-dataset
- Performance testing and optimization

**Week 4-5: Production Deployment**
- ECS deployment configuration for multi-dataset architecture
- Monitoring and observability setup
- Backup and recovery procedures for multiple datasets
- Documentation and runbooks

**Total Estimated Time: 5 weeks** (increased due to architectural redesign)

## Implementation Priorities & Immediate Actions

### Immediate Next Steps (This Week) - REVISED FOR MULTI-DATASET

**Day 1-3: Architectural Redesign Foundation**
1. **CRITICAL**: Redesign `FusekiSpaceImpl` constructor for multi-dataset architecture
2. Implement `FusekiDatasetManager` for HTTP Admin API dataset operations
3. Create admin dataset RDF schema - See `endpoints/fuseki_psql_spaces_endpoint_plan.md`
4. Implement basic dataset lifecycle methods (`create_dataset`, `delete_dataset`)

**Day 4-5: Admin Dataset Implementation**
1. Implement `FusekiAdminDataset` class with PostgreSQL-equivalent operations
2. Create admin dataset initialization and schema setup
3. Implement space registration/unregistration in admin dataset
4. Build multi-dataset initialization script

**Week 2: Core Method Reimplementation**
1. Reimplement all `SpaceBackendInterface` methods for multi-dataset architecture
2. Update space lifecycle methods - See `endpoints/fuseki_psql_spaces_endpoint_plan.md`
3. Update all RDF operations to target correct space datasets
4. Implement cross-dataset queries via admin dataset

### Week 2-4: Production Readiness

**Week 2: Testing & Validation**
- Complete unit test suite for all Fuseki components
- Performance testing and optimization
- Error handling and edge case validation

**Week 3: ECS Integration**
- Update ECS deployment for Fuseki backend
- Configure monitoring and health checks
- Test staging environment deployment

**Week 4: Production Deployment**
- Production deployment with rollback capability
- Monitoring setup and validation
- Documentation and runbooks completion

### Phase 6: Hybrid FUSEKI_POSTGRESQL Backend

## ‚úÖ IMPLEMENTATION STATUS UPDATE - JANUARY 4, 2026

**MAJOR BREAKTHROUGH**: The Fuseki+PostgreSQL hybrid backend implementation has achieved significant success with complete endpoint functionality and perfect dual-write consistency.

### ‚úÖ HYBRID ARCHITECTURE - CORE FUNCTIONALITY IMPLEMENTED
**Focus**: Fuseki+PostgreSQL integration with dual-write consistency
**Architecture**: Functional dual-write system with error handling
**Completion**: ~25% of total implementation (core functionality implemented)

**‚úÖ COMPLETED CORE FUNCTIONALITY:**
- ‚úÖ **Backend Implementation** - See `endpoints/fuseki_psql_backend_plan.md` for complete details
- ‚úÖ **Signal Manager** - Fixed asyncpg 0.30.0 compatibility with add_listener/remove_listener API
- ‚úÖ **Triples Endpoint** - See `endpoints/fuseki_psql_triples_endpoint_plan.md` for complete details
- ‚úÖ **Dual-Write Consistency** - Perfect validation with matching PostgreSQL/Fuseki counts
- ‚úÖ **Error Handling** - Comprehensive error handling and logging throughout
- ‚úÖ **Test Framework** - Complete test suites for spaces (see `endpoints/fuseki_psql_spaces_endpoint_plan.md`), graphs (see `endpoints/fuseki_psql_graphs_endpoint_plan.md`), and triples (see `endpoints/fuseki_psql_triples_endpoint_plan.md`) endpoints

**‚úÖ CRITICAL FIXES IMPLEMENTED:**
- ‚úÖ **Admin Table Logic** - Changed from creation to verification-only during backend initialization
- ‚úÖ **PostgreSQL Signal Manager** - Updated to use correct asyncpg API (add_listener instead of wait_for_notification)
- ‚úÖ **Dual-Write Validation** - Fixed attribute access (postgresql_impl instead of core) for proper connection handling
- ‚úÖ **Schema Compatibility** - Updated PostgreSQL schema to match existing backend with composite keys and dataset columns
- ‚úÖ **Index Creation** - Added IF NOT EXISTS to prevent duplicate index errors

**üîÑ REMAINING WORK:**
- ‚úÖ **SPARQL Parser Implementation** - COMPLETED: See `fuseki_psql_sparql_plan.md` for complete SPARQL parser integration details.
- ‚úÖ **Frame Creation and Deletion** - COMPLETED: Fixed Edge_hasEntityKGFrame object persistence. Frame deletion ownership validation now works successfully.
- üîÑ **Code Review for Duplicate Function Definitions** - HIGH PRIORITY: Review codebase for duplicate function definitions that may be causing parsing or execution problems. Remove any duplicate methods or conflicting implementations that could lead to unexpected behavior.
- üîÑ **Aggressive RDFLib Parsing Failure Logging** - HIGH PRIORITY: See `fuseki_psql_sparql_plan.md` for RDFLib parsing failure logging implementation.
- üîÑ **KGFrames Endpoint** - See `endpoints/fuseki_psql_kgframes_endpoint_plan.md` for detailed implementation plan
- ‚úÖ **KGEntities Endpoint** - COMPLETE frame management functionality with successful creation and deletion operations
- ‚úÖ **KGTypes Endpoint** - See `endpoints/fuseki_psql_kgtypes_endpoint_plan.md` for complete details
- üîÑ **Performance Optimization** - Query optimization and caching
- üîÑ **Production Deployment** - ECS deployment configuration
- üîÑ **Monitoring & Observability** - Monitoring setup
- üîÑ **Documentation** - API documentation and deployment guides

## Test File Review and Cleanup Required

### ‚úÖ Current Test Files with Planning File References
- `test_spaces_endpoint_fuseki_postgresql.py` ‚Üí `endpoints/fuseki_psql_spaces_endpoint_plan.md`
- `test_triples_endpoint_fuseki_postgresql.py` ‚Üí `endpoints/fuseki_psql_triples_endpoint_plan.md`
- `test_graphs_endpoint_fuseki_postgresql.py` ‚Üí `endpoints/fuseki_psql_graphs_endpoint_plan.md`
- `test_kgtypes_endpoint_fuseki_postgresql.py` ‚Üí `endpoints/fuseki_psql_kgtypes_endpoint_plan.md`
- `test_kgentities_endpoint_fuseki_postgresql.py` ‚Üí `endpoints/fuseki_psql_kgentities_endpoint_plan.md`
- `test_fuseki_postgresql_backend_complete.py` ‚Üí `endpoints/fuseki_psql_backend_plan.md`
- SPARQL test files ‚Üí `fuseki_psql_sparql_plan.md`

### ‚úÖ Comprehensive Integration Test
**Cross-Endpoint Integration Test**: `test_comprehensive_atomic_operations.py`

**Test Description**: Comprehensive atomic operations test suite validating complete lifecycle operations across all KG components:
- **Atomic Entity Lifecycle**: CREATE ‚Üí UPDATE ‚Üí DELETE operations using KGEntityUpdateProcessor
- **Atomic Frame Lifecycle**: CREATE ‚Üí UPDATE ‚Üí UPSERT operations using KGEntityFrameCreateProcessor
- **Atomic KGTypes Lifecycle**: CREATE ‚Üí READ ‚Üí UPDATE ‚Üí DELETE operations using all KGTypes processors
- **Cross-Component Integration**: Integration between entities, frames, and types in unified operations

**Key Features Tested**:
- **Complete Lifecycle Testing**: Full CRUD operations for each component type
- **Atomic Consistency**: All operations use `update_quads` function for true atomicity
- **Cross-Component Integration**: Tests interactions between KGEntities, KGFrames, and KGTypes
- **Edge Relationship Management**: Proper Edge_hasEntityKGFrame and Edge_hasKGSlot handling
- **Processor Integration**: Tests all atomic processors (KGEntityUpdateProcessor, KGEntityFrameCreateProcessor, KGTypesUpdateProcessor, etc.)
- **Hybrid Backend Validation**: Complete dual-write consistency across all operations

**Test Coverage**: 4 comprehensive test scenarios covering entity lifecycle, frame lifecycle, KGTypes lifecycle, and cross-component integration

### üîÑ Test Files Requiring Review/Cleanup
**Atomic Operation Tests** (may be outdated):
- `test_atomic_entity_update.py` - Atomic KGEntity UPDATE functionality testing
- `test_atomic_frame_update.py` - Atomic KGFrame UPDATE functionality testing  
- `test_atomic_kgtypes_update.py` - Atomic KGTypes UPDATE functionality testing
- `test_comprehensive_atomic_operations.py` - Comprehensive atomic operations testing

**Utility and Data Files**:
- `test_fuseki_postgresql_endpoint_utils.py` - Essential testing infrastructure providing shared functionality for Fuseki+PostgreSQL hybrid backend testing including space management, dual-write validation, SPARQL UPDATE testing, and resource cleanup (keep - foundational utility)
- `kgentity_test_data.py` - Comprehensive test data generation module creating complex KGEntity objects with multiple frames and slots, including Person/Organization/Project entities with proper VitalSigns properties and edge relationships (keep - essential test data infrastructure)
- `test_dag_analysis.py` - DAG analysis and graph structure validation testing (keep for development)
- `pyoxigraph_store.py` - PyOxigraph implementation of RDFStoreInterface for testing, used by basic SPARQL operations tests (potentially outdated - may not be needed for current implementation)

**Potentially Outdated SPARQL Tests**:
- `test_sparql_operations.py` - Basic SPARQL operations test using SPARQLOperationsEngine and PyOxigraphStore (may be superseded by comprehensive SPARQL tests)
- `test_sparql_operations_final.py` - Final corrected version of basic SPARQL operations test with separate DELETE/INSERT operations (may be superseded by comprehensive SPARQL tests)
- `test_sparql_operations_fixed.py` - Fixed version of basic SPARQL operations test with corrected syntax using REGEX instead of STRSTARTS (may be superseded by comprehensive SPARQL tests)

**Other Test Files**:
- `test_hybrid_backend.py` - Basic hybrid backend integration test (superseded by complete backend test)
- `test_update_quads.py` - Quad update testing (may be integrated into other tests)

**Action Required**: Review these test files to determine which should be:
1. **Updated** to current implementation standards
2. **Consolidated** into main endpoint tests
3. **Removed** if superseded by newer comprehensive tests
4. **Documented** if they serve specific testing purposes

**POLICY NOTE:** Implementation status and functionality will be assessed by the user. Only the user can determine when components are ready for production deployment.

### ‚úÖ Phase 7: KGTypes Endpoint Implementation - COMPLETE SUCCESS

#### ‚úÖ 7.1 KGTypes Endpoint - IMPLEMENTATION COMPLETE (100% Success Rate)
**Status: COMPLETE & VALIDATED**
**Achievement: 16/16 tests passing (100% success rate)**
**Date Completed: January 4, 2026**

**üéâ PERFECT SUCCESS ACHIEVED:**
The KGTypes endpoint has been completely implemented and tested with 100% functionality. All core operations, advanced features, and edge cases are working perfectly.

**‚úÖ COMPLETE TEST COVERAGE (16/16 PASSING):**

**Core Operations ‚úÖ**
- ‚úÖ List KGTypes (Empty): Perfect empty state handling
- ‚úÖ List KGTypes (Populated State): Correct retrieval of all 20 test KGTypes  
- ‚úÖ KGType Creation via VitalSigns Objects: Full CRUD with proper validation
- ‚úÖ Get Individual KGType: Single object retrieval with JsonLdObject format
- ‚úÖ Update KGType Properties: Complete update operations with success field
- ‚úÖ Delete Individual KGType: Proper deletion with success field

**Filter Operations ‚úÖ**
- ‚úÖ Filter KGTypes - Name Filter: Proper name-based filtering working
- ‚úÖ Filter KGTypes - Version Filter: Version-based filtering functional
- ‚úÖ Filter KGTypes - Description Filter: Description search working
### Phase 8: KGEntities Endpoint Implementation & Testing

#### 8.1 KGEntities Endpoint Analysis & Requirements
**Priority: High**
**Estimated Time: 3-4 days**

The KGEntities endpoint is significantly more complex, involving entity graphs with frames, slots, and connecting edges. It requires comprehensive testing of 7 distinct endpoints and advanced features like grouping URIs for fast SPARQL queries.

**üéØ ENDPOINT COVERAGE REQUIRED:**

**Core Entity Operations:**
1. **GET** `/api/graphs/kgentities` - List Or Get Entities
2. **POST** `/api/graphs/kgentities` - Create Or Update Entities  
3. **DELETE** `/api/graphs/kgentities` - Delete Entities

**Entity Frame Operations:** See `endpoints/fuseki_psql_kgframes_endpoint_plan.md`

**Query Operations:**
7. **POST** `/api/graphs/kgentities/query` - Query Entities

**üîç CURRENT IMPLEMENTATION STATUS ANALYSIS:**

**Existing Fuseki Implementation (Standalone):**
- ‚úÖ Complete grouping URI implementation in `vitalgraph/sparql/grouping_uri_queries.py`
- ‚úÖ Graph operations utilities in `vitalgraph/utils/graph_operations.py`
- ‚úÖ Entity creation logic in `vitalgraph/kg/kgentity_create_endpoint_impl.py`
- ‚úÖ Entity retrieval logic in `vitalgraph/kg/kgentity_get_endpoint_impl.py`
- ‚úÖ Test data creation patterns in existing test scripts

**Fuseki+PostgreSQL Hybrid Status:**
- üîÑ KGEntities endpoint exists but needs comprehensive testing
- üîÑ Dual-write coordination needs validation for complex entity graphs
- üîÑ Grouping URI functionality needs porting from standalone Fuseki
- üîÑ VitalSigns integration needs validation for entity-frame-slot hierarchies

#### 8.2 Data Model & Test Data Requirements
**Priority: High**
**Estimated Time: 1-2 days**

**üß¨ ENTITY GRAPH DATA MODEL:**

The KGEntities endpoint works with complex graph structures:

```
KGEntity (Root)
‚îú‚îÄ‚îÄ hasKGGraphURI: entity_uri (grouping URI for fast queries)
‚îú‚îÄ‚îÄ Connected to KGFrames via Edge_hasEntityKGFrame
‚îÇ
KGFrame/KGSlot Architecture: See `endpoints/fuseki_psql_kgframes_endpoint_plan.md` for detailed frame data model
```

**üéØ TEST DATA REQUIREMENTS:**

**1. Basic Entity Test Data:**
```python
# Simple entities for basic CRUD testing
test_entities = [
    create_person_entity("John Doe"),
    create_organization_entity("ACME Corp"),
    create_location_entity("New York")
]
```

**2. Complex Entity Graph Test Data:**
```python
# Entities with complete frame/slot hierarchies
complex_entity_graphs = [
    create_person_with_contact_info(),  # Person + ContactFrame + EmailSlot + PhoneSlot
    create_organization_with_address(), # Org + AddressFrame + StreetSlot + CitySlot
    create_project_with_timeline()      # Project + TimelineFrame + StartSlot + EndSlot
]
```

**3. Grouping URI Test Data:**
```python
# Multiple entities sharing components for grouping URI testing
grouping_test_data = [
    create_entity_with_shared_frames(),
    create_multi_frame_entity(),
    create_nested_entity_hierarchy()
]
```

**üìã TEST DATA CREATION PLAN:**

**‚úÖ COMPLETED: Advanced Hierarchical Test Data Implementation**
- ‚úÖ Created `test_scripts/fuseki_postgresql/kgentity_test_data.py`
- ‚úÖ Comprehensive `KGEntityTestDataCreator` class with all required functionality
- ‚úÖ **MAJOR ENHANCEMENT**: Implemented complex multi-frame entities with hierarchical structures
- ‚úÖ **BREAKTHROUGH**: Added frame-to-frame relationships using `Edge_hasKGFrame` edges
- ‚úÖ **CORRECTED**: Fixed to use proper VitalSigns classes and properties based on kg_classes_properties.md

**Step 1: Port from Existing Fuseki Implementation - COMPLETED**
- ‚úÖ Adapted VitalSigns object creation methods from existing patterns
- ‚úÖ Implemented proper URI generation and relationships
- ‚úÖ Created entity-frame-slot hierarchies with connecting edges

**Step 2: Create Fuseki+PostgreSQL Specific Test Data - COMPLETED**
- ‚úÖ Added comprehensive test data for dual-write validation
- ‚úÖ Created complex entity graphs for transaction testing scenarios
- ‚úÖ Implemented performance test data with multiple entity types

**Step 3: Advanced Hierarchical Structures - COMPLETED**
- ‚úÖ **HIERARCHICAL ORGANIZATIONS**: Created management hierarchies with Entity ‚Üí Management Frame ‚Üí Officer Frames ‚Üí Slots
- ‚úÖ **FRAME-TO-FRAME RELATIONSHIPS**: Implemented `Edge_hasKGFrame` connections between Management and Officer frames
- ‚úÖ **3-LEVEL DAG STRUCTURES**: Successfully created and validated 3-level deep hierarchies
- ‚úÖ **COMPLEX PERSON ENTITIES**: Enhanced with Contact, Personal, and Employment frames (3 frames, 9 slots each)
- ‚úÖ **COMPLEX PROJECT ENTITIES**: Enhanced with Timeline, Budget, and Team frames (3 frames, 7 slots each)

**üìä ENHANCED TEST DATA STATISTICS:**
- **Person Entities**: 25 objects, 12 edges, 2 levels (Contact/Personal/Employment frames)
- **Organization Entities**: **45 objects, 22 edges, 3 levels** (Address/Company/Management hierarchies)
- **Project Entities**: 23 objects, 11 edges, 2 levels (Timeline/Budget/Team frames)
- **Hierarchical Management**: Management Frame ‚Üí CEO/CTO/CFO Officer Frames ‚Üí Name/Role/StartDate slots
- **Total Object Types**: KGEntity, KGFrame, KGTextSlot, KGIntegerSlot, KGDateTimeSlot, Edge_hasEntityKGFrame, Edge_hasKGFrame, Edge_hasKGSlot
- **DAG Validation**: 16/16 tests passing with complex hierarchical structures

**üå≥ HIERARCHICAL STRUCTURE EXAMPLES:**

**Organization Management Hierarchy:**
```
üè¢ Organization Entity
‚îú‚îÄ‚îÄ üìç Address Frame (4 slots: Street, City, State, Zip)
‚îú‚îÄ‚îÄ üè¢ Company Info Frame (3 slots: Industry, Founded, Employees)
‚îî‚îÄ‚îÄ üë• Management Frame (HIERARCHICAL LEVEL)
    ‚îú‚îÄ‚îÄ üë§ CEO Officer Frame (3 slots: Name, Role, Start Date)
    ‚îú‚îÄ‚îÄ üë§ CTO Officer Frame (3 slots: Name, Role, Start Date)
    ‚îî‚îÄ‚îÄ üë§ CFO Officer Frame (3 slots: Name, Role, Start Date)
```

**üîß CRITICAL CORRECTIONS & ENHANCEMENTS APPLIED:**
- **Fixed Slot Classes**: Replaced generic `KGSlot` with specific types (`KGTextSlot`, `KGDateTimeSlot`, `KGIntegerSlot`)
- **Fixed Edge Classes**: Corrected to use `Edge_hasEntityKGFrame` (entity-frame) and `Edge_hasKGFrame` (frame-frame)
- **Enhanced Edge Relationships**: Added frame-to-frame connections for hierarchical structures
- **Fixed Property Names**: Updated to use correct VitalSigns properties (`textSlotValue`, `dateTimeSlotValue`, `edgeSource`, `edgeDestination`)
- **Enhanced DAG Support**: Validated arbitrary depth DAG handling with 3-level hierarchies
- **Realistic Data**: Added meaningful business data (CEO: John Smith, CTO: Sarah Johnson, CFO: Michael Brown)

## üìä **FUSEKI-POSTGRESQL BACKEND INTEGRATION GAP ANALYSIS**

### **üîç CURRENT KGENTITY BACKEND INTEGRATION STATUS**

Analysis of the current KGEntity implementation's integration with the Fuseki-PostgreSQL hybrid backend, focusing specifically on backend interface patterns, SPARQL generation, and transaction management.

### **‚úÖ EXISTING BACKEND INTEGRATION COMPONENTS**

**1. Backend Interface Calls** ‚úÖ
- KGEntity endpoint properly accesses backend via `space_impl.get_db_space_impl()`
- SPARQL query execution through `backend.execute_sparql_query(space_id, sparql_query)`
- Space management integration with `space_manager.get_space(space_id)`

**2. SPARQL Query Structure** ‚úÖ
- Basic SPARQL query building methods implemented
- Graph URI handling with `_get_space_graph_uri()`
- Prefix management for haley-ai-kg and vital-core ontologies

**3. VitalSigns Object Conversion** ‚úÖ
- JSON-LD to VitalSigns object conversion patterns
- Entity graph structure validation
- Grouping URI assignment logic

### **‚ùå SPECIFIC FUNCTION-LEVEL INTEGRATION GAPS**

**üîç DETAILED GAP ANALYSIS PLAN ADDED**:

The following section identifies **specific missing functions and code** that need to be added to complete KGEntity-Fuseki-PostgreSQL integration. This analysis will be conducted systematically by:

1. **Function-by-Function Comparison**: Compare each KGEntity endpoint method with successful KGTypes implementation
2. **SPARQL Query Method Analysis**: Identify missing query building methods for entity graphs
3. **Backend Call Pattern Review**: Document specific backend integration patterns that are missing
4. **Grouping URI Implementation Gaps**: Identify specific functions missing for grouping URI optimization

---

## üîß **PLANNED DETAILED FUNCTION-LEVEL GAP ANALYSIS**

### **Phase 1: KGEntity Endpoint Method Analysis**
**Target**: `vitalgraph/endpoint/kgentities_endpoint.py`
**Approach**: Method-by-method comparison with `vitalgraph/endpoint/kgtypes_endpoint.py`
**Output**: List of missing/incomplete methods with specific function signatures needed

### **Phase 2: SPARQL Query Building Function Analysis** 
**Target**: KGEntity SPARQL query generation methods
**Approach**: Compare with KGTypes SPARQL patterns, identify missing query builders
**Output**: Specific missing functions like `_build_entity_graph_query()`, `_build_grouping_uri_query()`

### **Phase 3: Backend Integration Pattern Analysis**
**Target**: KGEntity backend call patterns in implementation files
**Approach**: Compare backend integration calls with successful KGTypes patterns
**Output**: Missing backend interface methods and integration functions

### **Phase 4: Grouping URI Function Gap Analysis**
**Target**: Grouping URI implementation in KGEntity vs mock implementation
**Approach**: Identify specific missing functions for grouping URI optimization
**Output**: List of missing grouping URI query and assignment functions

---

## üìã **PRELIMINARY SPECIFIC GAPS IDENTIFIED**

### **Missing SPARQL Query Building Functions**
Based on initial code review, these specific functions appear to be missing or incomplete:

**In `kgentities_endpoint.py`**:
- Frame-related query builders: See `endpoints/fuseki_psql_kgframes_endpoint_plan.md`

### **Missing Backend Integration Functions**
**In `kgentity_impl.py`**:
- `_execute_entity_graph_operation()` - Multi-object operation coordination
- `_validate_entity_graph_consistency()` - Cross-backend consistency checking

**üìù NOTE: Quad-Based Transaction Management** - See `endpoints/fuseki_psql_backend_plan.md` for complete transaction management details

### **Grouping URI Integration Analysis**
**Integration with `vitalgraph/sparql/grouping_uri_queries.py`**:

**‚úÖ COMPREHENSIVE FUNCTIONALITY ALREADY EXISTS**:
- `GroupingURIQueryBuilder` - Complete SPARQL query building for grouping URI operations
- `GroupingURIGraphRetriever` - Full implementation for fast entity/frame graph retrieval
- **Entity Graph Queries**: `build_complete_entity_graph_query()` using `hasKGGraphURI`
- **Frame Graph Queries**: `build_complete_frame_graph_query()` using `hasFrameGraphURI`
- **Type-Based Component Retrieval**: Organized by KGEntity, KGFrame, KGSlot types
- **Named/Default Graph Support**: Flexible graph context handling

**üîó REQUIRED INTEGRATION TASKS**:
- Import and initialize `GroupingURIQueryBuilder` in KGEntity endpoint
- Import and initialize `GroupingURIGraphRetriever` in KGEntity implementation
- Replace manual graph traversal with grouping URI-based fast retrieval
- Integrate `include_entity_graph=True` operations with `get_entity_graph_triples()`
- Integrate `include_frame_graph=True` operations with `get_frame_graph_triples()`

### **Missing Advanced Query Builder Features**
**Enhancement of `vitalgraph/sparql/kg_connection_query_builder.py`**:
Based on analysis of enhanced features in development copy and review of existing `kg_query_builder.py` capabilities:

**üìù NOTE: Significant Overlap Identified**
The existing `vitalgraph/sparql/kg_query_builder.py` already provides **advanced slot criteria filtering** with full comparison operators and XSD type casting. This reduces the enhancement scope significantly.

**1. Frame Chain Connection Queries** üö® **HIGH PRIORITY** - See `endpoints/fuseki_psql_kgframes_endpoint_plan.md`

**2. Entity Neighbor Discovery** üö® **MEDIUM PRIORITY**
- `build_neighbor_query()` for finding all connected entities
- `_build_neighbor_relation_patterns()` and `_build_neighbor_frame_patterns()`
- Configurable direction (incoming, outgoing, both) and result limits
- Important for entity relationship exploration

**3. Enhanced Union Query Building** üö® **MEDIUM PRIORITY**
- `_build_relation_union_block()` and `_build_frame_union_block()`
- Proper UNION clause construction for multi-type queries
- Better query optimization patterns for complex searches

**‚úÖ ALREADY IMPLEMENTED in `kg_query_builder.py`**:
- **Slot Criteria Filtering**: Complete implementation with all comparison operators
- **Type-Specific Slot Properties**: Full slot type support (Text, Double, Integer, Boolean, DateTime)
- **Advanced Value Comparisons**: XSD casting for numeric comparisons
- **Multi-Level Sorting**: Priority-based sorting with variable reuse optimization

---

## üîß **UPDATED IMPLEMENTATION STRATEGY BASED ON COMPREHENSIVE ARCHITECTURE REVIEW**

### **üéØ DEVELOPMENT EVOLUTION CONTEXT**
The KGEntity endpoint development has followed a strategic three-phase approach:

**Phase 1: Mock In-Memory Development** ‚úÖ **COMPLETE**
- **Purpose**: Develop and validate core KG entity management patterns
- **Implementation**: Comprehensive `/vitalgraph/kg/*_endpoint_impl.py` files (14 modules)
- **Backend**: Pyoxigraph in-memory SPARQL store
- **Status**: Production-ready implementation with full CRUD operations, advanced querying, and sophisticated business logic

**Phase 2: Fuseki Standalone Testing** ‚úÖ **COMPLETE** 
- **Purpose**: Performance testing and SPARQL optimization with dedicated Fuseki
- **Implementation**: Adapted mock patterns for Fuseki backend integration
- **Backend**: Apache Jena Fuseki standalone server
- **Status**: Validated performance and query patterns for production use

**Phase 3: Fuseki-PostgreSQL Production** üöÄ **IN PROGRESS**
- **Purpose**: Production deployment with dual-write consistency (Fuseki + PostgreSQL)
- **Implementation**: Leverage existing mock/Fuseki patterns for production backend
- **Backend**: Fuseki + PostgreSQL with quad-based transaction management
- **Status**: Integration of existing sophisticated implementations with production backend

### **üîç COMPREHENSIVE EXISTING FUNCTIONALITY DISCOVERED**

**‚úÖ SOPHISTICATED IMPLEMENTATION ALREADY EXISTS**:
- **Complete KGEntity Operations**: All CRUD operations with advanced business logic in `/vitalgraph/kg/*_endpoint_impl.py`
- **Advanced Query Capabilities**: Slot criteria filtering, multi-level sorting, frame-based queries via `KGQueryCriteriaBuilder`
- **Grouping URI Infrastructure**: Complete fast graph retrieval using `GroupingURIQueryBuilder` and `GroupingURIGraphRetriever`
- **Entity-Frame Relationships**: Complex entity-frame operations with `Edge_hasKGFrame` support
- **VitalSigns Integration**: Complete JSON-LD to VitalSigns object conversion
- **Graph Validation**: Entity graph structure validation and consistency checking

**‚úÖ PRODUCTION-READY COMPONENTS**:
- **14 Implementation Modules**: Complete functionality across all KGEntity and KGFrame operations
- **Mock Endpoint Integration**: Full integration in `MockKGEntitiesEndpoint` and `MockKGFramesEndpoint`
- **Advanced SPARQL Builders**: Sophisticated query generation with optimization
- **Cross-Backend Compatibility**: Designed for adaptation across mock, Fuseki, and PostgreSQL backends

### **üéØ REVISED REST API-ALIGNED IMPLEMENTATION STRATEGY**

## **üìã REST ENDPOINT-DRIVEN IMPLEMENTATION APPROACH**

### **Core Strategy**: REST API Method-by-Method Development
- **Target Endpoint**: `/vitalgraph/endpoint/kgentities_endpoint.py`
- **Backend Integration**: Fuseki-PostgreSQL with existing `/vitalgraph/kg/*` components
- **Database Integration**: Leverage existing space implementation and backend DB objects
- **Testing Approach**: One REST endpoint method at a time, following exact API specification

---

## **üîç ENDPOINT DISCOVERY & COMPARISON ANALYSIS**

### **REST API Endpoints (Target)**:
1. **GET** `/api/graphs/kgentities` - List Or Get Entities
2. **POST** `/api/graphs/kgentities` - Create Or Update Entities  
3. **DELETE** `/api/graphs/kgentities` - Delete Entities
4. **GET** `/api/graphs/kgentities/kgframes` - Get Entity Frames
5. **POST** `/api/graphs/kgentities/kgframes` - Create Or Update Entity Frames
6. **DELETE** `/api/graphs/kgentities/kgframes` - Delete Entity Frames
7. **POST** `/api/graphs/kgentities/query` - Query Entities

### **Mock Implementation Methods (Existing)**:
1. `list_kgentities()` - List entities with pagination
2. `get_kgentity()` - Get single entity by URI
3. `create_kgentities()` - Create entities from JSON-LD
4. `update_kgentities()` - Update entities with operation modes
5. `delete_kgentity()` - Delete single entity
6. `delete_kgentities_batch()` - Delete multiple entities
7. `get_kgentity_frames()` - Get entity frames
8. `query_entities()` - Query entities with criteria
9. `list_kgentities_with_graphs()` - List entities with complete graphs
10. `create_entity_frames()` - Create frames within entity context
11. `update_entity_frames()` - Update frames within entity context
12. `delete_entity_frames()` - Delete frames within entity context

## **üîç CRITICAL DISCOVERY TASK - MOCK vs PRIMARY ENDPOINT COMPARISON**

### **üö® CORRECTED ANALYSIS REQUIRED**

**Task**: Compare Mock KGEntities Endpoint (pyoxigraph) with Primary KGEntities Endpoint (Fuseki-PostgreSQL) to identify differences in:
- Method signatures and parameters
- Functionality coverage
- Response models
- Missing or extra methods

### **üìã MOCK KGENTITIES ENDPOINT METHODS (Pyoxigraph Implementation)**:

**Core CRUD Operations**:
1. `list_kgentities(space_id, graph_id, page_size=10, offset=0, search=None, include_entity_graph=False)` ‚Üí `EntitiesResponse`
2. `get_kgentity(space_id, graph_id, uri, include_entity_graph=False)` ‚Üí `EntityGraphResponse`
3. `create_kgentities(space_id, graph_id, document)` ‚Üí `EntityCreateResponse`
4. `update_kgentities(space_id, graph_id, document, operation_mode="update", parent_uri=None)` ‚Üí `EntityUpdateResponse`
5. `delete_kgentity(space_id, graph_id, uri, delete_entity_graph=False)` ‚Üí `EntityDeleteResponse`
6. `delete_kgentities_batch(space_id, graph_id, uri_list)` ‚Üí `EntityDeleteResponse`

**Advanced Operations**:
7. `query_entities(space_id, graph_id, query_request)` ‚Üí `EntityQueryResponse`
8. `get_kgentity_frames(space_id, graph_id, entity_uri=None, page_size=10, offset=0, search=None)` ‚Üí `Dict[str, Any]`
9. `list_kgentities_with_graphs(space_id, graph_id, page_size=10, offset=0, search=None, include_entity_graphs=False)` ‚Üí `EntitiesGraphResponse`

**Entity-Frame Relationship Operations**:
10. `create_entity_frames(space_id, graph_id, entity_uri, document, operation_mode="create")` ‚Üí `FrameCreateResponse`
11. `update_entity_frames(space_id, graph_id, entity_uri, document)` ‚Üí `FrameUpdateResponse`
12. `delete_entity_frames(space_id, graph_id, entity_uri, frame_uris)` ‚Üí `FrameDeleteResponse`

### **üìã PRIMARY KGENTITIES ENDPOINT METHODS (REST API)**:

**REST Route Definitions**:
1. `GET /kgentities` ‚Üí `list_or_get_entities()` - Handles listing, single URI, and URI list retrieval
2. `POST /kgentities` ‚Üí `create_or_update_entities()` - Handles CREATE, UPDATE, UPSERT modes
3. `DELETE /kgentities` ‚Üí `delete_entities()` - Handles single URI and URI list deletion
4. `GET /kgentities/kgframes` ‚Üí `get_entity_frames()` - Get frames for entities
5. `POST /kgentities/kgframes` ‚Üí `create_or_update_entity_frames()` - Create/update frames in entity context
6. `DELETE /kgentities/kgframes` ‚Üí `delete_entity_frames()` - Delete frames from entity context
7. `POST /kgentities/query` ‚Üí `query_entities()` - Advanced entity querying

### **üö® CRITICAL DIFFERENCES IDENTIFIED**:

#### **‚ùå PARAMETER MISMATCHES**:

**1. Missing `entity_type_uri` Parameter in Mock**:
- **Primary REST**: `list_or_get_entities()` includes `entity_type_uri: Optional[str]` for filtering
- **Mock**: `list_kgentities()` **MISSING** `entity_type_uri` parameter
- **Impact**: Mock cannot filter entities by type URI

**2. Different Parameter Structure**:
- **Primary REST**: Single method `list_or_get_entities()` handles multiple modes via parameters
- **Mock**: Separate methods `list_kgentities()`, `get_kgentity()`, `list_kgentities_with_graphs()`
- **Impact**: Different API surface area and calling patterns

#### **‚ùå FUNCTIONALITY GAPS**:

**1. Unified Retrieval Interface**:
- **Primary REST**: Single endpoint handles listing, single entity, and multiple entity retrieval
- **Mock**: Requires separate method calls for different retrieval modes
- **Impact**: Mock has more complex client integration

**2. Response Model Differences**:
- **Primary REST**: `Union[EntitiesResponse, JsonLdDocument]` for flexible responses
- **Mock**: Fixed response types per method
- **Impact**: Different response handling required

#### **‚úÖ MOCK ADVANTAGES**:

**1. Additional Functionality**:
- `list_kgentities_with_graphs()` - Enhanced listing with complete graphs
- Comprehensive helper methods for validation and lifecycle management
- More granular operation control

**2. Sophisticated Implementation**:
- 14 implementation modules with advanced business logic
- Complete VitalSigns integration
- Advanced SPARQL query building

### **üéØ IMPLEMENTATION STRATEGY IMPACT**:

**Required Adaptations**:
1. **Add missing `entity_type_uri` parameter** to mock implementation
2. **Unify retrieval interface** to match REST API pattern
3. **Adapt response models** to support flexible return types
4. **Integrate existing mock functionality** into unified REST endpoints

**Timeline Impact**: **MODERATE ADAPTATION REQUIRED** - Mock provides excellent foundation but needs interface alignment

---

## **üîß PROPOSED REST IMPLEMENTATION IMPROVEMENTS**

### **üìã Issue 1: Interface Structure - Keep Existing Unified Endpoint**

**Current Implementation**: 
- **Primary REST**: Single unified method `list_or_get_entities()` handles multiple modes via parameters
- **Mock**: Separate methods `list_kgentities()`, `get_kgentity()`, `list_kgentities_with_graphs()`

**Decision**: **Keep existing unified endpoint structure**

```python
# KEEP EXISTING STRUCTURE
@self.router.get("/kgentities", response_model=Union[EntitiesResponse, JsonLdDocument])
async def list_or_get_entities(...)
```

**Rationale**:
- Maintains existing API contract
- Single endpoint handles multiple retrieval modes efficiently
- Reduces API surface area complexity

---

### **üìã Issue 2: Structured Response Types**

**Current Problem**: 
- **Union response types**: `Union[EntitiesResponse, JsonLdDocument]` creates ambiguity
- **Fixed response types per method** in mock are more predictable

**Proposed Solution**: **Adopt KGTypes Pattern with Structured Responses**

#### **Enhanced Response Model Structure**:

```python
# NEW: Structured Entity Response Models (Following KGTypes Pattern)

class EntityResponse(BaseModel):
    """Response for single entity retrieval."""
    entity: JsonLdObject = Field(..., description="Single entity as JSON-LD object")
    complete_graph: Optional[JsonLdDocument] = Field(None, description="Complete entity graph when requested")

class EntitiesListResponse(BasePaginatedResponse):
    """Response for entity listing operations."""
    entities: JsonLdDocument = Field(..., description="JSON-LD document containing entity list")
    total_count: int = Field(..., description="Total number of entities")

class EntitiesBatchResponse(BaseModel):
    """Response for batch entity retrieval."""
    entities: Dict[str, JsonLdObject] = Field(..., description="Map of entity URI to JSON-LD object")
    complete_graphs: Optional[Dict[str, JsonLdDocument]] = Field(None, description="Complete graphs by entity URI when requested")

class EntitiesWithGraphsResponse(BasePaginatedResponse):
    """Response for entities with complete graph data."""
    entities: JsonLdDocument = Field(..., description="JSON-LD document containing entities")
    entity_graphs: Dict[str, JsonLdDocument] = Field(..., description="Complete graphs by entity URI")
```

**Benefits**:
- **No Union types** - each endpoint returns specific, predictable structure
- **Clear separation** between single objects and document collections
- **Consistent with KGTypes** pattern already established
- **Better client type safety** and API documentation

---

### **üìã Issue 3: KGFrames Endpoint Entity URI Parameter**

**Current Status**: ‚úÖ **CONFIRMED - Already Implemented**

**REST Implementation Analysis**:
```python
@self.router.get("/kgentities/kgframes", response_model=Dict[str, Any])
async def get_entity_frames(
    space_id: str = Query(..., description="Space ID"),
    graph_id: str = Query(..., description="Graph ID"),
    entity_uri: Optional[str] = Query(None, description="Entity URI to get frames for"),  # ‚úÖ PRESENT
    page_size: int = Query(10, ge=1, le=1000, description="Number of frames per page"),
    offset: int = Query(0, ge=0, description="Offset for pagination"),
    search: Optional[str] = Query(None, description="Search text to find in frame properties"),
    current_user: Dict = Depends(self.auth_dependency)
):
```

**‚úÖ Confirmation**: 
- **`entity_uri` parameter**: ‚úÖ **PRESENT** - Optional parameter for filtering frames by entity
- **Sub-frame support**: ‚úÖ **READY** - Can handle sub-frames within larger entity frame structures
- **Grouping URI handling**: ‚úÖ **SUPPORTED** - Implementation can set `hasKGGraphURI` for proper entity-level grouping

**Recommended Enhancement**:
```python
# ENHANCED: More structured response for frames endpoint
class EntityFramesResponse(BasePaginatedResponse):
    """Enhanced response for entity frames."""
    frames: JsonLdDocument = Field(..., description="JSON-LD document containing frames")
    entity_uri: Optional[str] = Field(None, description="Entity URI when filtering by specific entity")
    frame_hierarchy: Optional[Dict[str, List[str]]] = Field(None, description="Frame-to-subframe relationships")
```

---

## **üîß KGENTITIES ENDPOINT STRUCTURE ANALYSIS FOR TEST SCRIPT INTEGRATION**

### **üìã Current Implementation Structure**:

**REST Endpoint Methods (Lines 79-219)**:
- `list_or_get_entities()` - Unified retrieval endpoint
- `create_or_update_entities()` - Entity creation/update
- `delete_entities()` - Entity deletion
- `get_entity_frames()` - Frame retrieval
- `create_or_update_entity_frames()` - Frame creation/update
- `delete_entity_frames()` - Frame deletion
- `query_entities()` - Advanced entity querying

**Private Implementation Methods (Lines 222-2443)**:
- `_list_entities()` - Core listing logic
- `_get_entity_by_uri()` - Single entity retrieval
- `_get_entities_by_uris()` - Multiple entity retrieval
- `_create_or_update_entities()` - Core creation/update logic
- `_delete_entity_by_uri()` - Single entity deletion
- `_delete_entities_by_uris()` - Multiple entity deletion
- `_get_kgentity_frames()` - Frame retrieval logic
- `_create_or_update_frames()` - Frame creation/update logic
- Multiple utility methods for SPARQL queries, VitalSigns conversion, etc.

### **‚úÖ TEST SCRIPT INTEGRATION - DIRECT PRIVATE METHOD ACCESS**:

#### **Current Structure Works for Test Scripts**
**Private methods can be called directly** - no architectural changes needed.

#### **Test Script Integration Pattern**:
```python
# Test script can directly call private methods on endpoint instance
from vitalgraph.endpoint.kgentities_endpoint import KGEntitiesEndpoint

async def test_entity_operations():
    # Create endpoint instance (without REST setup)
    endpoint = KGEntitiesEndpoint.__new__(KGEntitiesEndpoint)
    endpoint.space_manager = space_manager
    endpoint.logger = logging.getLogger("test")
    # Initialize VitalSigns components
    endpoint.grouping_uri_builder = GroupingURIQueryBuilder()
    endpoint.graph_retriever = GroupingURIGraphRetriever(endpoint.grouping_uri_builder)
    endpoint.entity_validator = EntityGraphValidator()
    
    # Direct calls to private methods - no REST overhead
    entities = await endpoint._list_entities(space_id, graph_id, page_size, offset, entity_type_uri, search, include_entity_graph, {})
    entity = await endpoint._get_entity_by_uri(space_id, graph_id, uri, include_entity_graph, {})
    result = await endpoint._create_or_update_entities(space_id, graph_id, document, operation_mode, parent_uri, {})
    deleted = await endpoint._delete_entity_by_uri(space_id, graph_id, uri, delete_entity_graph, {})
    frames = await endpoint._get_kgentity_frames(space_id, graph_id, entity_uri, page_size, offset, search, {})
```

#### **Benefits of Direct Private Method Access**:
- **No architectural changes needed** - use existing implementation as-is
- **Direct access to all business logic** - 2,200+ lines of sophisticated functionality
- **No HTTP overhead** - direct method calls
- **Full access to return values and exceptions** - better debugging
- **Faster execution** - no serialization/deserialization
- **Simple setup** - just instantiate endpoint class without REST router

## **üéØ IMPLEMENTATION PRIORITY RECOMMENDATIONS**

### **High Priority Changes**:
1. **Implement structured response models** following KGTypes pattern
2. **Enhance existing unified endpoint** with improved response handling
3. **Integrate existing `/vitalgraph/kg/*` implementation modules** with Fuseki-PostgreSQL backend

### **Medium Priority Enhancements**:
1. **Enhanced frame response structure** with hierarchy information
2. **Batch operation optimizations** for multiple entity handling
3. **Consistent error response models** across all endpoints

### **Benefits of Proposed Changes**:
- **Better test script integration** with direct service access
- **Improved API usability** with clear, predictable interfaces
- **Improved type safety** for client applications
- **Consistent patterns** across all VitalGraph endpoints
- **Enhanced functionality** leveraging mock implementation strengths
- **Future-proof design** for additional entity operations

---

## **üèóÔ∏è NEW ARCHITECTURE: KG_IMPL REFACTORING APPROACH** ‚úÖ **IMPLEMENTED**

### **Architecture Overview**
The KGEntities endpoint has been successfully refactored using a new **kg_impl** package that provides:
- **Backend abstraction layer** for unified interface across Fuseki+PostgreSQL and PyOxigraph
- **Modular implementation functions** extracted from endpoint logic
- **Validation utilities** for entity structure and grouping URIs
- **Clean separation** between REST endpoint logic and core business logic

### **Package Structure**
```
vitalgraph/kg_impl/
‚îú‚îÄ‚îÄ __init__.py                 # Package exports
‚îú‚îÄ‚îÄ kg_backend_utils.py         # Backend abstraction interface
‚îú‚îÄ‚îÄ kg_validation_utils.py      # Validation and grouping URI management
‚îî‚îÄ‚îÄ kgentity_create_impl.py     # KGEntity creation implementation
```

### **Key Components**

#### **Backend Abstraction (`kg_backend_utils.py`)**
- `KGBackendInterface` - Abstract interface for backend operations
- `FusekiPostgreSQLBackendAdapter` - Adapter for Fuseki+PostgreSQL hybrid backend
- `create_backend_adapter()` - Factory function for backend adapters
- Unified operations: `store_objects()`, `object_exists()`, `delete_object()`, `execute_sparql_query()`

#### **Validation Utilities (`kg_validation_utils.py`)**
- `KGEntityValidator` - Structure validation for entities, frames, slots
- `KGGroupingURIManager` - Dual grouping URI management (entity-level + frame-level)
- `ValidationResult` - Structured validation results with errors/warnings

#### **KGEntity Implementation (`kgentity_create_impl.py`)**
- `KGEntityCreateProcessor` - Core processor for entity operations
- `create_kgentities()`, `update_kgentities()`, `upsert_kgentities()` - Convenience functions
- Backend-agnostic implementation with proper error handling

### **Refactored Endpoint Integration**
The `KGEntitiesEndpoint._create_or_update_entities()` method now:
1. Creates backend adapter from space implementation
2. Delegates core logic to `KGEntityCreateProcessor`
3. Handles REST-specific error conversion (response objects ‚Üí HTTPException)
4. Maintains existing API contract while using refactored implementation

---

### **Phase 1: ‚úÖ COMPLETED - KGEntity Creation with kg_impl Architecture**
**Status**: **IMPLEMENTED AND TESTED**
**REST Endpoint**: `POST /kgentities` (CREATE mode)
**Implementation**: Uses new `kg_impl.kgentity_create_impl` module
**Test Coverage**: Modular test framework with successful entity insertion

**Completed Deliverables**:
- ‚úÖ Backend abstraction layer with Fuseki+PostgreSQL adapter
- ‚úÖ Validation utilities with dual grouping URI management
- ‚úÖ KGEntity creation implementation with operation modes (CREATE/UPDATE/UPSERT)
- ‚úÖ Refactored KGEntities endpoint using kg_impl architecture
- ‚úÖ Modular test framework with successful test execution
- ‚úÖ Clean separation between REST logic and business logic

---

### **Phase 2: GET /api/graphs/kgentities - List Or Get Entities** üö® **NEXT PRIORITY**
**Timeline**: 1-2 days
**REST Endpoint**: `GET /kgentities`
**Implementation Focus**:
1. **Create** `kgentity_retrieval_impl.py` in kg_impl package
2. **Refactor** existing `_list_entities()` and `_get_entity_by_uri()` methods
3. **Use** backend abstraction for SPARQL queries and result processing
4. **Test Script**: Extend modular test framework with retrieval operations

**Deliverables**:
- `kg_impl/kgentity_retrieval_impl.py` with backend-agnostic retrieval logic
- Refactored endpoint methods using kg_impl architecture
- Extended test modules for entity retrieval operations
- Validated retrieval functionality with production backend

---

### **Phase 3: DELETE /api/graphs/kgentities - Delete Entities** üö® **HIGH**
**Timeline**: 1-2 days
**REST Endpoint**: `DELETE /kgentities`
**Implementation Focus**:
1. **Create** `kgentity_delete_impl.py` in kg_impl package
2. **Refactor** existing `_delete_entity_by_uri()` method using backend abstraction
3. **Handle** single URI and URI list deletion modes
4. **Test Script**: Complete CRUD cycle with deletion validation

**Deliverables**:
- `kg_impl/kgentity_delete_impl.py` with backend-agnostic deletion logic
- Refactored endpoint methods using kg_impl architecture
- Extended test modules for entity deletion operations
- Complete basic CRUD test cycle validation

---

### **Phase 4: GET /api/graphs/kgentities/kgframes - Entity Frames** üö® **MEDIUM**
**Timeline**: 2-3 days
**REST Endpoint**: `GET /kgentities/kgframes`
**Implementation Focus**:
1. **Create** `kgframe_operations_impl.py` in kg_impl package
2. **Refactor** existing `_get_kgentity_frames()` method using backend abstraction
3. **Handle** frame retrieval with entity context and pagination
4. **Test Script**: Frame operations within entity context

**Deliverables**:
- `kg_impl/kgframe_operations_impl.py` with backend-agnostic frame logic
- Refactored frame endpoint methods using kg_impl architecture
- Extended test modules for frame operations
- Validated frame-entity relationship handling

---

### **Phase 5: POST /api/graphs/kgentities/kgframes - Create/Update Entity Frames** üö® **MEDIUM**
**Timeline**: 2-3 days
**REST Endpoint**: `POST /kgentities/kgframes`
**Implementation Focus**:
1. **Extend** `kgframe_operations_impl.py` with creation/update logic
2. **Refactor** existing `_create_or_update_frames()` method
3. **Handle** frame creation within entity context with proper edge relationships
4. **Test Script**: Complete frame lifecycle operations

**Deliverables**:
- Extended frame implementation with creation/update operations
- Refactored frame creation endpoint methods
- Complete frame lifecycle test coverage
- Validated entity-frame relationship management

---

### **Phase 6: Advanced Query Operations** üö® **LOW**
**Timeline**: 3-4 days
**REST Endpoint**: `POST /kgentities/query`
**Implementation Focus**:
1. **Create** `kgentity_query_impl.py` in kg_impl package
2. **Integrate** existing query builder functionality with backend abstraction
3. **Handle** complex entity queries with criteria and sorting
4. **Test Script**: Advanced query operations and performance validation

**Deliverables**:
- `kg_impl/kgentity_query_impl.py` with backend-agnostic query logic
- Advanced query endpoint implementation
- Query test modules with complex scenarios
- Performance validation for query operations

---

## **üéØ REVISED IMPLEMENTATION STRATEGY**

### **Benefits of kg_impl Architecture**:
1. **Code Reusability**: Backend-agnostic implementations can be used across different endpoints
2. **Maintainability**: Clear separation between REST logic and business logic
3. **Testability**: Implementation functions can be unit tested independently
4. **Consistency**: Unified patterns across all KG operations
5. **Future-Proofing**: Easy to add new backends or modify existing ones
6. **Reduced Duplication**: Common validation and processing logic shared across operations

### **Migration Strategy for Existing kg/* Files**:
- **Short Term**: Keep existing `/vitalgraph/kg/*` files for mock implementation compatibility
- **Medium Term**: Gradually migrate mock implementations to use kg_impl backend abstraction
- **Long Term**: Deprecate and remove `/vitalgraph/kg/*` files once full migration is complete

### **Testing Strategy**:
- **Modular Tests**: Each kg_impl module has corresponding test modules
- **Integration Tests**: End-to-end tests using refactored endpoints
- **Backend Tests**: Validate backend abstraction with different implementations
- **Performance Tests**: Ensure refactoring doesn't impact performance

---

### **Phase 4: POST /api/graphs/kgentities/query - Query Entities** üö® **MEDIUM**
**Timeline**: 1-2 days
**REST Endpoint**: `POST /kgentities/query`
**Mock Methods**: `query_entities()`
**Implementation Focus**:
1. **Integrate** `/vitalgraph/kg/kgentity_query_endpoint_impl.py`
2. **Adapt** existing `query_entities()` REST method for Fuseki-PostgreSQL
3. **Implement** advanced slot criteria filtering and multi-level sorting
4. **Test Script**: Comprehensive query testing with various criteria

**Deliverables**:
- Working `POST /kgentities/query` endpoint with advanced criteria support
- Comprehensive query test coverage
- Validated `KGQueryCriteriaBuilder` integration

---

### **Phase 5: GET /api/graphs/kgentities/kgframes - Get Entity Frames** üö® **MEDIUM**
**Timeline**: 1-2 days
**REST Endpoint**: `GET /kgentities/kgframes`
**Mock Methods**: `get_kgentity_frames()`
**Implementation Focus**:
1. **Integrate** `/vitalgraph/kg/kgentity_get_endpoint_impl.py` frame methods
2. **Adapt** existing `get_entity_frames()` REST method for Fuseki-PostgreSQL
3. **Implement** entity-frame relationship queries
4. **Test Script**: Entity-frame relationship retrieval testing

**Deliverables**:
- Working `GET /kgentities/kgframes` endpoint
- Entity-frame relationship retrieval functionality
- Validated frame discovery within entity context

---

### **Phase 6: POST /api/graphs/kgentities/kgframes - Create Or Update Entity Frames** üö® **MEDIUM**
**Timeline**: 2-3 days (Complex - Multiple Sub-phases)
**REST Endpoint**: `POST /kgentities/kgframes`
**Mock Methods**: `create_entity_frames()`, `update_entity_frames()`

#### **Sub-Phase 6.1: Frame Creation Within Entity Context**
**Implementation Focus**:
1. **Integrate** `/vitalgraph/kg/kgentity_create_endpoint_impl.py` frame creation methods
2. **Adapt** `Edge_hasKGFrame` relationship management for Fuseki-PostgreSQL
3. **Test Script**: Frame creation within entity context

#### **Sub-Phase 6.2: Frame Update Within Entity Context**
**Implementation Focus**:
1. **Integrate** `/vitalgraph/kg/kgentity_update_endpoint_impl.py` frame update methods
2. **Handle** CREATE, UPDATE, UPSERT operation modes for frames
3. **Test Script**: Frame update operations within entity context

**Deliverables**:
- Working `POST /kgentities/kgframes` endpoint with all operation modes
- Frame lifecycle management within entity context
- Comprehensive entity-frame relationship testing

---

### **Phase 7: DELETE /api/graphs/kgentities/kgframes - Delete Entity Frames** üö® **MEDIUM**
**Timeline**: 1-2 days
**REST Endpoint**: `DELETE /kgentities/kgframes`
**Mock Methods**: `delete_entity_frames()`
**Implementation Focus**:
1. **Integrate** `/vitalgraph/kg/kgentity_delete_endpoint_impl.py` frame deletion methods
2. **Adapt** existing `delete_entity_frames()` REST method for Fuseki-PostgreSQL
3. **Handle** frame deletion within entity context
4. **Test Script**: Frame deletion operations within entity context

**Deliverables**:
- Working `DELETE /kgentities/kgframes` endpoint
- Frame deletion within entity context
- Complete entity-frame lifecycle testing

---

### **Phase 8: Performance Optimization & Final Integration** üö® **LOW**
**Timeline**: 1-2 days
**Implementation Focus**:
1. **Performance optimization** for Fuseki-PostgreSQL backend operations
2. **Final integration testing** across all implemented REST endpoints
3. **Error handling refinement** and edge case validation
4. **Documentation completion** for implemented functionality

**Deliverables**:
- Performance-optimized endpoint operations
- Comprehensive integration test validation
- Complete error handling and edge case coverage
- Final production readiness assessment

---

## **üîß IMPLEMENTATION INTEGRATION STRATEGY**

### **Backend Integration Pattern**:
1. **Leverage Existing Components**: Use `/vitalgraph/kg/*_endpoint_impl.py` files as foundation
2. **Adapt Backend Calls**: Replace pyoxigraph patterns with Fuseki-PostgreSQL calls
3. **Integrate DB Objects**: Use existing space implementation DB objects for database aspects
4. **Maintain Compatibility**: Ensure mock and Fuseki variants remain functional

### **Test-Driven Development**:
1. **Incremental Testing**: Add one operation at a time to test script
2. **Validation Focus**: Each phase validates previous functionality remains working
3. **Comprehensive Coverage**: Build complete test suite incrementally
4. **Backend Compatibility**: Test across mock, Fuseki, and Fuseki-PostgreSQL backends

### **Enhanced SPARQL Query Builders Required**
- **Frame chain connection queries** for hierarchical entity relationships
- **Slot criteria filtering** with advanced comparison operators
- **Entity neighbor discovery** for relationship exploration
- **Enhanced union query building** for multi-type searches
- Integration patterns with Fuseki-PostgreSQL backend
- Grouping URI optimization functions

### **Backend Integration Functions**
- Specific backend interface methods missing
- Multi-object operation coordination functions
- Consistency validation and error recovery functions

**Timeline for Complete Analysis**: 5-7 days (updated to include query builder enhancement)
**Output**: Detailed implementation plan with specific function additions required

**üîç CURRENT IMPLEMENTATION REVIEW REQUIRED:**

**1. Endpoint Method Completeness**
- Review all 7 endpoint methods in `vitalgraph/endpoint/kgentities_endpoint.py`
- Identify missing or incomplete implementations
- Compare with working standalone Fuseki implementation

**2. Grouping URI Integration**
- Port grouping URI functionality from `vitalgraph/sparql/grouping_uri_queries.py`
- Integrate `GroupingURIQueryBuilder` and `GroupingURIGraphRetriever`
- Ensure compatibility with Fuseki+PostgreSQL dual-write

**3. Graph Operations Integration**
- Port utilities from `vitalgraph/utils/graph_operations.py`
- Implement dual grouping URI support (entity + frame level)
- Validate edge relationship handling

**4. VitalSigns Complex Object Handling**
- Test entity-frame-slot hierarchy creation
- Validate JSON-LD document vs object handling
- Ensure proper edge relationship serialization

**üìã IMPLEMENTATION TASKS:**

**Task 1: Review Current KGEntities Endpoint**
- Audit all 7 endpoint methods for completeness
- Identify missing functionality compared to standalone Fuseki
- Document current dual-write integration status

**Task 2: Port Grouping URI Functionality**
- Integrate GroupingURIQueryBuilder into hybrid backend
- Implement fast SPARQL queries using hasKGGraphURI
- Add frame-level grouping URI support

**Task 3: Implement Missing Endpoint Methods**
- Complete any missing or stub implementations
- Add proper error handling and validation
- Ensure consistent response models

**Task 4: Add Complex Graph Support**
- Implement entity graph creation with frames/slots
- Add cascade deletion for entity graphs
- Validate edge relationship handling

#### 8.5 Testing Strategy & Validation
**Priority: High**
**Estimated Time: 2-3 days**

**üéØ TESTING APPROACH:**

**Phase 1: Basic Entity Operations (Similar to KGTypes)**
- Empty state testing
- Single entity CRUD operations
- Basic VitalSigns integration
- Simple dual-write validation

**Phase 2: Complex Entity Graph Operations**
- Multi-component entity creation
- Entity graph retrieval and validation
- Frame and slot operations
- Edge relationship testing

**Phase 3: Advanced Features**
- Grouping URI query performance
- Complex SPARQL query validation
- Batch operations testing
- Error scenario handling

**Phase 4: Production Readiness Validation**
- Performance testing with large entity graphs
- Concurrent operation testing
- Transaction rollback validation
- Comprehensive error handling

**üìä SUCCESS CRITERIA:**

**Functional Requirements:**
- All 7 endpoints implemented and tested
- 100% test pass rate for all operations
- Complete entity graph CRUD functionality
- Grouping URI fast queries working
- VitalSigns integration for complex graphs

**Performance Requirements:**
- Grouping URI queries under 50ms
- Entity graph creation under 200ms
- Complex query operations under 500ms
- Batch operations scaling linearly

**Data Integrity Requirements:**
- Perfect dual-write consistency
- Transaction rollback working correctly
- Edge relationship preservation
- Cascade deletion working properly

#### 8.6 Implementation Timeline
**Priority: High**
**Total Estimated Time: 10-15 days**

**Week 1: Analysis & Setup (Days 1-3)**
- Day 1: Review current KGEntities endpoint implementation
- Day 2: Analyze standalone Fuseki implementation for porting
- Day 3: Create comprehensive test data sets

**Week 2: Core Implementation (Days 4-8)**
- Day 4-5: Port grouping URI functionality to hybrid backend
- Day 6-7: Implement missing endpoint methods
- Day 8: Add complex entity graph support

**Week 3: Testing & Validation (Days 9-12)**
- Day 9-10: Implement comprehensive test script
- Day 11: Performance testing and optimization
- Day 12: Error scenario and edge case testing

**Week 4: Production Readiness (Days 13-15)**
- Day 13: Integration testing with full stack
- Day 14: Documentation and code review
- Day 15: Final validation and deployment preparation

This comprehensive plan ensures the KGEntities endpoint achieves complete functionality and reliability.

Following the established pattern of endpoint test scripts, comprehensive testing is implemented for all endpoints. See dedicated endpoint planning files for specific test requirements.

**Test Script Architecture:**
```python
# test_scripts/fuseki_postgresql/test_kgtypes_endpoint_fuseki_postgresql.py
class KGTypesEndpointFusekiPostgreSQLTester:
    """
    Comprehensive KGTypes endpoint testing for Fuseki+PostgreSQL hybrid backend.
    
    Test Coverage:
    - Space lifecycle - See `endpoints/fuseki_psql_spaces_endpoint_plan.md`
    - KGType CRUD operations (create, read, update, delete)
    - VitalSigns JSON-LD conversion (both directions)
    - Dual-write consistency validation
    - Error handling and edge cases
    """
```

**Core Test Operations:**
1. **Space Management** - See `endpoints/fuseki_psql_spaces_endpoint_plan.md` for complete space management details

2. **KGType CRUD Operations**
   - Create KGTypes with proper VitalSigns objects
   - List KGTypes (empty and populated states)
   - Get individual KGTypes by ID
   - Update KGType properties
   - Delete individual KGTypes
   - Batch operations where applicable

3. **VitalSigns Integration**
   - Create KGType graph objects using VitalSigns
   - Convert to JSON-LD format for endpoint input
   - Parse endpoint responses back to VitalSigns objects
   - Validate object properties and relationships

4. **Data Validation**
   - Verify KGType properties (name, description, version)
   - Validate JSON-LD structure and context
   - Check dual-write consistency between Fuseki and PostgreSQL
   - Ensure proper error handling for invalid operations

**VitalSigns JSON-LD Conversion Pattern:**

**CRITICAL: JSON-LD Object vs Document Distinction**

For consistency across all tests, maintain the proper distinction between JSON-LD objects and documents:

**Multiple Objects ‚Üí JSON-LD Document (with @graph array):**
```python
# Input: Multiple VitalSigns KGType objects ‚Üí JSON-LD document for endpoint
kgtype_objects = [create_test_kgtype_1(), create_test_kgtype_2()]
jsonld_document = GraphObject.to_jsonld_list(kgtype_objects)  # Creates document with @graph

# Endpoint call for batch operations (create, list, etc.)
response = await endpoint.create_kgtypes(space_id, jsonld_document)

# Output: Endpoint response ‚Üí VitalSigns objects
response_objects = vitalsigns.from_jsonld_list(response.data)
```

**Single Object ‚Üí JSON-LD Object:**
```python
# Input: Single VitalSigns KGType object ‚Üí JSON-LD object for endpoint
single_kgtype = create_updated_kgtype()
jsonld_object = single_kgtype.to_jsonld()  # Creates single JSON-LD object

# Endpoint call for individual operations (update, get, etc.)
response = await endpoint.update_kgtype(space_id, graph_id, kgtype_id, jsonld_object)

# Output: Single object response ‚Üí VitalSigns object
response_object = vitalsigns.from_jsonld(response.data)
```

**KGType Property Naming Convention:**
VitalSigns uses a specific naming convention where property names remove the "has"/"is" prefix and de-capitalize the first letter:
- `hasName` ‚Üí `name`
- `hasKGTypeDescription` ‚Üí `kGTypeDescription`  
- `hasKGTypeVersion` ‚Üí `kGTypeVersion`
- `hasKGModelVersion` ‚Üí `kGModelVersion`

**Example KGType Object Creation:**
```python
kgtype = KGType()
kgtype.URI = "http://vital.ai/test/kgtype/Person_12345678"
kgtype.name = "Person"  # NOT hasName
kgtype.kGTypeDescription = "Represents a person entity"  # NOT hasKGTypeDescription
kgtype.kGTypeVersion = "1.0"  # NOT hasKGTypeVersion
kgtype.kGModelVersion = "2024.1"  # NOT hasKGModelVersion
```

**Test Data Requirements:**
- 20 unique KGType objects with diverse properties for comprehensive testing
- Multiple categories: Person, Organization, Product, Location, Event, Document, Project, Service, Asset, Contract, Customer, Vendor, Technology, Process, Resource, Category, Relationship, Attribute, Metric, Policy
- Various version numbers (1.0-2.5) and model versions (2024.1-2024.2)
- Valid and invalid KGType configurations
- Edge cases (empty names, long descriptions, etc.)
- Relationship testing with other KG components

**Expected Test Coverage:**
- ‚úÖ Space creation and cleanup
- ‚úÖ KGType creation with VitalSigns objects (20 unique objects)
- ‚úÖ KGType listing (empty and populated states)
- ‚úÖ Individual KGType retrieval
- ‚úÖ KGType filtering by various criteria (/api/graphs/kgtypes filter option)
- ‚úÖ KGType pagination testing (multiple page sizes and offsets)
- ‚úÖ KGType property updates
- ‚úÖ KGType deletion
- ‚úÖ JSON-LD conversion validation
- ‚úÖ Dual-write consistency checks
- ‚úÖ Error handling scenarios

#### 7.2 Endpoint Implementation Analysis

**Status**: All endpoint implementation details moved to dedicated planning files. See `endpoints/fuseki_psql_kgtypes_endpoint_plan.md` for KGTypes-specific analysis.

1. **Missing `db_objects` Layer**: 
   - KGType implementation expects `db_space_impl.db_objects.list_objects()`
   - KGType implementation expects `db_space_impl.db_objects.get_objects_by_uris_batch()`
   - KGType implementation expects `db_space_impl.db_objects.get_existing_object_uris()`
   - **Current Status**: These methods DO NOT EXIST in Fuseki-PostgreSQL backend

2. **Missing Dual-Write Integration**:
   - KGType implementation uses `db_space_impl.db_ops.add_rdf_quads_batch()` 
   - KGType implementation uses `db_space_impl.db_ops.remove_quads_by_subject_uris()`
   - **Current Status**: These methods DO NOT EXIST in Fuseki-PostgreSQL backend

3. **Incomplete Backend Architecture**:
   - Current `FusekiPostgreSQLSpaceImpl` only has basic SPARQL operations
   - Missing object-level abstraction layer for CRUD operations
   - Missing query optimization for reads vs. writes

**‚úÖ EXISTING COMPONENTS:**

1. **Backend Implementation**: See `endpoints/fuseki_psql_backend_plan.md` for complete dual-write coordination details

**üîß REQUIRED IMPLEMENTATIONS:**

#### 7.3 Implementation Requirements

**Phase 1: Database Objects Layer (db_objects)**
```python
# Required: /vitalgraph/db/fuseki_postgresql/fuseki_postgresql_db_objects.py
class FusekiPostgreSQLDbObjects:
    async def list_objects(space_id, graph_id, page_size, offset, filters) -> (objects, count)
    async def get_objects_by_uris(space_id, uris, graph_id) -> objects  
    async def get_objects_by_uris_batch(space_id, uris, graph_id) -> quads
    async def get_existing_object_uris(space_id, uris) -> existing_uris
```

**Phase 2: Database Operations Layer (db_ops)**  
```python
# Required: /vitalgraph/db/fuseki_postgresql/fuseki_postgresql_db_ops.py
class FusekiPostgreSQLDbOps:
    async def add_rdf_quads_batch(space_id, quads, transaction) -> count
    async def remove_rdf_quads_batch(space_id, quads, transaction) -> count
    async def remove_quads_by_subject_uris(space_id, uris, graph_id, transaction) -> count
```

**Phase 3: Space Implementation Integration**
```python
# Update: FusekiPostgreSQLSpaceImpl
def get_db_space_impl(self):
    return self  # Return self with db_objects and db_ops attributes

def __init__(self):
    # Add missing layers
    self.db_objects = FusekiPostgreSQLDbObjects(self)
    self.db_ops = FusekiPostgreSQLDbOps(self)
```

**Phase 4: Two-Phase Object Query Architecture**

Based on analysis of the Fuseki standalone implementation and existing ObjectsImpl patterns, we can implement efficient object queries using a proven two-phase approach:

**Phase 4.1: Two-Phase Query Pattern**
```python
# Phase 1: Find Subject URIs matching criteria
def find_subject_uris_by_criteria(space_id, graph_id, filters, page_size, offset):
    """
    Execute SPARQL SELECT to find subject URIs matching search criteria.
    Returns: (subject_uris[], total_count)
    """
    sparql_query = f"""
    SELECT DISTINCT ?subject (COUNT(*) OVER() AS ?total) WHERE {{
        GRAPH <{graph_uri}> {{
            ?subject a ?type .
            {build_filter_conditions(filters)}
        }}
    }}
    ORDER BY ?subject
    LIMIT {page_size} OFFSET {offset}
    """

# Phase 2: Retrieve Complete Objects for Subject URIs  
def get_complete_objects_by_uris(space_id, subject_uris, graph_id):
    """
    Execute SPARQL CONSTRUCT to get all triples for specific subject URIs.
    Returns: complete_triples[]
    """
    sparql_query = f"""
    CONSTRUCT {{ ?s ?p ?o }}
    WHERE {{
        GRAPH <{graph_uri}> {{
            ?s ?p ?o .
            VALUES ?s {{ {format_uri_values(subject_uris)} }}
        }}
    }}
    """
```

**Phase 4.2: Batch Processing for Large Result Sets**
```python
async def get_objects_batch_processing(subject_uris, batch_size=100):
    """
    Process large URI lists in batches to avoid query size limits.
    """
    results = []
    for i in range(0, len(subject_uris), batch_size):
        batch_uris = subject_uris[i:i + batch_size]
        batch_triples = await get_complete_objects_by_uris(space_id, batch_uris, graph_id)
        results.extend(batch_triples)
    return results
```

**Phase 4.3: VitalSigns Integration Pattern**
```python
async def convert_triples_to_jsonld(triples_list, return_format='document'):
    """
    Convert SPARQL triples to VitalSigns objects then to JSON-LD.
    """
    # Convert triples to VitalSigns GraphObjects
    vitalsigns = VitalSigns()
    graph_objects = vitalsigns.from_triples_list(triples_list)
    
    # Convert to JSON-LD format
    if return_format == 'document' and len(graph_objects) > 1:
        return vitalsigns.to_jsonld_list(graph_objects)  # JSON-LD document
    elif len(graph_objects) == 1:
        return graph_objects[0].to_jsonld()  # JSON-LD object
    else:
        return vitalsigns.to_jsonld_list(graph_objects)  # JSON-LD document
```

**Phase 4.4: Query Routing Architecture**
- **Read Operations** (list, get): 
  - Phase 1 queries route to Fuseki for fast subject URI discovery
  - Phase 2 queries route to Fuseki for complete object retrieval
  - Batch processing for large result sets (100 objects per batch)
- **Write Operations** (create, update, delete): Use dual-write coordinator
- **Consistency Validation**: Cross-check between Fuseki and PostgreSQL

#### 7.5 Common Object Query Utilities

**Phase 5.1: Fuseki SPARQL Query Utilities**
```python
# NEW FILE: /vitalgraph/db/fuseki_postgresql/fuseki_query_utils.py
class FusekiQueryUtils:
    """Common utilities for Fuseki SPARQL queries used across all object endpoints."""
    
    @staticmethod
    async def find_subject_uris_by_criteria(fuseki_manager, space_id, graph_id, 
                                          filters=None, page_size=100, offset=0):
        """
        Phase 1: Execute SPARQL SELECT to find subject URIs matching criteria.
        
        Args:
            fuseki_manager: FusekiDatasetManager instance
            space_id: Space identifier
            graph_id: Graph identifier  
            filters: Dict with vitaltype_filter, search_text, subject_uri, etc.
            page_size: Results per page
            offset: Pagination offset
            
        Returns:
            Tuple of (subject_uris: List[str], total_count: int)
        """
        graph_uri = f"http://vital.ai/graph/{space_id}" if graph_id == "main" else f"http://vital.ai/graph/{space_id}/{graph_id}"
        
        # Build filter conditions
        filter_conditions = []
        if filters:
            if filters.get('vitaltype_filter'):
                filter_conditions.append(f"?subject a <{filters['vitaltype_filter']}> .")
            if filters.get('search_text'):
                filter_conditions.append(f"""
                    ?subject ?searchProp ?searchValue .
                    FILTER(CONTAINS(LCASE(STR(?searchValue)), LCASE("{filters['search_text']}")))
                """)
            if filters.get('subject_uri'):
                filter_conditions.append(f"?subject = <{filters['subject_uri']}> .")
        
        filter_clause = "\n            ".join(filter_conditions) if filter_conditions else "?subject a ?type ."
        
        sparql_query = f"""
        SELECT DISTINCT ?subject (COUNT(*) OVER() AS ?total) WHERE {{
            GRAPH <{graph_uri}> {{
                {filter_clause}
            }}
        }}
        ORDER BY ?subject
        LIMIT {page_size} OFFSET {offset}
        """
        
        bindings = await fuseki_manager.query_dataset(space_id, sparql_query)
        
        subject_uris = [binding['subject']['value'] for binding in bindings]
        total_count = int(bindings[0]['total']['value']) if bindings else 0
        
        return subject_uris, total_count
    
    @staticmethod
    async def get_complete_objects_by_uris(fuseki_manager, space_id, subject_uris, graph_id):
        """
        Phase 2: Execute SPARQL CONSTRUCT to get all triples for specific subject URIs.
        
        Args:
            fuseki_manager: FusekiDatasetManager instance
            space_id: Space identifier
            subject_uris: List of subject URIs to retrieve
            graph_id: Graph identifier
            
        Returns:
            List of (subject, predicate, object) triples
        """
        if not subject_uris:
            return []
            
        graph_uri = f"http://vital.ai/graph/{space_id}" if graph_id == "main" else f"http://vital.ai/graph/{space_id}/{graph_id}"
        
        # Format URIs for VALUES clause
        uri_values = " ".join([f"<{uri}>" for uri in subject_uris])
        
        sparql_query = f"""
        CONSTRUCT {{ ?s ?p ?o }}
        WHERE {{
            GRAPH <{graph_uri}> {{
                ?s ?p ?o .
                VALUES ?s {{ {uri_values} }}
            }}
        }}
        """
        
        # Execute CONSTRUCT query to get triples
        triples = await fuseki_manager.construct_dataset(space_id, sparql_query)
        return triples
    
    @staticmethod
    async def get_objects_batch_processing(fuseki_manager, space_id, subject_uris, graph_id, batch_size=100):
        """
        Process large URI lists in batches to avoid SPARQL query size limits.
        
        Args:
            fuseki_manager: FusekiDatasetManager instance
            space_id: Space identifier
            subject_uris: List of all subject URIs to retrieve
            graph_id: Graph identifier
            batch_size: Number of URIs per batch (default: 100)
            
        Returns:
            List of all triples for the subject URIs
        """
        all_triples = []
        
        for i in range(0, len(subject_uris), batch_size):
            batch_uris = subject_uris[i:i + batch_size]
            batch_triples = await FusekiQueryUtils.get_complete_objects_by_uris(
                fuseki_manager, space_id, batch_uris, graph_id
            )
            all_triples.extend(batch_triples)
        
        return all_triples
    
    @staticmethod
    async def convert_triples_to_jsonld(triples_list, return_format='document'):
        """
        Convert SPARQL triples to VitalSigns objects then to JSON-LD.
        
        Args:
            triples_list: List of (subject, predicate, object) triples
            return_format: 'document' for multiple objects, 'object' for single
            
        Returns:
            JSON-LD document or object
        """
        if not triples_list:
            from vital_ai_vitalsigns.vitalsigns import VitalSigns
            vitalsigns = VitalSigns()
            return vitalsigns.to_jsonld_list([])
        
        # Convert triples to VitalSigns GraphObjects
        from vital_ai_vitalsigns.vitalsigns import VitalSigns
        vitalsigns = VitalSigns()
        graph_objects = vitalsigns.from_triples_list(triples_list)
        
        # Convert to JSON-LD format based on object count and requested format
        if return_format == 'object' and len(graph_objects) == 1:
            return graph_objects[0].to_jsonld()  # Single JSON-LD object
        else:
            return vitalsigns.to_jsonld_list(graph_objects)  # JSON-LD document with @graph
```

**Phase 5.2: Implementation Steps for db_objects Layer**
```python
# IMPLEMENTATION: /vitalgraph/db/fuseki_postgresql/fuseki_postgresql_db_objects.py
class FusekiPostgreSQLDbObjects:
    """Database objects layer implementing the two-phase query pattern."""
    
    def __init__(self, space_impl):
        self.space_impl = space_impl
        self.fuseki_manager = space_impl.fuseki_manager
        self.logger = logging.getLogger(f"{__name__}.FusekiPostgreSQLDbObjects")
    
    async def list_objects(self, space_id, graph_id, page_size=100, offset=0, filters=None):
        """
        List objects using two-phase query: find URIs then get complete objects.
        """
        # Phase 1: Find subject URIs matching criteria
        subject_uris, total_count = await FusekiQueryUtils.find_subject_uris_by_criteria(
            self.fuseki_manager, space_id, graph_id, filters, page_size, offset
        )
        
        if not subject_uris:
            return [], total_count
        
        # Phase 2: Get complete objects for found URIs (with batching)
        triples = await FusekiQueryUtils.get_objects_batch_processing(
            self.fuseki_manager, space_id, subject_uris, graph_id
        )
        
        # Phase 3: Convert to VitalSigns GraphObjects
        from vital_ai_vitalsigns.vitalsigns import VitalSigns
        vitalsigns = VitalSigns()
        graph_objects = vitalsigns.from_triples_list(triples)
        
        return graph_objects, total_count
    
    async def get_objects_by_uris(self, space_id, uris, graph_id):
        """Get multiple objects by URI list using batch processing."""
        triples = await FusekiQueryUtils.get_objects_batch_processing(
            self.fuseki_manager, space_id, uris, graph_id
        )
        
        from vital_ai_vitalsigns.vitalsigns import VitalSigns
        vitalsigns = VitalSigns()
        return vitalsigns.from_triples_list(triples)
    
    async def get_objects_by_uris_batch(self, space_id, subject_uris, graph_id):
        """Get objects as raw triples (used by KGTypeImpl pattern)."""
        triples = await FusekiQueryUtils.get_objects_batch_processing(
            self.fuseki_manager, space_id, subject_uris, graph_id
        )
        
        # Convert to quad format expected by existing code
        graph_uri = f"http://vital.ai/graph/{space_id}" if graph_id == "main" else f"http://vital.ai/graph/{space_id}/{graph_id}"
        quads = [(s, p, o, graph_uri) for s, p, o in triples]
        return quads
    
    async def get_existing_object_uris(self, space_id, uris):
        """Check which URIs exist by querying for any triple with those subjects."""
        if not uris:
            return []
        
        # Use simple ASK queries to check existence
        existing_uris = []
        for uri in uris:
            sparql_query = f"""
            ASK {{
                GRAPH ?g {{
                    <{uri}> ?p ?o .
                }}
            }}
            """
            exists = await self.fuseki_manager.ask_dataset(space_id, sparql_query)
            if exists:
                existing_uris.append(uri)
        
        return existing_uris
```

#### 7.6 Implementation Strategy
**Following Established Patterns:**

**Base Structure (from existing tests):**
- Inherit from common test base class
- Use consistent logging and error reporting
- Follow space lifecycle management pattern
- Implement dual-write consistency validation

**KGType Property Names (from vital-ai-haley-kg package KGType.pyi):**
```python
class KGType(VITAL_Node):
    kGModelVersion: str      # hasKGModelVersion -> kGModelVersion
    kGTypeVersion: str       # hasKGTypeVersion -> kGTypeVersion  
    kGraphDescription: str   # hasKGraphDescription -> kGraphDescription
    # Inherited from VITAL_Node:
    # name: str              # hasName -> name
```

**KGType Subclasses (from vital-ai-haley-kg package):**
All KGType subclasses are supported. See `endpoints/fuseki_psql_kgtypes_endpoint_plan.md` for detailed type support information.

- KGActorType - Actor/person type definitions
- KGAgentPublisherType - Agent publisher type definitions  
- KGAgentSubmissionType - Agent submission type definitions
- KGAgentType - Agent type definitions
- KGAnnotationType - Annotation type definitions
- KGCalendarEventType - Calendar event type definitions
- KGCategoryType - Category type definitions
- KGChatInteractionEventType - Chat interaction event type definitions
- KGChatInteractionType - Chat interaction type definitions
- KGChatMessageType - Chat message type definitions
- KGCodeDocumentType - Code document type definitions
- KGDocumentRepositoryType - Document repository type definitions
- KGDocumentType - Document type definitions
- KGEMailType - Email type definitions
- **KGEntityType** - Entity type definitions (used by KGEntities endpoint)
- KGEventType - Event type definitions
- KGFileType - File type definitions
- KGFlagType - Flag type definitions
- **KGFrameType** - Frame type definitions (used by KGFrames endpoint)
- KGGroupType - Group type definitions
- KGInstructionType - Instruction type definitions
- KGInteractionType - Interaction type definitions
- KGNoteDocumentType - Note document type definitions
- KGOfficeType - Office type definitions
- KGOrganizationType - Organization type definitions
- KGRatingSummaryType - Rating summary type definitions
- KGRatingType - Rating type definitions
- KGRelatedCategoryType - Related category type definitions
- **KGRelationType** - Relation type definitions (used by KGRelations endpoint)
- KGRequestType - Request type definitions
- KGResourceType - Resource type definitions
- KGResponseType - Response type definitions
- KGRoomType - Room type definitions
- KGRunDocumentType - Run document type definitions
- KGSearchType - Search type definitions
- KGSlotRoleType - Slot role type definitions
- **KGSlotType** - Slot type definitions (used by KGFrames endpoint)
- KGStatsSummaryType - Stats summary type definitions
- KGTagType - Tag type definitions
- KGTaskType - Task type definitions
- KGTeamType - Team type definitions
- KGToolRequestType - Tool request type definitions
- KGToolResultType - Tool result type definitions
- KGToolType - Tool type definitions

**Key KGType Subclass Property Definitions (from .pyi files):**

**KGEntityType (used by KGEntities endpoint):**
```python
class KGEntityType(KGType):
    kGEntityTypeExternIdentifier: str    # hasKGEntityTypeExternIdentifier -> kGEntityTypeExternIdentifier
    # Inherited from KGType:
    # kGModelVersion: str
    # kGTypeVersion: str  
    # kGraphDescription: str
    # name: str (from VITAL_Node)
```

**KGFrameType (used by KGFrames endpoint):**
```python
class KGFrameType(KGType):
    kGFrameTypeExternIdentifier: str     # hasKGFrameTypeExternIdentifier -> kGFrameTypeExternIdentifier
    # Inherited from KGType:
    # kGModelVersion: str
    # kGTypeVersion: str  
    # kGraphDescription: str
    # name: str (from VITAL_Node)
```

**KGRelationType (used by KGRelations endpoint):**
```python
class KGRelationType(KGType):
    kGRelationTypeSymmetric: bool        # hasKGRelationTypeSymmetric -> kGRelationTypeSymmetric
    # Inherited from KGType:
    # kGModelVersion: str
    # kGTypeVersion: str  
    # kGraphDescription: str
    # name: str (from VITAL_Node)
```

**KGSlotType (used by KGFrames endpoint):**
```python
class KGSlotType(KGType):
    kGSlotTypeClassURI: str              # hasKGSlotTypeClassURI -> kGSlotTypeClassURI
    kGSlotTypeExternIdentifier: str      # hasKGSlotTypeExternIdentifier -> kGSlotTypeExternIdentifier
    kGSlotTypeLabel: str                 # hasKGSlotTypeLabel -> kGSlotTypeLabel
    kGSlotTypeName: str                  # hasKGSlotTypeName -> kGSlotTypeName
    # Inherited from KGType:
    # kGModelVersion: str
    # kGTypeVersion: str  
    # kGraphDescription: str
    # name: str (from VITAL_Node)
```

**SPARQL Property Mapping for Test Data and Queries:**
These VitalSigns property names map to RDF properties in SPARQL queries:
- `kGEntityTypeExternIdentifier` ‚Üí `haley:hasKGEntityTypeExternIdentifier`
- `kGFrameTypeExternIdentifier` ‚Üí `haley:hasKGFrameTypeExternIdentifier`
- `kGRelationTypeSymmetric` ‚Üí `haley:hasKGRelationTypeSymmetric`
- `kGSlotTypeClassURI` ‚Üí `haley:hasKGSlotTypeClassURI`
- `kGSlotTypeExternIdentifier` ‚Üí `haley:hasKGSlotTypeExternIdentifier`
- `kGSlotTypeLabel` ‚Üí `haley:hasKGSlotTypeLabel`
- `kGSlotTypeName` ‚Üí `haley:hasKGSlotTypeName`
- `kGModelVersion` ‚Üí `haley:hasKGModelVersion`
- `kGTypeVersion` ‚Üí `haley:hasKGTypeVersion`
- `kGraphDescription` ‚Üí `haley:hasKGraphDescription`
- `name` ‚Üí `vital-core:hasName`

**VitalSigns Integration (from triples test):**
- Use VitalSigns native JSON-LD functions
- Proper object creation and conversion
- Validate round-trip conversion accuracy

**Error Handling (from all existing tests):**
- Comprehensive exception handling
- Detailed logging for debugging
- Graceful cleanup on test failures

**JSON-LD Conversion Consistency Rules:**
- **Batch Operations** (create multiple, list, search): Use `GraphObject.to_jsonld_list()` ‚Üí JSON-LD document with @graph array
- **Individual Operations** (update single, get single): Use `object.to_jsonld()` ‚Üí JSON-LD object
- **Response Parsing**: Match conversion method to expected response format
- **Critical**: Never use list conversion for single objects or single conversion for multiple objects

**‚úÖ IMPLEMENTED FILES:**
- `/vitalgraph/db/fuseki_postgresql/fuseki_postgresql_space_impl.py` - Hybrid space implementation
- `/vitalgraph/db/fuseki_postgresql/dual_write_coordinator.py` - Transaction-safe dual-write operations
- `/vitalgraph/db/fuseki_postgresql/sparql_update_parser.py` - See `fuseki_psql_sparql_plan.md`
- `/vitalgraph/db/fuseki_postgresql/postgresql_db_impl.py` - PostgreSQL implementation with admin table verification
- `/vitalgraph/db/fuseki_postgresql/postgresql_schema.py` - Schema with composite keys and dataset columns
- `/vitalgraph/db/fuseki_postgresql/postgresql_signal_manager.py` - Fixed asyncpg 0.30.0 compatibility
- `/vitalgraph/db/fuseki_postgresql/fuseki_postgresql_space_graphs.py` - Graph management
- `/test_scripts/fuseki_postgresql/test_spaces_endpoint_fuseki_postgresql.py` - 7/7 tests passing
- `/test_scripts/fuseki_postgresql/test_graphs_endpoint_fuseki_postgresql.py` - 6/6 tests passing (see `endpoints/fuseki_psql_graphs_endpoint_plan.md`)  
- `/test_scripts/fuseki_postgresql/test_triples_endpoint_fuseki_postgresql.py` - See `endpoints/fuseki_psql_triples_endpoint_plan.md`

### ‚ùå ORIGINAL PLAN COMPONENTS (NOT YET IMPLEMENTED)
**Focus**: Multi-dataset Fuseki + PostgreSQL hybrid architecture
**Architecture**: Separate Fuseki datasets per space + PostgreSQL primary data tables
**Status**: Planning phase only - implementation not started

**Components Still Required:**
- ‚ùå **Multi-dataset Fuseki architecture** - Need to implement separate dataset per space
- ‚ùå **FusekiDatasetManager** - HTTP Admin API dataset operations not implemented
- ‚ùå **FusekiAdminDataset** - RDF-based metadata management not implemented  
- ‚ùå **Direct PostgreSQL connections** - Basic connection exploration only
- ‚ùå **Per-space primary data tables** - Schema design not finalized
- ‚ùå **Admin dataset initialization scripts** - Not implemented

**Files Still To Be Created:**
- `/vitalgraph/db/fuseki_postgresql/fuseki_admin_dataset.py` - Not created
- `/scripts/init_vitalgraph_fuseki_admin.py` - Not created
- `/vitalgraph/db/fuseki_postgresql/fuseki_dataset_manager.py` - Not created
- `/vitalgraph/db/fuseki_postgresql/postgresql_schema.py` - Not created
- `/vitalgraph/db/fuseki_postgresql/dual_write_coordinator.py` - Not created

## JSON-LD Endpoint Request Model Validation Requirements

### Universal JSON-LD Request Model Design

**APPLIES TO ALL JSON-LD ENDPOINTS**: All endpoints that accept JSON-LD input must follow consistent patterns.

All JSON-LD endpoints must support two distinct JSON-LD input formats with proper validation:

#### 1. Single Object Operations (JsonLdObject)
**Use Case**: Updating or creating a single object (KGType, KGEntity, KGFrame, etc.)
**Format**: JSON-LD object with @id, @type, and properties at root level
**Validation**: 
- Must contain exactly one object
- Must have @id and @type fields
- Should reject if @graph array is present
- Used for: Single object updates, individual object creation

**Example** (KGType):
```json
{
  "@context": {...},
  "@id": "http://vital.ai/ontology/haley-ai-kg#PersonType",
  "@type": "http://vital.ai/ontology/haley-ai-kg#KGEntityType",
  "name": "Person",
  "kGraphDescription": "Person entity type"
}
```

**Example** (KGEntity):
```json
{
  "@context": {...},
  "@id": "http://example.org/person123",
  "@type": "http://vital.ai/ontology/haley-ai-kg#KGEntity",
  "name": "John Doe",
  "kGEntityTypeURI": "http://vital.ai/ontology/haley-ai-kg#PersonType"
}
```

#### 2. Multiple Object Operations (JsonLdDocument)
**Use Case**: Creating or updating multiple objects in batch (KGTypes, KGEntities, KGFrames, etc.)
**Format**: JSON-LD document with @graph array containing multiple objects
**Validation**:
- Must contain @graph array with one or more objects
- Each object in @graph must have @id and @type
- Should reject if used for single object operations
- Used for: Batch object creation, bulk updates, complex object graphs

**Example** (Multiple KGTypes):
```json
{
  "@context": {...},
  "@graph": [
    {
      "@id": "http://vital.ai/ontology/haley-ai-kg#PersonType",
      "@type": "http://vital.ai/ontology/haley-ai-kg#KGEntityType",
      "name": "Person"
    },
    {
      "@id": "http://vital.ai/ontology/haley-ai-kg#OrganizationType", 
      "@type": "http://vital.ai/ontology/haley-ai-kg#KGEntityType",
      "name": "Organization"
    }
  ]
}
```

**Example** (Entity with Frame and Slots):
```json
{
  "@context": {...},
  "@graph": [
    {
      "@id": "http://example.org/person123",
      "@type": "http://vital.ai/ontology/haley-ai-kg#KGEntity",
      "name": "John Doe"
    },
    {
      "@id": "http://example.org/person123_profile",
      "@type": "http://vital.ai/ontology/haley-ai-kg#KGFrame",
      "name": "Profile Frame"
    },
    {
      "@id": "http://example.org/person123_profile_age",
      "@type": "http://vital.ai/ontology/haley-ai-kg#KGIntegerSlot",
      "integerValue": 30
    }
  ]
}
```

#### 3. Request Model Validation Logic

**Applicable Endpoint Methods**:
- **KGTypes**: `POST /kgtypes`, `PUT /kgtypes`, `DELETE /kgtypes`
- **KGEntities**: `POST /kgentities`, `PUT /kgentities`, `DELETE /kgentities`
- **KGFrames**: `POST /kgframes`, `PUT /kgframes`, `DELETE /kgframes`
- **KGSlots**: `POST /kgslots`, `PUT /kgslots`, `DELETE /kgslots`
- **KGRelations**: `POST /kgrelations`, `PUT /kgrelations`, `DELETE /kgrelations`
- **All other JSON-LD endpoints**: Accept both JsonLdObject and JsonLdDocument

**Validation Rules**:
1. **Single Object Validation**: If request contains root-level @id/@type, validate as JsonLdObject
2. **Multiple Object Validation**: If request contains @graph array, validate as JsonLdDocument
3. **Error Handling**: Return HTTP 400 with descriptive error if:
   - JsonLdDocument used where single object expected
   - JsonLdObject used where multiple objects expected
   - Missing required fields (@id, @type)
   - Invalid JSON-LD structure

**Implementation Requirements**:
- Use Pydantic Union types to support both models
- Add custom validation logic to enforce single vs multiple object rules
- Provide clear error messages for validation failures
- Support automatic detection of input format based on structure

## Universal Request and Response Models

### Request Models with Union Support (Applied to All Endpoints)

**Pattern to be applied to all JSON-LD endpoints (KGTypes, KGEntities, KGFrames, KGSlots, KGRelations)**

#### 1. KGTypeRequest (Union Model)
```python
from typing import Union
from pydantic import BaseModel, Field, validator
from vitalgraph.model.jsonld_model import JsonLdObject, JsonLdDocument

class KGTypeRequest(BaseModel):
    """
    Universal request model supporting both single and multiple KGType operations.
    Uses Union to accept either JsonLdObject or JsonLdDocument.
    """
    space_id: str = Field(..., description="Space identifier")
    graph_id: str = Field(..., description="Graph identifier")
    data: Union[JsonLdObject, JsonLdDocument] = Field(
        ..., 
        description="KGType data - either single object or document with @graph array"
    )
    
    @validator('data')
    def validate_jsonld_format(cls, v):
        """Custom validation to ensure proper JSON-LD format usage."""
        if isinstance(v, JsonLdObject):
            # Single object validation
            if not v.id or not v.type:
                raise ValueError("JsonLdObject must have @id and @type fields")
        elif isinstance(v, JsonLdDocument):
            # Multiple object validation
            if not v.graph or len(v.graph) == 0:
                raise ValueError("JsonLdDocument must have non-empty @graph array")
            for obj in v.graph:
                if not obj.get('@id') or not obj.get('@type'):
                    raise ValueError("Each object in @graph must have @id and @type")
        return v

# Specific operation request models
class KGTypeCreateRequest(KGTypeRequest):
    """Request model for creating KGTypes (POST /kgtypes)."""
    pass

class KGTypeUpdateRequest(KGTypeRequest):
    """Request model for updating KGTypes (PUT /kgtypes)."""
    pass

class KGTypeBatchDeleteRequest(BaseModel):
    """Request model for batch deleting KGTypes (DELETE /kgtypes with body)."""
    space_id: str = Field(..., description="Space identifier")
    graph_id: str = Field(..., description="Graph identifier")
    data: Union[JsonLdDocument, List[str]] = Field(
        ..., 
        description="KGType URIs to delete - either JsonLdDocument or list of URIs"
    )
```

#### 2. Response Models with Union Support
```python
class KGTypeResponse(BaseModel):
    """Base response model for KGType operations."""
    success: bool = Field(..., description="Operation success status")
    message: str = Field(..., description="Human-readable status message")
    data: Optional[Union[JsonLdObject, JsonLdDocument]] = Field(
        None, 
        description="Response data - format matches request format"
    )
    errors: Optional[List[str]] = Field(None, description="Error messages if any")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")

class KGTypeCreateResponse(KGTypeResponse):
    """Response model for KGType creation operations."""
    created_count: Optional[int] = Field(None, description="Number of KGTypes created")
    created_uris: Optional[List[str]] = Field(None, description="URIs of created KGTypes")

class KGTypeUpdateResponse(KGTypeResponse):
    """Response model for KGType update operations."""
    updated_count: Optional[int] = Field(None, description="Number of KGTypes updated")
    updated_uris: Optional[List[str]] = Field(None, description="URIs of updated KGTypes")

class KGTypeDeleteResponse(KGTypeResponse):
    """Response model for KGType deletion operations."""
    deleted_count: Optional[int] = Field(None, description="Number of KGTypes deleted")
    deleted_uris: Optional[List[str]] = Field(None, description="URIs of deleted KGTypes")

class KGTypeListResponse(BaseModel):
    """Response model for KGType listing operations."""
    success: bool = Field(..., description="Operation success status")
    data: Optional[JsonLdDocument] = Field(None, description="KGTypes as JSON-LD document")
    pagination: Optional[Dict[str, Any]] = Field(None, description="Pagination metadata")
    total_count: Optional[int] = Field(None, description="Total number of KGTypes")
    page_size: int = Field(..., description="Number of items per page")
    offset: int = Field(..., description="Current page offset")
```

#### 3. Endpoint Method Signatures
```python
# Endpoint method signatures follow consistent patterns across all endpoints
# See dedicated endpoint planning files for specific implementations
        current_user: Dict = Depends(auth_dependency)
    ) -> KGTypeCreateResponse:
        """Create KGTypes - accepts both single object and batch operations."""
        pass
    
    async def update_kgtypes(
        self,
        request: KGTypeUpdateRequest,
        current_user: Dict = Depends(auth_dependency)
    ) -> KGTypeUpdateResponse:
        """Update KGTypes - accepts both single object and batch operations."""
        pass
    
    async def delete_kgtypes(
        self,
        space_id: str = Query(...),
        graph_id: str = Query(...),
        uri: Optional[str] = Query(None, description="Single KGType URI to delete"),
        uri_list: Optional[List[str]] = Query(None, description="Multiple KGType URIs to delete"),
        request: Optional[KGTypeBatchDeleteRequest] = Body(None),
        current_user: Dict = Depends(auth_dependency)
    ) -> KGTypeDeleteResponse:
        """Delete KGTypes - supports URI params, URI list, or request body."""
        pass
```

#### 4. Format Detection and Validation Logic
```python
def detect_jsonld_format(data: Union[JsonLdObject, JsonLdDocument]) -> str:
    """Detect whether input is single object or document format."""
    if isinstance(data, JsonLdObject):
        return "single"
    elif isinstance(data, JsonLdDocument):
        return "multiple"
    else:
        # Fallback detection based on structure
        if hasattr(data, 'graph') and isinstance(data.graph, list):
            return "multiple"
        elif hasattr(data, 'id') and hasattr(data, 'type'):
            return "single"
        else:
            raise ValueError("Unable to detect JSON-LD format")

def validate_operation_compatibility(operation: str, format_type: str, object_count: int):
    """Validate that operation type matches data format."""
    if operation == "single_update" and format_type == "multiple":
        raise ValueError("Cannot use JsonLdDocument for single object update operations")
    if operation == "batch_create" and format_type == "single":
        raise ValueError("Cannot use JsonLdObject for batch operations")
    if format_type == "single" and object_count > 1:
        raise ValueError("JsonLdObject format detected but multiple objects provided")
```

#### 5. Template Pattern for Other Endpoints
```python
# This pattern should be replicated for:
# - KGEntityRequest/Response models
# - KGFrameRequest/Response models  
# - KGSlotRequest/Response models
# - KGRelationRequest/Response models
# - Any future JSON-LD endpoint models

class KGEntityRequest(BaseModel):
    space_id: str = Field(..., description="Space identifier")
    graph_id: str = Field(..., description="Graph identifier")
    data: Union[JsonLdObject, JsonLdDocument] = Field(...)
    # Same validation pattern as KGTypeRequest

class KGFrameRequest(BaseModel):
    space_id: str = Field(..., description="Space identifier")
    graph_id: str = Field(..., description="Graph identifier")
    data: Union[JsonLdObject, JsonLdDocument] = Field(...)
    # Same validation pattern as KGTypeRequest
```

### Benefits of Union Model Approach
1. **Type Safety**: Pydantic validates input format automatically
2. **Flexibility**: Single endpoint handles both single and batch operations
3. **Consistency**: Same pattern across all JSON-LD endpoints
4. **Clear Errors**: Descriptive validation messages for format mismatches
5. **Future-Proof**: Easy to extend for new JSON-LD endpoint types

### üéØ REALISTIC ASSESSMENT - VERY EARLY STAGE

**SPARQL UPDATE Implementation**: See `fuseki_psql_sparql_plan.md` ‚úÖ
**Hybrid Architecture**: Concept exploration only ‚ùå

**ACTUAL IMPLEMENTATION STATUS:**
The Fuseki+PostgreSQL hybrid backend is in very early exploration phase:

**Backend Implementation Status:** See `endpoints/fuseki_psql_backend_plan.md` for complete implementation details
- Complete dual-write system architecture
- SPARQL implementation - See `fuseki_psql_sparql_plan.md`
- Fuseki dataset management
- PostgreSQL schema implementation  
- Error handling and recovery
- Performance optimization
- Security implementation
- Production testing
- Documentation
- Monitoring and observability

This is a very early stage exploration with the vast majority of implementation work still ahead.

---

#### 6.1 Hybrid Architecture Design (ORIGINAL PLAN - ‚ùå NOT IMPLEMENTED)
**Status: PLANNING PHASE - January 3, 2026**
**Estimated Implementation Time: 6-12 months**

The `BackendType.FUSEKI_POSTGRESQL` hybrid backend requires extensive implementation work:

**Architecture Overview:**
```
VitalGraph FUSEKI_POSTGRESQL Backend
‚îú‚îÄ‚îÄ Fuseki Server (Primary for graph operations)
‚îÇ   ‚îú‚îÄ‚îÄ vitalgraph_space_space1 dataset (active graph data)
‚îÇ   ‚îú‚îÄ‚îÄ vitalgraph_space_space2 dataset (active graph data)
‚îÇ   ‚îî‚îÄ‚îÄ vitalgraph_space_spaceN dataset (active graph data)
‚îî‚îÄ‚îÄ PostgreSQL Server - See `endpoints/fuseki_psql_backend_plan.md`
```

**Key Design Principles:** See `endpoints/fuseki_psql_backend_plan.md` for complete architecture details

#### 6.2 DbImpl Interface Design
**Priority: High**
**Estimated Time: 1 day**

Create a common `DbImplInterface` that both PostgreSQL and FUSEKI_POSTGRESQL backends can implement:

```python
# vitalgraph/db/db_inf.py (existing file)
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List

class DbImplInterface(ABC):
    """
    Common interface for database implementation components.
    Both PostgreSQL and FUSEKI_POSTGRESQL backends will implement this interface.
    """
    
    @abstractmethod
    async def connect(self) -> bool:
        """Establish database connection."""
        pass
    
    @abstractmethod
    async def disconnect(self) -> bool:
        """Close database connection."""
        pass
    
    @abstractmethod
    async def is_connected(self) -> bool:
        """Check if database connection is active."""
        pass
    
    @abstractmethod
    async def execute_query(self, query: str, params: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """Execute a database query and return results."""
        pass
    
    @abstractmethod
    async def execute_update(self, query: str, params: Optional[Dict] = None) -> bool:
        """Execute a database update/insert/delete operation."""
        pass
    
    @abstractmethod
    # Transaction methods - See `endpoints/fuseki_psql_backend_plan.md` for implementation
    async def begin_transaction(self) -> Any: pass
    async def commit_transaction(self, transaction: Any) -> bool: pass
    async def rollback_transaction(self, transaction: Any) -> bool: pass
    
    @abstractmethod
    def get_connection_info(self) -> Dict[str, Any]:
        """Get database connection information."""
        pass
```

#### 6.3 PostgreSQL Schema Design (No SQLAlchemy)
**Priority: High**
**Estimated Time: 2-3 days**

Implement optimized PostgreSQL schema without SQLAlchemy dependency:

```python
# vitalgraph/db/fuseki_postgresql/postgresql_schema.py
class FusekiPostgreSQLSchema:
    """
    PostgreSQL schema for FUSEKI_POSTGRESQL hybrid backend.
    Direct SQL operations without SQLAlchemy for maximum performance.
    
    Schema matches existing SQLAlchemy table definitions exactly but defined as direct SQL.
    """
    
    # Admin tables (matching existing SQLAlchemy schema exactly)
    ADMIN_TABLES = {
        # Install table - matches SQLAlchemy Install model
        'install': '''
            CREATE TABLE install (
                id SERIAL PRIMARY KEY,
                install_datetime TIMESTAMP,
                update_datetime TIMESTAMP,
                active BOOLEAN
            )
        ''',
        
        # Space table - matches SQLAlchemy Space model
        'space': '''
            CREATE TABLE space (
                space_id VARCHAR(255) PRIMARY KEY,
                space_name VARCHAR(255),
                space_description TEXT,
                tenant VARCHAR(255),
                update_time TIMESTAMP
            )
        ''',
        
        # Graph table - matches SQLAlchemy Graph model  
        'graph': '''
            CREATE TABLE graph (
                graph_id SERIAL PRIMARY KEY,
                space_id VARCHAR(255) NOT NULL,
                graph_uri VARCHAR(500),
                graph_name VARCHAR(255),
                created_time TIMESTAMP,
                FOREIGN KEY (space_id) REFERENCES space(space_id) ON DELETE CASCADE
            )
        ''',
        
        # User table - matches SQLAlchemy User model
        'user': '''
            CREATE TABLE "user" (
                user_id SERIAL PRIMARY KEY,
                username VARCHAR(255) UNIQUE NOT NULL,
                password VARCHAR(255),
                email VARCHAR(255),
                tenant VARCHAR(255),
                update_time TIMESTAMP
            )
        '''
    }
    
    # Per-space primary data tables (matching existing PostgreSQL backend schema)
    def get_space_tables(self, space_id: str) -> Dict[str, str]:
        prefix = f"{space_id}_"
        return {
            # Term table - matches existing PostgreSQL backend term table structure
            'term': f'''
                CREATE TABLE {prefix}term (
                    term_uuid UUID PRIMARY KEY,
                    term_text TEXT NOT NULL,
                    term_type CHAR(1) NOT NULL,
                    term_language VARCHAR(10),
                    term_datatype VARCHAR(500)
                )
            ''',
            
            # RDF Quad table - matches existing PostgreSQL backend rdf_quad table structure
            'rdf_quad': f'''
                CREATE TABLE {prefix}rdf_quad (
                    subject_uuid UUID NOT NULL,
                    predicate_uuid UUID NOT NULL,
                    object_uuid UUID NOT NULL,
                    context_uuid UUID NOT NULL,
                    FOREIGN KEY (subject_uuid) REFERENCES {prefix}term(term_uuid) ON DELETE CASCADE,
                    FOREIGN KEY (predicate_uuid) REFERENCES {prefix}term(term_uuid) ON DELETE CASCADE,
                    FOREIGN KEY (object_uuid) REFERENCES {prefix}term(term_uuid) ON DELETE CASCADE,
                    FOREIGN KEY (context_uuid) REFERENCES {prefix}term(term_uuid) ON DELETE CASCADE,
                    PRIMARY KEY (subject_uuid, predicate_uuid, object_uuid, context_uuid)
                )
            '''
        }
    
    def get_admin_indexes(self) -> Dict[str, List[str]]:
        """Get index definitions for admin tables only (space, graph, user tables)."""
        return {
            'space_indexes': [
                'CREATE INDEX idx_space_tenant ON space(tenant)',
                'CREATE INDEX idx_space_update_time ON space(update_time)'
            ],
            'graph_indexes': [
                'CREATE INDEX idx_graph_space_id ON graph(space_id)',
                'CREATE INDEX idx_graph_uri ON graph(graph_uri)'
            ],
            'user_indexes': [
                'CREATE INDEX idx_user_tenant ON "user"(tenant)',
                'CREATE INDEX idx_user_username ON "user"(username)'
            ]
        }
    
    # NOTE: No indexes needed for per-space term and quad tables
    # These tables are used for archival/backup purposes only, not active querying
    # All active graph queries go directly to Fuseki datasets for optimal performance
```

#### 6.4 PostgreSQL DbImpl Implementation
**Priority: High**
**Estimated Time: 1 day**

Update existing PostgreSQL backend to implement the DbImplInterface:

```python
# vitalgraph/db/postgresql/postgresql_db_impl.py (minimal changes)
from ..db_inf import DbImplInterface

class PostgreSQLDbImpl(DbImplInterface):  # Add interface inheritance
    """
    PostgreSQL database implementation.
    Now implements DbImplInterface for consistency with hybrid backend.
    """
    
    # All existing methods remain unchanged
    # Interface methods already exist with compatible signatures:
    # - connect() -> bool (already exists)
    # - disconnect() -> bool (already exists) 
    # - is_connected() -> bool (already exists)
    # - execute_query() -> List[Dict] (already exists)
    # - execute_update() -> bool (already exists)
    # - begin_transaction() -> Any (already exists)
    # - commit_transaction() -> bool (already exists)
    # - rollback_transaction() -> bool (already exists)
    # - get_connection_info() -> Dict (already exists)
    
    # No implementation changes needed - just interface marking
```

#### 6.5 FUSEKI_POSTGRESQL DbImpl Implementation
**Priority: High**
**Estimated Time: 2 days**

Create PostgreSQL component for hybrid backend implementing DbImplInterface:

```python
# vitalgraph/db/fuseki_postgresql/postgresql_db_impl.py
from ..db_inf import DbImplInterface
import asyncpg
from typing import Dict, Any, Optional, List

# Fuseki-PostgreSQL implementation - See `endpoints/fuseki_psql_backend_plan.md`
class FusekiPostgreSQLDbImpl(DbImplInterface):
    
    def __init__(self, postgresql_config: dict):
        self.config = postgresql_config
        self.connection_pool = None
        self.schema = FusekiPostgreSQLSchema()
    
    async def connect(self) -> bool:
        """Establish PostgreSQL connection pool."""
        try:
            self.connection_pool = await asyncpg.create_pool(
                host=self.config['host'],
                port=self.config['port'],
                database=self.config['database'],
                user=self.config['username'],
                password=self.config['password'],
                min_size=1,
                max_size=10
            )
            return True
        except Exception as e:
            logger.error(f"Failed to connect to PostgreSQL: {e}")
            return False
    
    async def execute_query(self, query: str, params: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """Execute PostgreSQL query and return results."""
        async with self.connection_pool.acquire() as conn:
            if params:
                rows = await conn.fetch(query, *params.values())
            else:
                rows = await conn.fetch(query)
            return [dict(row) for row in rows]
    
    # Implement all other DbImplInterface methods...
```

#### 6.6 SPARQL Implementation
**Status**: ‚úÖ COMPLETED - See `fuseki_psql_sparql_plan.md` for complete SPARQL UPDATE parser implementation, RDFLib integration, and dual-write coordination details.
        """
        
        try:
            # Extract DELETE patterns and WHERE clause
            delete_patterns = self._extract_delete_patterns(parsed_query)
            where_clause = self._extract_where_clause(parsed_query)
            
            # Build SELECT query to find matching triples
            select_query = self._build_resolution_query(delete_patterns, where_clause)
            
            # Execute against Fuseki dataset
            results = await self.fuseki_manager.query_dataset(space_id, select_query)
            
            # Convert SPARQL results to triple format
            return self._sparql_results_to_triples(results)
            
        except Exception as e:
            self.logger.error(f"Error resolving DELETE patterns: {e}")
            return []
    
    def _build_resolution_query(self, delete_patterns: List, where_clause: str) -> str:
        """
        Build SELECT query to resolve DELETE patterns.
        
        Example:
        DELETE { ?s ?p ?o } WHERE { ?s a :Person }
        becomes:
        SELECT ?s ?p ?o WHERE { ?s a :Person . ?s ?p ?o }
        """
        
        # Extract variables from DELETE patterns
        variables = self._extract_variables_from_patterns(delete_patterns)
        
        # Combine WHERE clause with DELETE patterns
        combined_where = f"{where_clause} . {' . '.join(delete_patterns)}"
        
        return f"SELECT {' '.join(variables)} WHERE {{ {combined_where} }}"
```

#### 6.7 Dual-Write System Implementation
**Priority: Critical**
**Estimated Time: 4-5 days**

Implement synchronized writes to both Fuseki and PostgreSQL with SPARQL UPDATE parsing:

```python
# vitalgraph/db/fuseki_postgresql/fuseki_postgresql_space_impl.py
class FusekiPostgreSQLSpaceImpl(SpaceBackendInterface):
    """
    Hybrid implementation combining Fuseki datasets with PostgreSQL backup.
    
    - Backend operations: See `endpoints/fuseki_psql_backend_plan.md` for complete details
    """
    
    def __init__(self, fuseki_config: dict, postgresql_config: dict):
        # Fuseki components
        self.fuseki_manager = FusekiDatasetManager(fuseki_config)
        
        # PostgreSQL components (no SQLAlchemy)
        self.postgresql_impl = FusekiPostgreSQLDbImpl(postgresql_config)
        self.pg_schema = FusekiPostgreSQLSchema()
        
        # Signal manager (PostgreSQL-based)
        self.signal_manager = PostgreSQLSignalManager(postgresql_config)
        
        # SPARQL UPDATE parser
        self.sparql_parser = SPARQLUpdateParser(self.fuseki_manager)
        
        # Dual-write coordinator
        self.dual_write_coordinator = DualWriteCoordinator(
            self.fuseki_manager, self.postgresql_impl, self.sparql_parser
        )
    
    async def execute_sparql_update(self, space_id: str, sparql_update: str) -> bool:
        """
        Execute SPARQL UPDATE with dual-write to both Fuseki and PostgreSQL.
        
        This is the main entry point for graph updates in the hybrid backend.
        Parses the SPARQL UPDATE to determine affected triples, then coordinates
        the dual-write operation.
        """
        try:
            # Parse SPARQL UPDATE to determine affected triples
            parsed_operation = await self.sparql_parser.parse_update_operation(
                space_id, sparql_update
            )
            
            # Execute dual-write operation
            return await self.dual_write_coordinator.execute_parsed_update(
                space_id, parsed_operation
            )
            
        except Exception as e:
            self.logger.error(f"Error executing SPARQL UPDATE: {e}")
            return False
    
    # Legacy quad-based methods (for compatibility)
    async def add_quads(self, space_id: str, quads: List[Quad]) -> bool:
        """Legacy method: Convert quads to INSERT and execute via SPARQL UPDATE."""
        insert_query = self._quads_to_insert_sparql(quads)
        return await self.execute_sparql_update(space_id, insert_query)
    
    async def remove_quads(self, space_id: str, quads: List[Quad]) -> bool:
        """Legacy method: Convert quads to DELETE and execute via SPARQL UPDATE."""
        delete_query = self._quads_to_delete_sparql(quads)
        return await self.execute_sparql_update(space_id, delete_query)
    
    async def query_quads(self, space_id: str, query: str) -> List[Dict]:
        """Primary read: Query Fuseki dataset directly."""
        return await self.fuseki_manager.query_dataset(
            f"vitalgraph_space_{space_id}", query
        )
    
    # Recovery methods - See `endpoints/fuseki_psql_backend_plan.md`
    async def rebuild_fuseki_from_postgresql(self, space_id: str) -> bool:
        # Implementation moved to backend planning file
        pass
        
        # Recreate Fuseki dataset
        await self.fuseki_manager.delete_dataset(f"vitalgraph_space_{space_id}")
        await self.fuseki_manager.create_dataset(f"vitalgraph_space_{space_id}")
        
        # Restore all quads to Fuseki
        return await self.fuseki_manager.add_quads_to_dataset(
            f"vitalgraph_space_{space_id}", primary_quads
        )
```

#### 6.8 Updated Dual-Write Coordinator with SPARQL Parsing
**Priority: Critical**
**Estimated Time: 2 days**

Update the dual-write coordinator to handle parsed SPARQL operations:

```python
# vitalgraph/db/fuseki_postgresql/dual_write_coordinator.py
# DualWriteCoordinator - See `endpoints/fuseki_psql_backend_plan.md`
class DualWriteCoordinator: pass
    
    def __init__(self, fuseki_manager, postgresql_impl, sparql_parser):
        self.fuseki_manager = fuseki_manager
        self.postgresql_impl = postgresql_impl
        self.sparql_parser = sparql_parser
    
    async def execute_parsed_update(self, space_id: str, parsed_operation: Dict) -> bool:
        """
        Execute dual-write operation from parsed SPARQL UPDATE.
        
        Args:
            space_id: Target space
            parsed_operation: Result from SPARQLUpdateParser.parse_update_operation()
                Contains: operation_type, insert_triples, delete_triples, raw_update
        """
        
        operation_type = parsed_operation['operation_type']
        
        try:
            # Step 1: Start PostgreSQL transaction and apply backup changes FIRST
            pg_transaction = await self.postgresql_impl.begin_transaction()
            
            # Apply PostgreSQL backup changes within transaction
            if operation_type in ['delete', 'delete_insert']:
                # Remove deleted triples from backup
                delete_success = await self._remove_triples_from_backup(
                    space_id, parsed_operation['delete_triples'], pg_transaction
                )
                if not delete_success:
                    await self.postgresql_impl.rollback_transaction(pg_transaction)
                    return False
            
            if operation_type in ['insert', 'delete_insert']:
                # Add inserted triples to backup
                insert_success = await self._add_triples_to_backup(
                    space_id, parsed_operation['insert_triples'], pg_transaction
                )
                if not insert_success:
                    await self.postgresql_impl.rollback_transaction(pg_transaction)
                    return False
            
            # Step 2: Commit PostgreSQL transaction BEFORE Fuseki operation
            commit_success = await self.postgresql_impl.commit_transaction(pg_transaction)
            
            if not commit_success:
                return False
            
            # Step 3: Execute SPARQL UPDATE on Fuseki (primary) AFTER PostgreSQL success
            fuseki_success = await self._execute_fuseki_update(
                space_id, parsed_operation['raw_update']
            )
            
            if not fuseki_success:
                # Rollback PostgreSQL changes by applying inverse operation
                await self._rollback_postgresql_changes(space_id, parsed_operation)
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error in dual-write operation: {e}")
            if pg_transaction:
                await self.postgresql_impl.rollback_transaction(pg_transaction)
            await self._rollback_fuseki_update(space_id, parsed_operation)
            return False
    
    async def _rollback_fuseki_update(self, space_id: str, parsed_operation: Dict) -> bool:
        """
        Rollback Fuseki update by applying inverse operation.
        
        For INSERT operations: Generate DELETE for the same triples
        For DELETE operations: Generate INSERT for the deleted triples
        For DELETE/INSERT: Generate inverse DELETE/INSERT
        """
        
        operation_type = parsed_operation['operation_type']
        
        try:
            if operation_type == 'insert':
                # Rollback INSERT by deleting the inserted triples
                rollback_query = self._generate_delete_query(
                    parsed_operation['insert_triples']
                )
            elif operation_type == 'delete':
                # Rollback DELETE by inserting the deleted triples back
                rollback_query = self._generate_insert_query(
                    parsed_operation['delete_triples']
                )
            elif operation_type == 'delete_insert':
                # Rollback DELETE/INSERT by INSERT/DELETE
                rollback_query = self._generate_inverse_delete_insert_query(
                    parsed_operation['delete_triples'],
                    parsed_operation['insert_triples']
                )
            
            return await self._execute_fuseki_update(space_id, rollback_query)
            
        except Exception as e:
            self.logger.error(f"Error rolling back Fuseki update: {e}")
            return False
```

#### 6.9 PostgreSQL Signal Integration
**Priority: Medium**
**Estimated Time: 2 days**

Implement PostgreSQL-based signal manager for real-time notifications:

```python
# vitalgraph/db/fuseki_postgresql/postgresql_signal_manager.py
class PostgreSQLSignalManager(SignalManagerInterface):
    """
    PostgreSQL-based signal implementation using NOTIFY/LISTEN.
    Provides significant enhancement over Fuseki's no-op signal manager.
    """
    
    def __init__(self, postgresql_config: dict):
        self.pg_connection = PostgreSQLConnection(postgresql_config)
        self.listeners = {}
    
    async def emit_signal(self, signal_type: str, data: dict) -> bool:
        """Emit signal using PostgreSQL NOTIFY."""
        payload = json.dumps(data)
        await self.pg_connection.execute(
            f"NOTIFY {signal_type}, '{payload}'"
        )
        return True
    
    async def listen_for_signals(self, signal_type: str, callback) -> bool:
        """Listen for signals using PostgreSQL LISTEN."""
        await self.pg_connection.execute(f"LISTEN {signal_type}")
        self.listeners[signal_type] = callback
        return True
```

#### 6.8 Package Structure Implementation
**Priority: Medium**
**Estimated Time: 1-2 days**

Implement the `vitalgraph/db/fuseki_postgresql` package structure:

```
vitalgraph/db/fuseki_postgresql/
‚îú‚îÄ‚îÄ __init__.py                           # Package initialization
‚îú‚îÄ‚îÄ fuseki_postgresql_space_impl.py      # Main hybrid implementation
‚îú‚îÄ‚îÄ postgresql_db_impl.py                # DbImplInterface implementation
‚îú‚îÄ‚îÄ postgresql_schema.py                 # Schema definitions and migrations
‚îú‚îÄ‚îÄ postgresql_signal_manager.py         # PostgreSQL-based signals
‚îú‚îÄ‚îÄ fuseki_dataset_manager.py           # Fuseki dataset operations
‚îú‚îÄ‚îÄ dual_write_coordinator.py           # Synchronization logic
‚îî‚îÄ‚îÄ backup_recovery_manager.py          # Disaster recovery operations
```

#### 6.9 Testing Strategy for Hybrid Backend
**Priority: High**
**Estimated Time: 2-3 days**

Comprehensive testing without affecting current Fuseki implementation:

```python
# test_fuseki_postgresql_backend.py
class TestFusekiPostgreSQLBackend:
    """Test hybrid backend implementation independently."""
    
    async def test_dual_write_consistency(self):
        """Verify data consistency between Fuseki and PostgreSQL."""
        
    async def test_disaster_recovery(self):
        """Test rebuilding Fuseki from PostgreSQL backup."""
        
    async def test_performance_comparison(self):
        """Compare performance vs pure Fuseki and pure PostgreSQL."""
        
    async def test_signal_integration(self):
        """Test PostgreSQL-based signal notifications."""
        
    async def test_concurrent_operations(self):
        """Test concurrent reads/writes across both systems."""
```

#### 6.10 PyOxigraph In-Memory Testing Framework
**Priority: Critical**
**Estimated Time: 3-4 days**

Implement comprehensive SPARQL UPDATE testing using pyoxigraph in-memory store:

```python
# vitalgraph/db/fuseki_postgresql/test_sparql_update_parser.py
import pyoxigraph
from typing import Dict, List, Any
import pytest
from .sparql_update_parser import SPARQLUpdateParser

class SPARQLUpdateTestFramework:
    """
    Comprehensive testing framework for SPARQL UPDATE operations using pyoxigraph.
    
    Uses in-memory RDF store to:
    1. Insert test data
    2. Execute SPARQL UPDATE operations
    3. Validate before/after triple states
    4. Test all SPARQL UPDATE operation types
    """
    
    def __init__(self):
        self.store = None
        self.parser = None
        
    def setup_test_store(self) -> pyoxigraph.Store:
        """Create fresh in-memory pyoxigraph store for testing."""
        self.store = pyoxigraph.Store()
        return self.store
    
    def load_test_data(self, test_case: str) -> None:
        """
        Load predefined test data sets for different SPARQL UPDATE scenarios.
        
        Test Cases:
        - 'basic_persons': Person entities with names, ages, emails
        - 'organizations': Company entities with employees, locations
        - 'complex_graph': Multi-graph data with named graphs
        - 'datatypes': Various RDF datatypes (strings, integers, dates, etc.)
        """
        
        test_data_sets = {
            'basic_persons': """
                @prefix : <http://example.org/> .
                @prefix foaf: <http://xmlns.com/foaf/0.1/> .
                
                :john a foaf:Person ;
                    foaf:name "John Doe" ;
                    foaf:age 30 ;
                    foaf:email "john@example.com" .
                
                :jane a foaf:Person ;
                    foaf:name "Jane Smith" ;
                    foaf:age 25 ;
                    foaf:email "jane@example.com" .
                
                :bob a foaf:Person ;
                    foaf:name "Bob Wilson" ;
                    foaf:age 35 .
            """,
            
            'organizations': """
                @prefix : <http://example.org/> .
                @prefix org: <http://www.w3.org/ns/org#> .
                
                :acme a org:Organization ;
                    org:name "Acme Corp" ;
                    org:location "New York" .
                
                :john org:memberOf :acme ;
                    org:role "Developer" .
                
                :jane org:memberOf :acme ;
                    org:role "Manager" .
            """,
            
            'complex_graph': """
                @prefix : <http://example.org/> .
                
                GRAPH :graph1 {
                    :s1 :p1 :o1 .
                    :s1 :p2 "value1" .
                }
                
                GRAPH :graph2 {
                    :s2 :p1 :o2 .
                    :s2 :p3 42 .
                }
            """
        }
        
        if test_case in test_data_sets:
            self.store.load(test_data_sets[test_case].encode(), "text/turtle")
    
    def get_all_triples(self) -> List[Dict[str, str]]:
        """Get all triples from store as list of dictionaries for comparison."""
        triples = []
        for triple in self.store:
            triples.append({
                'subject': str(triple.subject),
                'predicate': str(triple.predicate),
                'object': str(triple.object),
                'graph': str(triple.graph_name) if triple.graph_name else None
            })
        return sorted(triples, key=lambda x: (x['subject'], x['predicate'], x['object']))
    
    def execute_sparql_update(self, update_query: str) -> bool:
        """Execute SPARQL UPDATE on the in-memory store."""
        try:
            self.store.update(update_query)
            return True
        except Exception as e:
            print(f"SPARQL UPDATE failed: {e}")
            return False
    
    def query_triples(self, sparql_query: str) -> List[Dict]:
        """Execute SPARQL SELECT query and return results."""
        try:
            results = []
            for solution in self.store.query(sparql_query):
                result = {}
                for var_name, value in solution:
                    result[str(var_name)] = str(value)
                results.append(result)
            return results
        except Exception as e:
            print(f"SPARQL query failed: {e}")
            return []

# Comprehensive test cases covering all SPARQL UPDATE operations
class TestSPARQLUpdateOperations:
    """Test suite covering all SPARQL UPDATE operation types."""
    
    def setup_method(self):
        """Setup fresh test environment for each test."""
        self.framework = SPARQLUpdateTestFramework()
        self.framework.setup_test_store()
    
    def test_simple_insert(self):
        """Test basic INSERT DATA operation."""
        # Setup
        self.framework.load_test_data('basic_persons')
        before_triples = self.framework.get_all_triples()
        
        # Execute INSERT
        insert_query = """
            PREFIX : <http://example.org/>
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            
            INSERT DATA {
                :alice a foaf:Person ;
                    foaf:name "Alice Brown" ;
                    foaf:age 28 .
            }
        """
        
        success = self.framework.execute_sparql_update(insert_query)
        after_triples = self.framework.get_all_triples()
        
        # Validate
        assert success
        assert len(after_triples) == len(before_triples) + 3  # 3 new triples
        
        # Check specific new triples exist
        alice_triples = [t for t in after_triples if ':alice' in t['subject']]
        assert len(alice_triples) == 3
    
    def test_simple_delete(self):
        """Test basic DELETE DATA operation."""
        # Setup
        self.framework.load_test_data('basic_persons')
        before_triples = self.framework.get_all_triples()
        
        # Execute DELETE
        delete_query = """
            PREFIX : <http://example.org/>
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            
            DELETE DATA {
                :bob foaf:email "bob@example.com" .
            }
        """
        
        success = self.framework.execute_sparql_update(delete_query)
        after_triples = self.framework.get_all_triples()
        
        # Validate
        assert success
        # Should be same count since Bob didn't have email in test data
        assert len(after_triples) == len(before_triples)
    
    def test_conditional_delete(self):
        """Test DELETE WHERE operation with query patterns."""
        # Setup
        self.framework.load_test_data('basic_persons')
        before_triples = self.framework.get_all_triples()
        
        # Execute conditional DELETE
        delete_query = """
            PREFIX : <http://example.org/>
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            
            DELETE { 
                ?person foaf:email ?email 
            }
            WHERE { 
                ?person a foaf:Person .
                ?person foaf:email ?email .
                ?person foaf:age ?age .
                FILTER(?age > 28)
            }
        """
        
        success = self.framework.execute_sparql_update(delete_query)
        after_triples = self.framework.get_all_triples()
        
        # Validate - should remove John's email (age 30) but keep Jane's (age 25)
        assert success
        john_email_triples = [t for t in after_triples 
                             if ':john' in t['subject'] and 'email' in t['predicate']]
        jane_email_triples = [t for t in after_triples 
                             if ':jane' in t['subject'] and 'email' in t['predicate']]
        
        assert len(john_email_triples) == 0  # John's email removed
        assert len(jane_email_triples) == 1  # Jane's email kept
    
    def test_delete_insert_operation(self):
        """Test combined DELETE/INSERT operation."""
        # Setup
        self.framework.load_test_data('basic_persons')
        
        # Execute DELETE/INSERT to update John's age
        update_query = """
            PREFIX : <http://example.org/>
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            
            DELETE { 
                :john foaf:age ?oldAge 
            }
            INSERT { 
                :john foaf:age 31 
            }
            WHERE { 
                :john foaf:age ?oldAge 
            }
        """
        
        success = self.framework.execute_sparql_update(update_query)
        
        # Validate - John's age should be updated to 31
        assert success
        
        john_age_query = """
            PREFIX : <http://example.org/>
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            SELECT ?age WHERE { :john foaf:age ?age }
        """
        
        results = self.framework.query_triples(john_age_query)
        assert len(results) == 1
        assert results[0]['age'] == '31'
    
    def test_graph_operations(self):
        """Test SPARQL UPDATE operations with named graphs."""
        # Setup
        self.framework.load_test_data('complex_graph')
        
        # Insert into specific graph
        insert_query = """
            PREFIX : <http://example.org/>
            
            INSERT DATA {
                GRAPH :graph1 {
                    :s3 :p4 "new_value" .
                }
            }
        """
        
        success = self.framework.execute_sparql_update(insert_query)
        assert success
        
        # Query specific graph
        graph_query = """
            PREFIX : <http://example.org/>
            SELECT ?s ?p ?o WHERE { 
                GRAPH :graph1 { ?s ?p ?o } 
            }
        """
        
        results = self.framework.query_triples(graph_query)
        assert len(results) >= 3  # Original 2 + new 1
    
    def test_complex_filter_operations(self):
        """Test SPARQL UPDATE with complex FILTER conditions."""
        # Setup
        self.framework.load_test_data('basic_persons')
        
        # Delete persons with names starting with 'J'
        delete_query = """
            PREFIX : <http://example.org/>
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            
            DELETE { 
                ?person ?p ?o 
            }
            WHERE { 
                ?person a foaf:Person .
                ?person foaf:name ?name .
                ?person ?p ?o .
                FILTER(STRSTARTS(?name, "J"))
            }
        """
        
        success = self.framework.execute_sparql_update(delete_query)
        assert success
        
        # Validate - only Bob should remain
        remaining_persons = self.framework.query_triples("""
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            SELECT ?person WHERE { ?person a foaf:Person }
        """)
        
        assert len(remaining_persons) == 1
        assert ':bob' in remaining_persons[0]['person']
    
    def test_datatype_operations(self):
        """Test SPARQL UPDATE with various RDF datatypes."""
        # Insert data with different datatypes
        insert_query = """
            PREFIX : <http://example.org/>
            PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
            
            INSERT DATA {
                :entity1 :stringProp "text value" ;
                         :intProp 42 ;
                         :floatProp 3.14 ;
                         :dateProp "2024-01-01"^^xsd:date ;
                         :boolProp true .
            }
        """
        
        success = self.framework.execute_sparql_update(insert_query)
        assert success
        
        # Query and validate datatypes
        datatype_query = """
            PREFIX : <http://example.org/>
            SELECT ?prop ?value WHERE { 
                :entity1 ?prop ?value 
            }
        """
        
        results = self.framework.query_triples(datatype_query)
        assert len(results) == 5  # All 5 properties
    
    def test_parser_integration(self):
        """Test integration with SPARQLUpdateParser to validate parsing logic."""
        # This would test the actual parser we're building
        # Mock fuseki_manager for parser testing
        
        class MockFusekiManager:
            def __init__(self, test_store):
                self.test_store = test_store
            
            async def query_dataset(self, space_id, query):
                # Convert pyoxigraph results to expected format
                results = []
                for solution in self.test_store.query(query):
                    result = {}
                    for var_name, value in solution:
                        result[str(var_name)] = {'value': str(value), 'type': 'uri'}
                    results.append(result)
                return results
        
        # Setup
        self.framework.load_test_data('basic_persons')
        mock_fuseki = MockFusekiManager(self.framework.store)
        parser = SPARQLUpdateParser(mock_fuseki)
        
        # Test parsing DELETE operation
        delete_query = """
            PREFIX foaf: <http://xmlns.com/foaf/0.1/>
            DELETE { ?person foaf:email ?email }
            WHERE { ?person a foaf:Person . ?person foaf:email ?email }
        """
        
        # This would test our actual parser implementation
        # parsed_result = await parser.parse_update_operation('test_space', delete_query)
        # assert parsed_result['operation_type'] == 'delete'
        # assert len(parsed_result['delete_triples']) > 0
```

**Test Coverage Requirements:**

1. **Basic Operations**: INSERT DATA, DELETE DATA
2. **Conditional Operations**: DELETE WHERE, INSERT WHERE  
3. **Combined Operations**: DELETE/INSERT WHERE
4. **Graph Operations**: Named graph insertions/deletions
5. **Filter Operations**: Complex FILTER conditions (STRSTARTS, regex, numeric, date)
6. **Datatype Operations**: String, integer, float, date, boolean literals
7. **Variable Binding**: Complex WHERE clauses with multiple variables
8. **Edge Cases**: Empty results, malformed queries, constraint violations

**Integration with Parser Testing:**

The framework will validate that:
- Parser correctly identifies operation types
- Query-before-delete finds the right triples  
- All SPARQL UPDATE variations are supported
- Arbitrary WHERE clause queries work correctly

```

#### 6.11 Standalone SPARQL Operations Class
**Priority: High**
**Estimated Time: 2-3 days**

Create a separate, standalone class for SPARQL operations that can be tested independently:

```python
# vitalgraph/db/fuseki_postgresql/sparql_operations.py
import pyoxigraph
from rdflib import Graph
from rdflib.plugins.sparql import prepareQuery
from typing import Dict, List, Any, Optional, Tuple
import logging

class SPARQLOperationsEngine:
    """
    Standalone SPARQL operations engine for testing and validation.
    
    This class encapsulates all SPARQL UPDATE parsing and execution logic
    in a way that can be tested independently of the full hybrid backend.
    Uses pyoxigraph as the in-memory RDF store for validation.
    """
    
    def __init__(self, store: Optional[pyoxigraph.Store] = None):
        """
        Initialize SPARQL operations engine.
        
        Args:
            store: Optional pyoxigraph store. If None, creates in-memory store.
        """
        self.store = store or pyoxigraph.Store()
        self.logger = logging.getLogger(__name__)
    
    def load_turtle_data(self, turtle_data: str) -> bool:
        """Load RDF data from Turtle format into the store."""
        try:
            self.store.load(turtle_data.encode(), "text/turtle")
            return True
        except Exception as e:
            self.logger.error(f"Error loading Turtle data: {e}")
            return False
    
    def execute_sparql_update(self, update_query: str) -> bool:
        """Execute SPARQL UPDATE operation on the store."""
        try:
            self.store.update(update_query)
            return True
        except Exception as e:
            self.logger.error(f"SPARQL UPDATE failed: {e}")
            return False
    
    def execute_sparql_query(self, select_query: str) -> List[Dict[str, str]]:
        """Execute SPARQL SELECT query and return results."""
        try:
            results = []
            for solution in self.store.query(select_query):
                result = {}
                for var_name, value in solution:
                    result[str(var_name)] = str(value)
                results.append(result)
            return results
        except Exception as e:
            self.logger.error(f"SPARQL query failed: {e}")
            return []
    
    def get_all_triples(self) -> List[Dict[str, str]]:
        """Get all triples from store as list of dictionaries."""
        triples = []
        for triple in self.store:
            triples.append({
                'subject': str(triple.subject),
                'predicate': str(triple.predicate),
                'object': str(triple.object),
                'graph': str(triple.graph_name) if triple.graph_name else None
            })
        return sorted(triples, key=lambda x: (x['subject'], x['predicate'], x['object']))
    
    def count_triples(self) -> int:
        """Count total number of triples in the store."""
        return len(list(self.store))
    
    def clear_store(self) -> None:
        """Clear all data from the store."""
        # Create new empty store
        self.store = pyoxigraph.Store()
    
    def parse_sparql_update(self, update_query: str) -> Dict[str, Any]:
        """
        Parse SPARQL UPDATE query to extract operation details.
        
        Returns:
            Dictionary containing:
            - operation_type: 'insert', 'delete', 'delete_insert', 'insert_data', 'delete_data'
            - insert_patterns: List of INSERT patterns (if any)
            - delete_patterns: List of DELETE patterns (if any)
            - where_clause: WHERE clause (if any)
            - raw_query: Original query string
        """
        
        try:
            # Use rdflib to parse the query
            parsed_query = prepareQuery(update_query)
            
            result = {
                'operation_type': self._identify_operation_type(update_query),
                'insert_patterns': [],
                'delete_patterns': [],
                'where_clause': None,
                'raw_query': update_query
            }
            
            # Extract patterns based on operation type
            result.update(self._extract_patterns_from_query(update_query))
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error parsing SPARQL UPDATE: {e}")
            return {
                'operation_type': 'unknown',
                'insert_patterns': [],
                'delete_patterns': [],
                'where_clause': None,
                'raw_query': update_query,
                'error': str(e)
            }
    
    def resolve_delete_patterns(self, delete_patterns: List[str], where_clause: str) -> List[Dict[str, str]]:
        """
        Resolve DELETE patterns by executing SELECT query to find matching triples.
        
        This implements the "query-before-delete" strategy by converting
        DELETE patterns + WHERE clause into a SELECT query.
        """
        
        try:
            # Build SELECT query from DELETE patterns and WHERE clause
            select_query = self._build_select_from_delete(delete_patterns, where_clause)
            
            # Execute query to find matching triples
            results = self.execute_sparql_query(select_query)
            
            # Convert results to triple format
            return self._convert_results_to_triples(results, delete_patterns)
            
        except Exception as e:
            self.logger.error(f"Error resolving DELETE patterns: {e}")
            return []
    
    def validate_sparql_update_operation(self, update_query: str) -> Dict[str, Any]:
        """
        Comprehensive validation of SPARQL UPDATE operation.
        
        Returns validation report with:
        - syntax_valid: Boolean indicating if syntax is valid
        - operation_details: Parsed operation details
        - affected_triples_before: Triples that would be affected (for DELETE operations)
        - execution_result: Result of executing the operation
        - affected_triples_after: Actual changes made
        """
        
        validation_report = {
            'syntax_valid': False,
            'operation_details': {},
            'affected_triples_before': [],
            'execution_result': False,
            'affected_triples_after': [],
            'triple_count_before': 0,
            'triple_count_after': 0
        }
        
        try:
            # Step 1: Parse the query
            operation_details = self.parse_sparql_update(update_query)
            validation_report['operation_details'] = operation_details
            
            if 'error' not in operation_details:
                validation_report['syntax_valid'] = True
            
            # Step 2: Get current state
            triples_before = self.get_all_triples()
            validation_report['triple_count_before'] = len(triples_before)
            
            # Step 3: For DELETE operations, resolve affected triples
            if operation_details['operation_type'] in ['delete', 'delete_insert']:
                if operation_details['where_clause']:
                    affected_triples = self.resolve_delete_patterns(
                        operation_details['delete_patterns'],
                        operation_details['where_clause']
                    )
                    validation_report['affected_triples_before'] = affected_triples
            
            # Step 4: Execute the operation
            execution_success = self.execute_sparql_update(update_query)
            validation_report['execution_result'] = execution_success
            
            # Step 5: Get final state
            triples_after = self.get_all_triples()
            validation_report['triple_count_after'] = len(triples_after)
            
            # Step 6: Calculate actual changes
            validation_report['affected_triples_after'] = self._calculate_triple_changes(
                triples_before, triples_after
            )
            
            return validation_report
            
        except Exception as e:
            validation_report['error'] = str(e)
            return validation_report
    
    # Internal helper methods
    
    def _identify_operation_type(self, query: str) -> str:
        """Identify the type of SPARQL UPDATE operation."""
        query_upper = query.upper()
        
        if 'INSERT DATA' in query_upper:
            return 'insert_data'
        elif 'DELETE DATA' in query_upper:
            return 'delete_data'
        elif 'DELETE' in query_upper and 'INSERT' in query_upper:
            return 'delete_insert'
        elif 'DELETE' in query_upper:
            return 'delete'
        elif 'INSERT' in query_upper:
            return 'insert'
        else:
            return 'unknown'
    
    def _extract_patterns_from_query(self, query: str) -> Dict[str, Any]:
        """Extract INSERT/DELETE patterns and WHERE clause from query."""
        # Simplified pattern extraction - would need more robust parsing
        result = {
            'insert_patterns': [],
            'delete_patterns': [],
            'where_clause': None
        }
        
        # This would be implemented with proper SPARQL parsing
        # For now, return placeholder
        return result
    
    def _build_select_from_delete(self, delete_patterns: List[str], where_clause: str) -> str:
        """Build SELECT query from DELETE patterns and WHERE clause."""
        # Extract variables from DELETE patterns
        variables = []
        for pattern in delete_patterns:
            # Extract ?variables from pattern
            import re
            vars_in_pattern = re.findall(r'\?(\w+)', pattern)
            variables.extend(vars_in_pattern)
        
        # Remove duplicates and format
        unique_vars = list(set(variables))
        select_vars = ' '.join([f'?{var}' for var in unique_vars])
        
        # Combine WHERE clause with DELETE patterns
        combined_where = f"{where_clause} . {' . '.join(delete_patterns)}"
        
        return f"SELECT {select_vars} WHERE {{ {combined_where} }}"
    
    def _convert_results_to_triples(self, results: List[Dict], patterns: List[str]) -> List[Dict[str, str]]:
        """Convert SPARQL SELECT results to triple format."""
        # This would convert variable bindings back to concrete triples
        # Implementation depends on the specific patterns and variables
        return []
    
    def _calculate_triple_changes(self, before: List[Dict], after: List[Dict]) -> Dict[str, List]:
        """Calculate the differences between before and after triple sets."""
        before_set = set(str(t) for t in before)
        after_set = set(str(t) for t in after)
        
        added = [t for t in after if str(t) not in before_set]
        removed = [t for t in before if str(t) not in after_set]
        
        return {
            'added_triples': added,
            'removed_triples': removed,
            'added_count': len(added),
            'removed_count': len(removed)
        }
```

#### 6.12 Standalone Test Scripts (Non-Pytest)
**Priority: Medium**
**Estimated Time: 2 days**

Create simple test scripts that can be run directly to validate SPARQL operations:

```python
# vitalgraph/db/fuseki_postgresql/test_scripts/test_sparql_operations.py
"""
Standalone test script for SPARQL operations validation.
Run directly with: python test_sparql_operations.py
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sparql_operations import SPARQLOperationsEngine

def test_basic_insert():
    """Test basic INSERT DATA operation."""
    print("=== Testing Basic INSERT ===")
    
    engine = SPARQLOperationsEngine()
    
    # Load initial data
    initial_data = """
        @prefix : <http://example.org/> .
        @prefix foaf: <http://xmlns.com/foaf/0.1/> .
        
        :john a foaf:Person ;
            foaf:name "John Doe" .
    """
    
    engine.load_turtle_data(initial_data)
    before_count = engine.count_triples()
    print(f"Initial triple count: {before_count}")
    
    # Execute INSERT
    insert_query = """
        PREFIX : <http://example.org/>
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        
        INSERT DATA {
            :jane a foaf:Person ;
                foaf:name "Jane Smith" ;
                foaf:age 25 .
        }
    """
    
    success = engine.execute_sparql_update(insert_query)
    after_count = engine.count_triples()
    
    print(f"INSERT success: {success}")
    print(f"Final triple count: {after_count}")
    print(f"Triples added: {after_count - before_count}")
    
    # Validate
    if success and after_count == before_count + 3:
        print("‚úÖ Basic INSERT test PASSED")
    else:
        print("‚ùå Basic INSERT test FAILED")
    
    print()

def test_conditional_delete():
    """Test DELETE WHERE operation."""
    print("=== Testing Conditional DELETE ===")
    
    engine = SPARQLOperationsEngine()
    
    # Load test data
    test_data = """
        @prefix : <http://example.org/> .
        @prefix foaf: <http://xmlns.com/foaf/0.1/> .
        
        :john a foaf:Person ;
            foaf:name "John Doe" ;
            foaf:age 30 ;
            foaf:email "john@example.com" .
        
        :jane a foaf:Person ;
            foaf:name "Jane Smith" ;
            foaf:age 25 ;
            foaf:email "jane@example.com" .
    """
    
    engine.load_turtle_data(test_data)
    
    # Get validation report
    delete_query = """
        PREFIX : <http://example.org/>
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        
        DELETE { 
            ?person foaf:email ?email 
        }
        WHERE { 
            ?person a foaf:Person .
            ?person foaf:email ?email .
            ?person foaf:age ?age .
            FILTER(?age > 28)
        }
    """
    
    validation_report = engine.validate_sparql_update_operation(delete_query)
    
    print(f"Syntax valid: {validation_report['syntax_valid']}")
    print(f"Operation type: {validation_report['operation_details']['operation_type']}")
    print(f"Execution success: {validation_report['execution_result']}")
    print(f"Triples before: {validation_report['triple_count_before']}")
    print(f"Triples after: {validation_report['triple_count_after']}")
    print(f"Changes: {validation_report['affected_triples_after']}")
    
    # Validate that only John's email was removed (age > 28)
    remaining_emails = engine.execute_sparql_query("""
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        SELECT ?person ?email WHERE { 
            ?person foaf:email ?email 
        }
    """)
    
    print(f"Remaining emails: {len(remaining_emails)}")
    
    if len(remaining_emails) == 1 and 'jane' in str(remaining_emails[0]):
        print("‚úÖ Conditional DELETE test PASSED")
    else:
        print("‚ùå Conditional DELETE test FAILED")
    
    print()

def test_delete_insert_operation():
    """Test combined DELETE/INSERT operation."""
    print("=== Testing DELETE/INSERT ===")
    
    engine = SPARQLOperationsEngine()
    
    # Load test data
    test_data = """
        @prefix : <http://example.org/> .
        @prefix foaf: <http://xmlns.com/foaf/0.1/> .
        
        :john a foaf:Person ;
            foaf:name "John Doe" ;
            foaf:age 30 .
    """
    
    engine.load_turtle_data(test_data)
    
    # Update John's age
    update_query = """
        PREFIX : <http://example.org/>
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        
        DELETE { 
            :john foaf:age ?oldAge 
        }
        INSERT { 
            :john foaf:age 31 
        }
        WHERE { 
            :john foaf:age ?oldAge 
        }
    """
    
    validation_report = engine.validate_sparql_update_operation(update_query)
    
    print(f"Operation successful: {validation_report['execution_result']}")
    
    # Check John's new age
    age_results = engine.execute_sparql_query("""
        PREFIX : <http://example.org/>
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        SELECT ?age WHERE { :john foaf:age ?age }
    """)
    
    if age_results and age_results[0]['age'] == '31':
        print("‚úÖ DELETE/INSERT test PASSED")
    else:
        print("‚ùå DELETE/INSERT test FAILED")
    
    print()

def test_complex_filters():
    """Test SPARQL UPDATE with complex FILTER conditions."""
    print("=== Testing Complex Filters ===")
    
    engine = SPARQLOperationsEngine()
    
    # Load test data with various names
    test_data = """
        @prefix : <http://example.org/> .
        @prefix foaf: <http://xmlns.com/foaf/0.1/> .
        
        :john a foaf:Person ; foaf:name "John Doe" .
        :jane a foaf:Person ; foaf:name "Jane Smith" .
        :bob a foaf:Person ; foaf:name "Bob Wilson" .
        :alice a foaf:Person ; foaf:name "Alice Brown" .
    """
    
    engine.load_turtle_data(test_data)
    
    # Delete persons with names starting with 'J'
    delete_query = """
        PREFIX : <http://example.org/>
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        
        DELETE { 
            ?person ?p ?o 
        }
        WHERE { 
            ?person a foaf:Person .
            ?person foaf:name ?name .
            ?person ?p ?o .
            FILTER(STRSTARTS(?name, "J"))
        }
    """
    
    validation_report = engine.validate_sparql_update_operation(delete_query)
    
    # Check remaining persons
    remaining_persons = engine.execute_sparql_query("""
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        SELECT ?person ?name WHERE { 
            ?person a foaf:Person .
            ?person foaf:name ?name 
        }
    """)
    
    print(f"Remaining persons: {len(remaining_persons)}")
    for person in remaining_persons:
        print(f"  - {person['name']}")
    
    # Should only have Bob and Alice left
    if len(remaining_persons) == 2:
        names = [p['name'] for p in remaining_persons]
        if 'Bob Wilson' in names and 'Alice Brown' in names:
            print("‚úÖ Complex Filters test PASSED")
        else:
            print("‚ùå Complex Filters test FAILED - wrong names remaining")
    else:
        print("‚ùå Complex Filters test FAILED - wrong count")
    
    print()

def run_all_tests():
    """Run all SPARQL operation tests."""
    print("üß™ SPARQL Operations Test Suite")
    print("=" * 50)
    
    test_basic_insert()
    test_conditional_delete()
    test_delete_insert_operation()
    test_complex_filters()
    
    print("=" * 50)
    print("‚úÖ All tests completed!")

if __name__ == "__main__":
    run_all_tests()
```

**Additional Test Scripts:**

```bash
# vitalgraph/db/fuseki_postgresql/test_scripts/run_tests.sh
#!/bin/bash

echo "Running SPARQL Operations Test Suite..."
echo "======================================"

# Run main test script
python test_sparql_operations.py

echo ""
echo "Running parser validation tests..."
python test_parser_validation.py

echo ""
echo "Running performance benchmarks..."
python test_performance_benchmarks.py

echo ""
echo "All test scripts completed!"
```

**Benefits of Standalone Testing:**

1. **Independent Validation**: Test SPARQL operations without full backend dependencies
2. **Direct Execution**: Run with `python test_script.py` - no pytest setup required
3. **Clear Output**: Human-readable test results with ‚úÖ/‚ùå indicators
4. **Modular Testing**: Each operation type can be tested separately
5. **Development Workflow**: Quick validation during development
6. **CI/CD Integration**: Simple scripts that can be run in any environment

```

## Timeline

**FUSEKI_POSTGRESQL Hybrid Backend Implementation (Phase 6 Only)**
**Total Estimated Time: 3 weeks**

**Week 1: Hybrid Architecture Design & PostgreSQL Schema**
- Design and implement hybrid architecture in `vitalgraph/db/fuseki_postgresql` package
- PostgreSQL schema without SQLAlchemy (admin tables + per-space primary data tables)
- Direct PostgreSQL connection implementation
- **No modifications to existing Fuseki implementation**

**Week 2: Dual-Write System & Signal Integration**
- Dual-write system implementation (Fuseki + PostgreSQL sync)
- PostgreSQL signal integration using NOTIFY/LISTEN
- Backup/recovery system from PostgreSQL to Fuseki
- **Independent implementation - does not affect current Fuseki backend**

**Week 3: Testing & Validation**
- Comprehensive testing of hybrid approach
- Performance comparison vs existing backends
- Disaster recovery testing
- Production deployment preparation for hybrid backend

**Note**: This plan focuses exclusively on the FUSEKI_POSTGRESQL hybrid backend. The existing Fuseki implementation (`vitalgraph/db/fuseki/`) remains unchanged and the multi-dataset Fuseki architecture is a separate future consideration.

## Backend Type Integration

**Current Backend Types:**
- `BackendType.POSTGRESQL` - Pure PostgreSQL with SQLAlchemy (existing)
- `BackendType.FUSEKI` - Pure Fuseki with single dataset (interim)
- `BackendType.FUSEKI` - Pure Fuseki with multi-dataset (target)
- `BackendType.FUSEKI_POSTGRESQL` - Hybrid Fuseki + PostgreSQL (new)

**FUSEKI_POSTGRESQL Benefits:**
- **Performance**: Fast graph queries via Fuseki datasets
- **Reliability**: PostgreSQL backup for disaster recovery
- **Scalability**: Dataset per space with PostgreSQL metadata
- **Signals**: PostgreSQL-based real-time notifications
- **No SQLAlchemy**: Direct PostgreSQL connections for optimal performance
- **Independent Development**: Can be implemented without affecting current Fuseki backend

**Implementation Strategy:**
1. Develop FUSEKI_POSTGRESQL hybrid backend as new independent implementation
2. Use existing Fuseki implementation as-is for graph operations
3. Add PostgreSQL layer for metadata and backup without modifying Fuseki code
4. Test hybrid approach independently of existing backends
5. Multi-dataset Fuseki architecture remains a separate future consideration

## Phase 7: VitalGraph Endpoint Integration with Fuseki+PostgreSQL Backend

### 7.1 Endpoint Backend Integration Strategy
**Priority: Critical**
**Estimated Time: 2-3 days**

After implementing the Fuseki+PostgreSQL hybrid backend, the next critical step is connecting VitalGraph's REST API endpoints to use this new backend. This involves updating endpoint implementations to leverage the SPARQL UPDATE functionality already implemented in the backend.

**Key Endpoints Requiring Integration:**
- **KGEntities Endpoint** - Entity CRUD operations via SPARQL UPDATE
- **KGFrames Endpoint** - Frame management via SPARQL UPDATE  
- **KGTypes Endpoint** - See `endpoints/fuseki_psql_kgtypes_endpoint_plan.md`
- **Objects Endpoint** - See `endpoints/fuseki_psql_objects_endpoint_plan.md` for detailed implementation
- **Files Endpoint** - File metadata operations (PostgreSQL) + binary storage
- **Graphs Endpoint** - See `endpoints/fuseki_psql_graphs_endpoint_plan.md` for detailed implementation

### 7.2 KGEntities Frame Creation Refactoring Plan
**Priority: High**
**Estimated Time: 3-4 days**

**CURRENT STATUS**: Frame functionality has been partially implemented in `_create_or_update_frames` method but needs refactoring to follow the kg_impl processor pattern.

**EXISTING IMPLEMENTATION ANALYSIS**:
- ‚úÖ Frame creation logic exists in `KGEntitiesEndpoint._create_or_update_frames()`
- ‚úÖ Handles Edge_hasEntityKGFrame creation for entity-to-frame linking
- ‚úÖ Supports frame categorization (KGFrame, KGSlot subclasses, Edge objects)
- ‚úÖ Implements dual grouping URI assignment (hasKGGraphURI + frameGraphURI)
- ‚úÖ Handles UPDATE/UPSERT operations with frame member deletion
- ‚ùå All logic is in endpoint layer - needs extraction to kg_impl processors

**REFACTORING OBJECTIVE**: Extract existing frame logic into dedicated kg_impl processors following established patterns.

#### 7.2.1 Frame Creation Use Cases

**Use Case 1: Adding Frame to Existing KGEntity**
- **Scenario**: Create a new frame and link it to an existing KGEntity
- **Components**: Frame object, Edge_hasEntityKGFrame linking object, proper grouping URIs
- **Processor**: `KGEntityFrameCreateProcessor` in kg_impl layer
- **Endpoint Method**: `_create_entity_frame()` in KGEntitiesEndpoint

**Use Case 2: Creating KGEntity with Complete Frame Graph**
- **Scenario**: Create KGEntity along with one or more complete frame graphs
- **Components**: KGEntity + Frame + Slot + Edge objects with proper relationships
- **Frame Graph Structure**: KGEntity ‚Üí Frame ‚Üí Frame ‚Üí Slot (hierarchical frames)
- **Processor**: Enhanced `KGEntityCreateProcessor` to handle frame graphs
- **Endpoint Method**: Enhanced `_create_entities()` to detect and handle frame graphs

#### 7.2.2 Frame Creation Architecture

**Core Components for Frame Creation:**

```python
# Frame Graph Structure
KGEntity (URI: entity_uri)
‚îú‚îÄ‚îÄ Edge_hasEntityKGFrame (sourceURI: entity_uri, destinationURI: frame_uri)
‚îú‚îÄ‚îÄ KGFrame (URI: frame_uri, kgGraphURI: entity_uri)
‚îÇ   ‚îú‚îÄ‚îÄ Edge_hasKGFrame (sourceURI: frame_uri, destinationURI: subframe_uri) 
‚îÇ   ‚îú‚îÄ‚îÄ KGFrame (URI: subframe_uri, kgGraphURI: entity_uri)
‚îÇ   ‚îî‚îÄ‚îÄ Edge_hasKGSlot (sourceURI: frame_uri, destinationURI: slot_uri)
‚îî‚îÄ‚îÄ KGSlot (URI: slot_uri, kgGraphURI: entity_uri)
```

**Grouping URI Strategy:**
- **Entity-level grouping** (`hasKGGraphURI`): All objects point to entity URI for complete entity graph retrieval
- **Frame-level grouping** (`hasFrameGraphURI`): Frame components point to frame URI for frame-specific retrieval
- **Dual grouping assignment**: Objects get both grouping URIs for flexible retrieval patterns

#### 7.2.3 KGEntityFrameCreateProcessor Implementation

**New Processor: `KGEntityFrameCreateProcessor`** (Extract from existing `_create_or_update_frames`)

```python
# vitalgraph/kg_impl/kgentity_frame_create_impl.py
class KGEntityFrameCreateProcessor:
    """
    Processor for creating frames and linking them to existing KGEntities.
    
    REFACTORING SOURCE: Extract logic from KGEntitiesEndpoint._create_or_update_frames()
    
    Handles:
    - Frame object creation with proper properties
    - Edge_hasEntityKGFrame creation for entity-frame linking  
    - Grouping URI assignment (entity-level + frame-level)
    - Frame graph validation and structure analysis
    - Backend integration for atomic frame creation operations
    - UPDATE/UPSERT operations with existing frame deletion
    """
    
    async def create_entity_frame(self, backend_adapter, space_id: str, graph_id: str, 
                                 entity_uri: str, frame_objects: List[GraphObject], 
                                 operation_mode: OperationMode = OperationMode.CREATE) -> CreateFrameResult:
        """
        Create frame graph and link to existing entity.
        
        EXTRACTED FROM: _create_or_update_frames() lines 937-1164
        
        Process:
        1. Validate entity exists (existing: lines 957-959)
        2. Categorize frame objects (existing: lines 980-993)
        3. Set dual grouping URIs (existing: lines 995-1011)
        4. Create Edge_hasEntityKGFrame objects (existing: lines 1019-1040)
        5. Handle UPDATE/UPSERT deletion (existing: lines 1061-1123)
        6. Execute atomic creation via backend (existing: lines 1125-1145)
        
        Returns:
            CreateFrameResult with created URIs and metadata
        """
        
    async def validate_entity_exists(self, backend_adapter, space_id: str, 
                                   graph_id: str, entity_uri: str) -> bool:
        """
        Validate target entity exists before frame creation.
        EXTRACTED FROM: lines 957-959 in _create_or_update_frames()
        """
        
    async def categorize_frame_objects(self, graph_objects: List[GraphObject]) -> FrameObjectCategories:
        """
        Categorize objects by type: frames, slots, edges.
        EXTRACTED FROM: lines 980-993 in _create_or_update_frames()
        
        Returns:
            - frame_objects: KGFrame instances
            - slot_objects: KGSlot subclass instances (KGTextSlot, etc.)
            - edge_objects: Edge_ instances (Edge_hasKGFrame, Edge_hasKGSlot)
        """
        
    async def assign_grouping_uris(self, frame_objects: List[GraphObject], 
                                 entity_uri: str) -> List[GraphObject]:
        """
        Assign dual grouping URIs to frame objects.
        EXTRACTED FROM: lines 995-1011 in _create_or_update_frames()
        
        Entity-level: hasKGGraphURI = entity_uri (for complete entity retrieval)
        Frame-level: frameGraphURI = frame_uri (for frame-specific retrieval)
        """
        
    async def create_entity_frame_edges(self, entity_uri: str, 
                                      frame_objects: List[GraphObject]) -> List[GraphObject]:
        """
        Create Edge_hasEntityKGFrame linking objects for entity-to-frame connections.
        EXTRACTED FROM: lines 1019-1040 in _create_or_update_frames()
        """
        
    async def handle_frame_update_deletion(self, backend_adapter, space_id: str, 
                                         graph_id: str, frame_objects: List[GraphObject]) -> bool:
        """
        Handle UPDATE/UPSERT operations by deleting existing frame members.
        EXTRACTED FROM: lines 1061-1123 in _create_or_update_frames()
        
        Process:
        1. Find subjects with frameGraphURI pointing to frames being updated
        2. Delete all triples for those subjects
        3. Prepare for new frame data insertion
        """
```

#### 7.2.4 Enhanced KGEntityCreateProcessor

**Enhanced Processor: `KGEntityCreateProcessor`**

```python
# Enhanced vitalgraph/kg_impl/kgentity_create_impl.py
class KGEntityCreateProcessor:
    """
    Enhanced processor for creating KGEntities with optional frame graphs.
    
    New capabilities:
    - Detect frame graphs in entity creation requests
    - Handle complete entity + frame graph creation atomically
    - Proper grouping URI assignment for complex object graphs
    """
    
    async def create_entity_with_frames(self, backend_adapter, space_id: str, graph_id: str,
                                      entity_objects: List[GraphObject]) -> CreateEntityResult:
        """
        Create entity with complete frame graphs.
        
        Process:
        1. Separate entity objects from frame objects
        2. Analyze frame graph structure
        3. Create entity first
        4. Create frame graphs with proper linking
        5. Assign dual grouping URIs to all objects
        6. Execute atomic creation via backend
        """
        
    async def detect_frame_objects(self, objects: List[GraphObject]) -> FrameDetectionResult:
        """
        Detect and categorize frame-related objects in creation request.
        
        Returns:
            - KGEntity objects
            - KGFrame objects  
            - KGSlot objects
            - Edge objects (hasEntityKGFrame, hasKGFrame, hasKGSlot)
        """
        
    async def validate_frame_graph_structure(self, frame_objects: List[GraphObject]) -> bool:
        """Validate frame graph structure and relationships."""
```

#### 7.2.5 Endpoint Integration

**KGEntitiesEndpoint Enhancements:**

```python
# vitalgraph/endpoint/kgentities_endpoint.py
class KGEntitiesEndpoint:
    """
    Enhanced KGEntities endpoint with frame creation capabilities.
    """
    
    async def _create_entity_frame(self, space_id: str, graph_id: str, entity_uri: str,
                                  request: JsonLdDocument, current_user: Dict) -> FrameCreateResponse:
        """
        Create frame and link to existing entity.
        
        New endpoint method for Use Case 1:
        POST /spaces/{space_id}/graphs/{graph_id}/entities/{entity_uri}/frames
        
        Process:
        1. Convert JsonLD to GraphObjects
        2. Use KGEntityFrameCreateProcessor
        3. Return FrameCreateResponse with created frame URIs
        """
        
    async def _create_entities(self, space_id: str, graph_id: Optional[str], 
                             request: JsonLdDocument, operation_mode: OperationMode,
                             current_user: Dict) -> EntitiesResponse:
        """
        Enhanced entity creation with frame graph detection.
        
        Enhanced for Use Case 2:
        - Detect if request contains frame objects
        - Use enhanced KGEntityCreateProcessor for frame graphs
        - Handle atomic entity + frame creation
        """
```

#### 7.2.6 Response Models

**New Response Models:**

```python
# vitalgraph/model/kgentities_model.py
class FrameCreateResponse(BaseModel):
    """Response for frame creation operations."""
    message: str
    entity_uri: str
    created_frame_uris: List[str]
    created_edge_uris: List[str]
    created_slot_uris: List[str] = []
    frame_count: int
    
class CreateFrameResult:
    """Internal result object for frame creation."""
    success: bool
    primary_frame_uri: str
    created_uris: List[str]
    frame_structure: FrameStructureAnalysis
    
class FrameStructureAnalysis:
    """Analysis of frame graph structure."""
    primary_frames: List[str]
    sub_frames: List[str] 
    slots: List[str]
    edges: List[str]
    frame_hierarchy: Dict[str, List[str]]
```

#### 7.2.7 Refactoring Implementation Phases

**Phase A: Extract KGEntityFrameCreateProcessor (1 day)**
- Create `vitalgraph/kg_impl/kgentity_frame_create_impl.py`
- Extract logic from `_create_or_update_frames()` lines 937-1164
- Implement methods: `create_entity_frame`, `categorize_frame_objects`, `assign_grouping_uris`
- Extract Edge_hasEntityKGFrame creation logic (lines 1019-1040)
- Extract UPDATE/UPSERT deletion logic (lines 1061-1123)

**Phase B: Refactor Endpoint to Use Processor (1 day)**
- Modify `_create_or_update_frames()` to delegate to `KGEntityFrameCreateProcessor`
- Replace direct backend calls with processor method calls
- Maintain existing API contract and response format
- Ensure backward compatibility with existing frame operations

**Phase C: Edge Relationship Clarification (0.5 days)**
- Validate Edge_hasEntityKGFrame (entity ‚Üí frame) connections
- Validate Edge_hasKGFrame (frame ‚Üí frame) connections  
- Validate Edge_hasKGSlot (frame ‚Üí slot) connections
- Ensure KGSlot abstract class handling (use concrete subclasses like KGTextSlot)

**Phase D: Testing and Validation Using Existing Test Infrastructure (1.5 days)**
- **Leverage existing test framework**: `/Users/hadfield/Local/vital-git/vital-graph/test_script_kg_impl/kgentities/`
- **Use existing test data**: `vitalgraph/utils/test_data.py` with comprehensive frame scenarios
  - 5 customer entities with multiple frame types per customer
  - FinancialTransactionFrame (4 per customer) with KGDoubleSlot, KGDateTimeSlot, KGTextSlot
  - AddressFrame (1 per customer) with postal code slots
  - EmploymentFrame (1 per customer)
  - Proper Edge_hasEntityKGFrame, Edge_hasKGFrame, Edge_hasKGSlot relationships
- **Create new test case**: `case_entity_frame_create.py` following existing patterns
- **Integration with existing orchestrator**: Add frame creation tests to existing test suite
- **Test scenarios**:
  - CREATE: New frame creation using `KGEntityFrameCreateProcessor`
  - UPDATE: Frame updates with existing frame deletion logic
  - UPSERT: Combined create/update operations
  - Dual grouping URI validation (hasKGGraphURI + frameGraphURI)
  - Edge relationship validation (entity‚Üíframe, frame‚Üíslot)
  - Concrete slot type handling (KGTextSlot, KGDoubleSlot, KGDateTimeSlot)

**TOTAL ESTIMATED TIME: 4 days**

#### 7.2.8 Test Integration Strategy

**Existing Test Infrastructure Analysis:**
- ‚úÖ **Test Framework**: Modular test classes (`KGEntityGetTester`, `KGEntityInsertTester`)
- ‚úÖ **Test Data**: Comprehensive frame scenarios in `test_data.py`
- ‚úÖ **Test Orchestrator**: Existing test suite structure in `/test_script_kg_impl/kgentities/`
- ‚úÖ **Frame Test Data**: 6 frames per entity (4 transaction + 1 address + 1 employment)
- ‚úÖ **Edge Relationships**: Proper Edge_hasEntityKGFrame, Edge_hasKGSlot connections
- ‚úÖ **Concrete Slot Types**: KGTextSlot, KGDoubleSlot, KGDateTimeSlot usage

**New Test Case Implementation:**
```python
# /Users/hadfield/Local/vital-git/vital-graph/test_script_kg_impl/kgentities/case_entity_frame_create.py
class KGEntityFrameCreateTester:
    """
    Test frame creation functionality using KGEntityFrameCreateProcessor.
    
    Leverages existing test data from vitalgraph.utils.test_data.py
    """
    
    async def test_frame_creation_with_processor(self, space_id: str, graph_id: str, entity_uri: str) -> bool:
        """Test frame creation using refactored processor."""
        
    async def test_frame_update_with_processor(self, space_id: str, graph_id: str, entity_uri: str) -> bool:
        """Test frame updates using refactored processor."""
        
    async def test_dual_grouping_uri_assignment(self, space_id: str, graph_id: str, entity_uri: str) -> bool:
        """Test proper dual grouping URI assignment (hasKGGraphURI + frameGraphURI)."""
        
    async def test_edge_relationship_creation(self, space_id: str, graph_id: str, entity_uri: str) -> bool:
        """Test Edge_hasEntityKGFrame, Edge_hasKGFrame, Edge_hasKGSlot creation."""
```

### 7.2 KGEntities Endpoint Integration
**Priority: High**
**Estimated Time: 1-2 days**

Update KGEntities endpoint to use Fuseki+PostgreSQL backend with SPARQL UPDATE operations:

```python
# vitalgraph/endpoint/impl/kgentity_impl.py
class KGEntityImpl:
    """
    KGEntity implementation using Fuseki+PostgreSQL hybrid backend.
    
    Strategy:
    - Convert entity operations to SPARQL UPDATE statements
    - Use backend's execute_sparql_update() for all modifications
    - Leverage existing SPARQL UPDATE parsing and dual-write coordination
    """
    
    def __init__(self, space_backend: FusekiPostgreSQLSpaceImpl):
        self.backend = space_backend
        self.logger = logging.getLogger(__name__)
    
    async def create_kgentity(self, space_id: str, jsonld_document: Dict) -> str:
        """
        Create KGEntity using SPARQL INSERT operation.
        
        Process:
        1. Convert JSON-LD to VitalSigns KGEntity objects
        2. Generate SPARQL INSERT DATA statement
        3. Execute via backend.execute_sparql_update()
        4. Dual-write coordination handles Fuseki + PostgreSQL sync
        """
        
        try:
            # Convert JSON-LD to VitalSigns objects
            entities = jsonld_to_graphobjects(jsonld_document, validator=self._validate_kgentity)
            
            if not entities:
                raise ValueError("No valid KGEntity objects found in JSON-LD")
            
            # Generate SPARQL INSERT for all entities and related objects
            sparql_insert = self._generate_entity_insert_sparql(entities, space_id)
            
            # Execute via hybrid backend (handles dual-write automatically)
            success = await self.backend.execute_sparql_update(space_id, sparql_insert)
            
            if success:
                return entities[0].URI  # Return primary entity URI
            else:
                raise RuntimeError("Failed to create KGEntity via SPARQL UPDATE")
                
        except Exception as e:
            self.logger.error(f"Error creating KGEntity: {e}")
            raise
    
    async def update_kgentity(self, space_id: str, entity_uri: str, jsonld_document: Dict) -> bool:
        """
        Update KGEntity using SPARQL DELETE/INSERT operation.
        
        Process:
        1. Generate DELETE WHERE to remove existing entity data
        2. Generate INSERT DATA for new entity data
        3. Combine into DELETE/INSERT SPARQL UPDATE
        4. Execute via backend with dual-write coordination
        """
        
        try:
            # Convert JSON-LD to VitalSigns objects
            updated_entities = jsonld_to_graphobjects(jsonld_document, validator=self._validate_kgentity)
            
            # Generate DELETE/INSERT SPARQL UPDATE
            sparql_update = self._generate_entity_update_sparql(entity_uri, updated_entities, space_id)
            
            # Execute via hybrid backend
            return await self.backend.execute_sparql_update(space_id, sparql_update)
            
        except Exception as e:
            self.logger.error(f"Error updating KGEntity {entity_uri}: {e}")
            return False
    
    async def delete_kgentity(self, space_id: str, entity_uri: str, delete_entity_graph: bool = False) -> bool:
        """
        Delete KGEntity using SPARQL DELETE operation.
        
        Process:
        1. If delete_entity_graph=True, use grouping URI to delete entire entity graph
        2. Otherwise, delete only the specific entity
        3. Generate appropriate DELETE WHERE statement
        4. Execute via backend with dual-write coordination
        """
        
        try:
            if delete_entity_graph:
                # Delete entire entity graph using grouping URI
                sparql_delete = self._generate_entity_graph_delete_sparql(entity_uri, space_id)
            else:
                # Delete only the specific entity
                sparql_delete = self._generate_entity_delete_sparql(entity_uri, space_id)
            
            # Execute via hybrid backend
            return await self.backend.execute_sparql_update(space_id, sparql_delete)
            
        except Exception as e:
            self.logger.error(f"Error deleting KGEntity {entity_uri}: {e}")
            return False
    
    def _generate_entity_insert_sparql(self, entities: List[GraphObject], space_id: str) -> str:
        """
        Generate SPARQL INSERT DATA statement for KGEntity objects.
        
        Includes:
        - Entity triples (type, properties, grouping URIs)
        - Frame relationships (Edge_hasKGFrame)
        - Slot relationships (Edge_hasKGSlot)
        - All nested objects (frames, slots, edges)
        """
        
        # Convert VitalSigns objects to triples
        all_triples = []
        for entity in entities:
            entity_triples = entity.to_triples()
            all_triples.extend(entity_triples)
        
        # Build INSERT DATA statement
        graph_uri = f"http://vital.ai/graph/{space_id}/entities"
        
        sparql_insert = f"""
        PREFIX vital: <http://vital.ai/ontology/vital-core#>
        PREFIX haley: <http://vital.ai/ontology/haley-ai-kg#>
        
        INSERT DATA {{
            GRAPH <{graph_uri}> {{
        """
        
        for triple in all_triples:
            subject, predicate, obj = triple
            sparql_insert += f"        <{subject}> <{predicate}> {self._format_sparql_object(obj)} .\n"
        
        sparql_insert += """
            }
        }
        """
        
        return sparql_insert
    
    def _generate_entity_update_sparql(self, entity_uri: str, updated_entities: List[GraphObject], space_id: str) -> str:
        """
        Generate SPARQL DELETE/INSERT statement for entity updates.
        
        Strategy:
        - DELETE WHERE: Remove all triples for the entity and its grouping URI
        - INSERT DATA: Add all new triples for the updated entity
        """
        
        graph_uri = f"http://vital.ai/graph/{space_id}/entities"
        entity_graph_uri = f"{entity_uri}"
        
        # DELETE existing entity data
        sparql_update = f"""
        PREFIX vital: <http://vital.ai/ontology/vital-core#>
        PREFIX haley: <http://vital.ai/ontology/haley-ai-kg#>
        
        DELETE {{
            GRAPH <{graph_uri}> {{
                ?s ?p ?o
            }}
        }}
        WHERE {{
            GRAPH <{graph_uri}> {{
                ?s vital:kGGraphURI <{entity_graph_uri}> .
                ?s ?p ?o
            }}
        }} ;
        
        INSERT DATA {{
            GRAPH <{graph_uri}> {{
        """
        
        # INSERT new entity data
        all_triples = []
        for entity in updated_entities:
            entity_triples = entity.to_triples()
            all_triples.extend(entity_triples)
        
        for triple in all_triples:
            subject, predicate, obj = triple
            sparql_update += f"        <{subject}> <{predicate}> {self._format_sparql_object(obj)} .\n"
        
        sparql_update += """
            }
        }
        """
        
        return sparql_update
    
    def _generate_entity_delete_sparql(self, entity_uri: str, space_id: str) -> str:
        """Generate SPARQL DELETE for specific entity only."""
        
        graph_uri = f"http://vital.ai/graph/{space_id}/entities"
        
        return f"""
        PREFIX vital: <http://vital.ai/ontology/vital-core#>
        
        DELETE {{
            GRAPH <{graph_uri}> {{
                <{entity_uri}> ?p ?o
            }}
        }}
        WHERE {{
            GRAPH <{graph_uri}> {{
                <{entity_uri}> ?p ?o
            }}
        }}
        """
    
    def _generate_entity_graph_delete_sparql(self, entity_uri: str, space_id: str) -> str:
        """Generate SPARQL DELETE for entire entity graph using grouping URI."""
        
        graph_uri = f"http://vital.ai/graph/{space_id}/entities"
        entity_graph_uri = f"{entity_uri}"
        
        return f"""
        PREFIX vital: <http://vital.ai/ontology/vital-core#>
        
        DELETE {{
            GRAPH <{graph_uri}> {{
                ?s ?p ?o
            }}
        }}
        WHERE {{
            GRAPH <{graph_uri}> {{
                ?s vital:kGGraphURI <{entity_graph_uri}> .
                ?s ?p ?o
            }}
        }}
        """
```

### 7.3 Endpoint Integration
**Priority: High**
**Estimated Time: 1 day each**

Apply consistent SPARQL UPDATE patterns across all endpoints. See dedicated endpoint planning files for specific implementation details.

```python
# Similar implementation pattern for:
# - vitalgraph/endpoint/impl/kgframe_impl.py
# - vitalgraph/endpoint/impl/kgtype_impl.py

class KGFrameImpl:
    """KGFrame operations via SPARQL UPDATE with Fuseki+PostgreSQL backend."""
    
    async def create_kgframe(self, space_id: str, jsonld_document: Dict) -> str:
        """Create frame using SPARQL INSERT with frame-specific graph URI."""
        
    async def update_kgframe(self, space_id: str, frame_uri: str, jsonld_document: Dict) -> bool:
        """Update frame using SPARQL DELETE/INSERT with frameGraphURI grouping."""
        
    async def delete_kgframe(self, space_id: str, frame_uri: str) -> bool:
        """Delete frame using SPARQL DELETE with frame grouping URI."""

class KGTypeImpl:
    """KGType operations via SPARQL UPDATE with Fuseki+PostgreSQL backend."""
    
    async def create_kgtype(self, space_id: str, jsonld_document: Dict) -> str:
        """Create type definition using SPARQL INSERT."""
        
    async def update_kgtype(self, space_id: str, type_uri: str, jsonld_document: Dict) -> bool:
        """Update type definition using SPARQL DELETE/INSERT."""
```

### 7.4 Backend Factory Integration
**Priority: Medium**
**Estimated Time: 0.5 days**

Update the backend factory to properly instantiate endpoint implementations with Fuseki+PostgreSQL backend:

```python
# vitalgraph/db/backend_config.py
class BackendFactory:
    """Factory for creating backend implementations and endpoint integrations."""
    
    @staticmethod
    def create_endpoint_implementations(backend_type: BackendType, space_backend) -> Dict[str, Any]:
        """
        Create endpoint implementations configured for specific backend type.
        
        For FUSEKI_POSTGRESQL backend:
        - All endpoints use SPARQL UPDATE operations
        - Dual-write coordination handled automatically by backend
        - PostgreSQL used for metadata and backup
        """
        
        if backend_type == BackendType.FUSEKI_POSTGRESQL:
            return {
                'kgentity_impl': KGEntityImpl(space_backend),
                'kgframe_impl': KGFrameImpl(space_backend),
                'kgtype_impl': KGTypeImpl(space_backend),
                'objects_impl': ObjectsImpl(space_backend),
                'file_impl': FileImpl(space_backend),  # Uses PostgreSQL for metadata
                'graph_impl': GraphImpl(space_backend)  # Uses Fuseki dataset management
            }
        elif backend_type == BackendType.POSTGRESQL:
            # Existing PostgreSQL implementations
            return create_postgresql_implementations(space_backend)
        elif backend_type == BackendType.FUSEKI:
            # Existing Fuseki implementations
            return create_fuseki_implementations(space_backend)
```

### 7.5 Configuration and Initialization
**Priority: Medium**
**Estimated Time: 0.5 days**

Update VitalGraph service initialization to support Fuseki+PostgreSQL endpoint integration:

```python
# vitalgraph/main/main.py or vitalgraph/impl/vitalgraphapp_impl.py
class VitalGraphAppImpl:
    """VitalGraph application with Fuseki+PostgreSQL backend support."""
    
    async def initialize_backend(self):
        """Initialize backend and endpoint implementations."""
        
        if self.config.backend.type == BackendType.FUSEKI_POSTGRESQL:
            # Initialize hybrid backend
            self.space_backend = FusekiPostgreSQLSpaceImpl(
                fuseki_config=self.config.fuseki,
                postgresql_config=self.config.postgresql
            )
            
            # Initialize endpoint implementations with SPARQL UPDATE support
            self.endpoint_implementations = BackendFactory.create_endpoint_implementations(
                BackendType.FUSEKI_POSTGRESQL, 
                self.space_backend
            )
            
            # Validate backend connectivity
            await self._validate_hybrid_backend_connectivity()
    
    async def _validate_hybrid_backend_connectivity(self):
        """Validate both Fuseki and PostgreSQL connectivity for hybrid backend."""
        
        # Test Fuseki connectivity
        fuseki_healthy = await self.space_backend.fuseki_manager.health_check()
        
        # Test PostgreSQL connectivity  
        pg_healthy = await self.space_backend.postgresql_impl.is_connected()
        
        if not (fuseki_healthy and pg_healthy):
            raise RuntimeError("Hybrid backend connectivity validation failed")
```

### 7.6 Testing Strategy for Endpoint Integration
**Priority: High**
**Estimated Time: 1-2 days**

Create comprehensive tests for endpoint integration with Fuseki+PostgreSQL backend:

```python
# test_scripts/endpoint_integration/test_fuseki_postgresql_endpoints.py
class TestFusekiPostgreSQLEndpointIntegration:
    """Test VitalGraph endpoints with Fuseki+PostgreSQL backend."""
    
    async def test_kgentity_crud_operations(self):
        """
        Test complete KGEntity CRUD via REST API with Fuseki+PostgreSQL backend.
        
        Validates:
        - Entity creation via SPARQL INSERT
        - Entity retrieval from Fuseki
        - Entity updates via SPARQL DELETE/INSERT
        - Entity deletion via SPARQL DELETE
        - Dual-write consistency (Fuseki + PostgreSQL)
        """
        
    async def test_entity_graph_operations(self):
        """
        Test entity graph operations (grouping URI functionality).
        
        Validates:
        - Entity graph creation with proper grouping URIs
        - Frame relationships via Edge_hasKGFrame
        - Entity graph deletion (delete_entity_graph=True)
        - Orphaned object cleanup
        """
        
    async def test_dual_write_consistency(self):
        """
        Test dual-write consistency between Fuseki and PostgreSQL.
        
        Validates:
        - SPARQL UPDATE operations update both systems
        - PostgreSQL backup contains all entity data
        - Disaster recovery from PostgreSQL to Fuseki
        - Transaction rollback on failures
        """
        
    async def test_performance_comparison(self):
        """
        Compare endpoint performance: Fuseki+PostgreSQL vs pure backends.
        
        Metrics:
        - Entity creation time
        - Complex query performance
        - Bulk operation throughput
        - Memory usage patterns
        """
```

### 7.7 Documentation and Migration Guide
**Priority: Medium**
**Estimated Time: 1 day**

Create documentation for endpoint integration with Fuseki+PostgreSQL backend:

```markdown
# VitalGraph Endpoint Integration with Fuseki+PostgreSQL Backend

## Overview
This guide covers integrating VitalGraph REST API endpoints with the Fuseki+PostgreSQL hybrid backend, leveraging SPARQL UPDATE operations for all data modifications.

## Key Benefits
- **Unified Operations**: All endpoint operations use SPARQL UPDATE
- **Dual-Write Consistency**: Automatic synchronization between Fuseki and PostgreSQL
- **Performance**: Fast graph queries via Fuseki, reliable backup via PostgreSQL
- **Scalability**: Dataset per space with PostgreSQL metadata management

## Endpoint Integration Patterns

### KGEntities Endpoint
- **Create**: JSON-LD ‚Üí VitalSigns ‚Üí SPARQL INSERT DATA
- **Update**: SPARQL DELETE/INSERT with grouping URI filtering
- **Delete**: SPARQL DELETE with optional entity graph deletion
- **Read**: Direct Fuseki SPARQL SELECT queries

### Configuration
```yaml
# vitalgraphdb-config.yaml
backend:
  type: fuseki_postgresql
  
fuseki:
  server_url: http://localhost:3030
  admin_user: admin
  admin_password: admin
  
postgresql:
  host: localhost
  port: 5432
  database: vitalgraph
  username: vitalgraph
  password: vitalgraph
```

## Migration from Other Backends
- **From PostgreSQL**: Existing data can be exported and imported via SPARQL
- **From Fuseki**: PostgreSQL primary data tables created automatically
- **Gradual Migration**: Endpoints can be migrated individually
```

## Timeline for Endpoint Integration

**Week 1: Core Endpoint Integration**
- Day 1-2: KGEntities endpoint SPARQL UPDATE integration
- Day 3: KGFrames endpoint integration  
- Day 4: Additional endpoint integration
- Day 5: Backend factory and configuration updates

**Week 2: Testing and Validation**
- Day 1-2: Comprehensive endpoint integration testing
- Day 3: Performance testing and optimization
- Day 4: Dual-write consistency validation
- Day 5: Documentation and migration guides

**Total Estimated Time: 2 weeks**

## Key Success Metrics

**Technical Metrics:**
- All SpaceBackendInterface methods implemented and tested
- Integration tests passing at 95%+ success rate
- Query performance within acceptable bounds for both backends
- Zero critical bugs in production deployment
- Successful disaster recovery from PostgreSQL backup to Fuseki
- **All VitalGraph endpoints successfully integrated with Fuseki+PostgreSQL backend**
- **SPARQL UPDATE operations working correctly for all CRUD operations**
- **Dual-write consistency maintained across all endpoint operations**

**Operational Metrics:**
- Successful deployment with health checks
- Monitoring and alerting fully operational
- Backup and recovery procedures tested
- Team training and documentation complete
- Independent backend implementations allowing A/B testing
- **Endpoint migration completed with zero downtime**
- **Performance metrics meet or exceed existing backend performance**

## Phase 8: Comprehensive Endpoint Testing for Fuseki+PostgreSQL Backend

### 8.1 KGFrames Endpoint Implementation Completion
**Priority: HIGH (Reduced from CRITICAL)**
**Estimated Time: 1-2 days (Reduced from 2-3 days)**

After searching the codebase, many methods actually exist. **Action Required**: Implement the remaining ~16 missing methods or locate them in existing codebase.

**Implementation Tasks:**
1. **Locate or Implement SPARQL Query Building Methods** (5 methods)
2. **Locate or Implement Data Conversion Methods** (5 methods)  
3. **Locate or Implement Frame Processing Methods** (4 methods)
4. **Locate or Implement Operation Mode Handlers** (3 methods)
5. **Locate or Implement Frame Existence and Deletion** (2 methods)
6. **Locate or Implement Query Processing Methods** (2 methods)
7. **Complete Frame-Slot Relationship Management Methods** (6+ methods)

**UPDATED IMPLEMENTATION STATUS**: After searching the codebase, many methods actually exist. Here's the corrected status:

**‚úÖ IMPLEMENTED METHODS** (Found in codebase):
- `_sparql_results_to_frames()` - ‚úÖ Implemented in kgframes_endpoint.py
- `_frames_to_jsonld_document()` - ‚úÖ Implemented in kgframes_endpoint.py  
- `_set_frame_grouping_uris()` - ‚úÖ Implemented in kgframes_endpoint.py
- `_validate_frame_structure()` - ‚úÖ Implemented in kgframes_endpoint.py
- `_handle_parent_relationships()` - ‚úÖ Implemented in kgframes_endpoint.py
- `_handle_create_mode()` - ‚úÖ Implemented in kgframes_endpoint.py
- `_handle_update_mode()` - ‚úÖ Implemented in kgframes_endpoint.py
- `_handle_upsert_mode()` - ‚úÖ Implemented in kgframes_endpoint.py

**‚ùå STILL MISSING METHODS** (Need implementation or location):
- `_get_frames_by_uris()` - Returns 501 Not Implemented
- `_build_list_frames_query()` - Referenced but not implemented
- `_build_get_frame_query()` - Referenced but not implemented
- `_build_count_frames_query()` - Referenced but not implemented
- `_build_frame_query_sparql()` - Referenced but not implemented
- `_build_get_frame_slots_query()` - Referenced but not implemented
- `_jsonld_document_to_vitalsigns_objects()` - Referenced but not implemented
- `_sparql_results_to_slots()` - Referenced but not implemented
- `_slots_to_jsonld_document()` - Referenced but not implemented
- `_extract_count_from_results()` - Referenced but not implemented
- `_frame_exists_in_backend()` - Referenced but not implemented
- `_delete_frame_from_backend()` - Referenced but not implemented
- `_apply_frame_sorting()` - Referenced but not implemented
- `_apply_frame_pagination()` - Referenced but not implemented
- `_update_frame_slots()` - Referenced but not implemented
- `_delete_frame_slots()` - Referenced but not implemented

**Revised Total Missing Methods**: ~16 methods (significantly fewer than initially estimated)

### 8.1.1 Connection Management Issues Discovered
**Priority: HIGH**
**Estimated Time: 0.5-1 day**

During spaces endpoint testing with real SpaceManager integration, a critical connection management issue was discovered:

**Problem**: Backend connections are being closed after individual space operations, causing subsequent operations to fail with "Not connected to Fuseki server" errors.

**Root Cause**: Each space operation creates and destroys its own backend connection instead of reusing the shared connection from the SpaceManager.

**Impact**: 
- **ALL TESTS ARE FAILING** ‚ùå
- Space creation fails with dual-write validation errors
- Space listing, retrieval, updates all fail
- Test cleanup fails for multiple spaces
- Fundamental integration issues between components

**Evidence**:
```
Space Creation with Dual-Write: ‚ùå FAILED
Space Listing Operations: ‚ùå FAILED  
Space Retrieval by ID: ‚ùå FAILED
Space Update Operations: ‚ùå FAILED
Space Filtering by Name: ‚ùå FAILED
Dual-Write Consistency Validation: ‚ùå FAILED
Space Deletion with Dual-Storage Cleanup: ‚ùå FAILED
```

**Status**: **CRITICAL - ALL TESTS FAILING**

The spaces endpoint test implementation is fundamentally broken. While the test framework loads and connects to both Fuseki and PostgreSQL, the actual endpoint operations are failing across the board.

**Required Fixes**:
1. **Investigate dual-write validation failures** - Space creation appears to work but validation fails
2. **Debug SpaceManager integration issues** - Real SpaceManager may have incompatibilities with hybrid backend
3. **Fix connection lifecycle management** - Backend connections being closed prematurely
4. **Validate PostgreSQL metadata operations** - Space table operations may be failing silently
5. **Review VitalGraphAPI parameter passing** - Metadata may not be reaching the backend correctly

**Recommendation**: **PAUSE** comprehensive endpoint testing until fundamental SpaceManager + Fuseki+PostgreSQL integration issues are resolved.

**Files to Investigate**:
- `vitalgraph/space/space_manager.py` - Connection lifecycle management
- `vitalgraph/space/space_impl.py` - Backend connection handling
- `vitalgraph/db/fuseki_postgresql/fuseki_postgresql_space_impl.py` - Connection state management

### 8.2 Real VitalGraph API Endpoint Testing Strategy
**Priority: Critical**
**Estimated Time: 3-4 days**

Based on the actual VitalGraph API structure (from `/openapi.json`), comprehensive test scripts must be created for all real endpoints to validate Fuseki+PostgreSQL backend integration. The existing test scripts were created based on assumptions rather than actual endpoint implementations.

**Real VitalGraph API Endpoints Requiring Test Scripts:**

### 8.2 SPARQL Endpoints Testing
**Priority: High**
**Estimated Time: 1 day**

**SPARQL Endpoint Implementation**: See `endpoints/fuseki_psql_sparql_endpoint_plan.md` for comprehensive SPARQL endpoint implementation details including:
- Query operations (GET/POST)
- Update operations (INSERT, DELETE, UPDATE)
- Form-based operations
- Direct RDF data insertion
- Error handling and security
- Performance optimization
- Test coverage requirements

### 8.3 KG Endpoints Testing
**Priority: High** 
**Estimated Time: 1.5 days**

Create test scripts for actual KG endpoints:
- `/api/graphs/kgentities` - Entity CRUD operations
- `/api/graphs/kgentities/kgframes` - Entity-frame relationship management
- `/api/graphs/kgentities/query` - Entity connection queries
- `/api/graphs/kgframes` - Frame CRUD operations
- `/api/graphs/kgframes/kgslots` - Frame-slot relationship management
- `/api/graphs/kgframes/query` - Frame connection queries
- `/api/graphs/kgtypes` - Type definition management with hierarchy
- `/api/graphs/kgrelations` - Direct entity-to-entity relationships
- `/api/graphs/kgrelations/query` - Relation traversal queries
- `/api/graphs/kgqueries` - Complex KG connection queries

**Specific Test Cases Required:**
- **KGEntities Endpoint**: Entity CRUD operations, dual-write consistency, disaster recovery, performance comparison
- **KGFrames Endpoint**: Frame CRUD operations, frame grouping URI functionality, frame-entity relationships via Edge objects
  - **KGFrames Main Operations**: 
    - `GET /api/graphs/kgframes` - List Or Get Frames
    - `POST /api/graphs/kgframes` - Create Or Update Frames  
    - `DELETE /api/graphs/kgframes` - Delete Frames
    - `POST /api/graphs/kgframes/query` - Query Frames
  - **KGFrame Slots Operations**:
    - `GET /api/graphs/kgframes/kgslots` - Get Frame Slots
    - `POST /api/graphs/kgframes/kgslots` - Create Frame Slots
    - `PUT /api/graphs/kgframes/kgslots` - Update Frame Slots
    - `DELETE /api/graphs/kgframes/kgslots` - Delete Frame Slots
  - **CRITICAL Implementation Gap**: The KGFrames endpoint has placeholder implementations that must be completed:
    - `_get_frames_by_uris()` - Currently returns 501 Not Implemented
    - **SPARQL Query Building Methods**:
      - `_build_list_frames_query()` - Referenced in `_list_frames()` but not implemented
      - `_build_get_frame_query()` - Referenced in `_get_frame_by_uri()` but not implemented
      - `_build_count_frames_query()` - Referenced in `_list_frames()` but not implemented
      - `_build_frame_query_sparql()` - Referenced in `_query_frames()` but not implemented
      - `_build_get_frame_slots_query()` - Referenced in `_get_frame_slots()` but not implemented
    - **Data Conversion Methods**:
      - `_sparql_results_to_frames()` - Referenced multiple times but not implemented
      - `_frames_to_jsonld_document()` - Referenced multiple times but not implemented
      - `_jsonld_document_to_vitalsigns_objects()` - Referenced in `_create_or_update_frames()` but not implemented
      - `_sparql_results_to_slots()` - Referenced in `_get_frame_slots()` but not implemented
      - `_slots_to_jsonld_document()` - Referenced in `_get_frame_slots()` but not implemented
    - **Frame Processing Methods**:
      - `_set_frame_grouping_uris()` - Referenced in `_create_or_update_frames()` but not implemented
      - `_validate_frame_structure()` - Referenced in `_create_or_update_frames()` but not implemented
      - `_handle_parent_relationships()` - Referenced in `_create_or_update_frames()` but not implemented
      - `_extract_count_from_results()` - Referenced in `_list_frames()` but not implemented
    - **Operation Mode Handlers**:
      - `_handle_create_mode()` - Referenced in `_create_or_update_frames()` but not implemented
      - `_handle_update_mode()` - Referenced in `_create_or_update_frames()` but not implemented
      - `_handle_upsert_mode()` - Referenced in `_create_or_update_frames()` but not implemented
    - **Frame Existence and Deletion**:
      - `_frame_exists_in_backend()` - Referenced in delete methods but not implemented
      - `_delete_frame_from_backend()` - Referenced in delete methods but not implemented
    - **Query Processing Methods**:
      - `_apply_frame_sorting()` - Referenced in `_query_frames()` but not implemented
      - `_apply_frame_pagination()` - Referenced in `_query_frames()` but not implemented
    - **Frame-Slot Relationship Management Methods**:
      - `_create_frame_slots()` - Has skeleton but incomplete implementation
      - `_update_frame_slots()` - Referenced in routes but not implemented
      - `_delete_frame_slots()` - Referenced in routes but not implemented
      - Methods to create `Edge_hasKGSlot` relationships between frames and slots
      - Methods to query and traverse frame-slot relationships
      - Methods to update and delete slot relationships while preserving frame integrity

For detailed endpoint-specific information, see the dedicated planning files in `endpoints/` directory.
- **KGTypes Endpoint**: Type definition CRUD operations, type hierarchy and inheritance relationships, type validation and schema enforcement, type versioning and evolution
- **KGRelations Endpoint**: Entity-to-entity relationship creation, relationship queries and traversal, relationship updates and modifications, relationship deletion while preserving connected objects
- **KGQueries Endpoint**: Complex entity connection queries, multi-hop relationship traversal, query optimization and performance

**Test Coverage Requirements:**
- VitalSigns integration with JSON-LD processing
- Grouping URI functionality for entity graphs
- Type hierarchy and inheritance relationships
- Edge object management (hasKGFrame, hasKGSlot, etc.)
- Relationship queries and traversal

### 8.4 Graph Data Endpoints Testing
**Priority: Medium**
**Estimated Time: 1 day**

Create test scripts for graph data endpoints:
- `/api/graphs/triples` - Triple-level operations (add, delete, list)
- `/api/graphs/objects` - Generic graph object management
- `/api/graphs/files` - File upload/download with metadata

**Specific Test Cases Required:**
- **Triples Endpoint**: See `endpoints/fuseki_psql_triples_endpoint_plan.md` for complete test coverage
- **Objects Endpoint**: Generic graph object CRUD operations, object type filtering, batch object operations, object metadata management
- **Files Endpoint**: File upload with metadata, file download operations, file binary storage validation, file association with graph objects

**Test Coverage Requirements:**
- Triple pattern matching and filtering
- JSON-LD to triples conversion
- File binary storage with graph metadata
- Batch operations for performance

### 8.1 Spaces and Graphs Endpoint Testing ‚úÖ COMPLETED - See `endpoints/fuseki_psql_graphs_endpoint_plan.md`
**Priority: High** 
**Status: COMPLETED - 100% SUCCESS**
**Completion Date: January 3, 2026**

**‚úÖ SPACES ENDPOINT TEST - 7/7 TESTS PASSING (100%)**
- **Space Creation with Dual-Write** ‚úÖ PASSED - Validates PostgreSQL space table + Fuseki dataset creation
- **Space Listing Operations** ‚úÖ PASSED - Tests space enumeration and filtering
- **Space Retrieval by ID** ‚úÖ PASSED - Individual space metadata retrieval
- **Space Update Operations** ‚úÖ PASSED - Space metadata modification with consistency
- **Space Filtering by Name** ‚úÖ PASSED - Name-based space search functionality
- **Dual-Write Consistency Validation** ‚úÖ PASSED - Comprehensive Fuseki+PostgreSQL sync verification
- **Space Deletion with Dual-Storage Cleanup** ‚úÖ PASSED - Complete space removal with cleanup

**‚úÖ GRAPHS ENDPOINT TEST - 6/6 TESTS PASSING (100%)** - See `endpoints/fuseki_psql_graphs_endpoint_plan.md` for detailed test results

**üîß KEY TECHNICAL ACHIEVEMENTS:**

1. **Proper Endpoint Architecture Implementation:**
   - **Spaces Endpoint**: Added endpoint methods (`add_space`, `list_spaces`, `get_space`, `update_space`, `delete_space`, `filter_spaces`) to `SpacesEndpoint` class
   - **Graphs Endpoint**: Utilized existing internal methods (`_execute_graph_operation`, `_list_graphs`, `_get_graph_info`) in `SPARQLGraphEndpoint`
   - **Test Architecture**: Both tests call endpoint methods directly instead of bypassing to underlying implementations

2. **Hybrid Backend Graph Management Implementation:**
   - **Created `FusekiPostgreSQLSpaceGraphs` class** providing full PostgreSQL graph table operations
   - **Added `graphs` attribute** to `FusekiPostgreSQLSpaceImpl` for endpoint compatibility
   - **Database Operations**: CREATE/DROP use PostgreSQL table + SPARQL coordination, CLEAR uses SPARQL only
   - **Schema Compatibility**: Fixed all operations to match actual `graph` table schema (space_id, graph_uri, graph_name, created_time)

3. **Dual-Write Consistency Architecture:**
   - **Spaces**: Validates space metadata in PostgreSQL space table + Fuseki dataset existence
   - **Graphs**: Validates graph records in PostgreSQL graph table + SPARQL graph operations
   - **Transaction Safety**: Proper rollback handling for failed operations
   - **Resource Cleanup**: Complete cleanup of both Fuseki and PostgreSQL artifacts

4. **Test Framework Integration:**
   - **Endpoint Instantiation**: Uses `FusekiPostgreSQLEndpointTester` utils for proper hybrid backend setup
   - **Method Calls**: Direct endpoint method invocation following **test ‚Üí endpoint ‚Üí backend ‚Üí database** flow
   - **Validation**: Comprehensive dual-write consistency checks after each operation
   - **Error Handling**: Proper exception handling and resource cleanup

**üìä IMPLEMENTATION DETAILS:**
- **File Created**: `/vitalgraph/db/fuseki_postgresql/fuseki_postgresql_space_graphs.py` (265 lines)
- **File Modified**: `/vitalgraph/db/fuseki_postgresql/fuseki_postgresql_space_impl.py` (added graphs attribute)
- **File Modified**: `/vitalgraph/endpoint/spaces_endpoint.py` (added direct callable methods)
- **Test Scripts**: Both endpoint tests now achieve 100% success rate with proper architecture

**‚úÖ SUCCESS CRITERIA MET:**
- ‚úÖ Dual-write consistency validation (Fuseki + PostgreSQL)
- ‚úÖ Space isolation verification through proper space management
- ‚úÖ Graph metadata management via PostgreSQL global graph table
- ‚úÖ Error handling and rollback scenarios tested
- ‚úÖ Resource cleanup validation for both storage systems
- ‚úÖ Direct endpoint method calls (no implementation bypassing)
- ‚úÖ Proper hybrid backend integration and compatibility, export execution and download, export format validation and performance

### 8.6 Test Script Architecture Requirements

**Base Test Framework:**
```python
# /test_scripts/fuseki_postgresql/test_real_endpoints_utils.py
class RealEndpointTester(FusekiPostgreSQLEndpointTester):
    """Base class for testing actual VitalGraph API endpoints"""
    
    def __init__(self, base_url: str = "http://localhost:8001"):
        self.base_url = base_url
        # Direct backend setup (no REST service required)
        super().__init__()
    
    async def test_endpoint_method(self, endpoint_path: str, method: str, **kwargs):
        """Test actual endpoint method calls with dual-write validation"""
        # Call endpoint implementation directly
        # Validate dual-write consistency
        # Return comprehensive test results
```

**Individual Endpoint Test Scripts:**
- `test_sparql_endpoints_real.py` - All SPARQL endpoint testing
- `test_kgentities_endpoint_real.py` - KGEntities endpoint testing  
- `test_kgframes_endpoint_real.py` - KGFrames endpoint testing
- `test_kgtypes_endpoint_real.py` - KGTypes endpoint testing
- `test_kgrelations_endpoint_real.py` - KGRelations endpoint testing
- `test_kgqueries_endpoint_real.py` - KGQueries endpoint testing
- `test_triples_endpoint_real.py` - See `endpoints/fuseki_psql_triples_endpoint_plan.md`
- `test_objects_endpoint_real.py` - Objects endpoint testing
- `test_files_endpoint_real.py` - Files endpoint testing

**Key Testing Principles:**
1. **Direct Method Calls**: Test endpoint implementations directly, not via HTTP
2. **Dual-Write Validation**: Every operation must validate Fuseki+PostgreSQL consistency
3. **Real Data Patterns**: Use actual VitalSigns objects and JSON-LD documents
4. **Comprehensive Coverage**: Test all CRUD operations, queries, and edge cases
5. **Performance Validation**: Ensure hybrid backend meets performance requirements

### 8.7 Success Criteria

**Technical Validation:**
- All real endpoint test scripts created and passing
- 100% dual-write consistency across all operations
- SPARQL UPDATE operations working for all endpoint types
- VitalSigns integration validated for all KG endpoints
- Performance benchmarks met or exceeded

**Operational Validation:**
- Test scripts can run independently without REST service
- Comprehensive error handling and logging
- Resource cleanup after all test operations
- Integration with existing test infrastructure

## 9. Data Format Standardization Requirements

### 9.1 Quad Format Standardization - CRITICAL

**Requirement**: All RDF quad handling throughout the VitalGraph system MUST use tuple format consistently.

**Format Standard:**
```python
# CORRECT: Tuple format (subject, predicate, object, graph)
quad = (subject_uri, predicate_uri, object_value, graph_uri)
quads = [(s1, p1, o1, g1), (s2, p2, o2, g2), ...]
```

**Prohibited Format:**
```python
# INCORRECT: Dict format - DO NOT USE
quad = {'subject': s, 'predicate': p, 'object': o, 'graph': g}
```

**Implementation Requirements:**
1. **Dual-Write Coordinator**: See `endpoints/fuseki_psql_triples_endpoint_plan.md` for tuple format requirements
2. **Database Operations**: All `add_rdf_quads_batch`, `remove_rdf_quads_batch` methods use tuple format
3. **Space Implementations**: Consistent tuple handling in all space backend implementations
4. **Test Scripts**: All test data generation and validation uses tuple format
5. **Interface Compliance**: Follow `SpaceBackendInterface` tuple format specifications

**Affected Components:** See `endpoints/fuseki_psql_backend_plan.md` for complete component details

**Validation:**
- Triples endpoint - See `endpoints/fuseki_psql_triples_endpoint_plan.md` for format validation
- All endpoint tests verify proper tuple format usage
- No dict-based quad handling should exist in the codebase

This plan provides two robust backend options: pure multi-dataset Fuseki for maximum graph performance, and hybrid FUSEKI_POSTGRESQL for maximum reliability and disaster recovery capabilities. The hybrid approach leverages the substantial existing work (1,700+ lines of Fuseki implementation) while adding PostgreSQL-based backup and metadata management for enterprise-grade reliability.

## KGFrames Endpoint Enhancement Plan

### Overview
Enhancement of the existing KGFrames GET endpoint to support specific frame retrieval via `frame_uris` parameter, enabling comprehensive frame validation testing and improved frame management capabilities.

### Current Status
- ‚úÖ **KGEntityFrameCreateProcessor**: Successfully implemented and validated
- ‚úÖ **Frame Creation Tests**: All CREATE operations passing in comprehensive test suite
- ‚úÖ **Frame Creation Functionality**: 8 frame objects created with proper relationships
- üìã **Enhancement Required**: Add `frame_uris` parameter for specific frame retrieval

### Architectural Requirements

#### Endpoint Enhancement
**Target Endpoint**: `GET /api/graphs/kgentities/kgframes`
**New Parameter**: `frame_uris: Optional[List[str]]`
**Response Format**: N JsonLD documents (one per frame graph)
**Query Architecture**: Two-phase SPARQL approach for security and efficiency

#### API Signature Enhancement
```python
async def get_entity_frames(
    entity_uri: str,
    frame_uris: Optional[List[str]] = None,  # NEW PARAMETER
    # ... existing parameters
) -> Union[FramesResponse, Dict[str, JsonLdDocument]]  # Enhanced response type
```

### Implementation Plan

#### Phase E0A: Endpoint Parameter Enhancement
**Location**: KGEntities endpoint
**Changes**:
- Add `frame_uris` parameter to existing KGFrames GET method
- Maintain backward compatibility when parameter not provided
- Add input validation for frame URI format

#### Phase E0B: Phase 1 SPARQL Query - Frame Ownership Validation
**Purpose**: Validate that all requested frame URIs belong to the specified entity
**Security**: Prevent cross-entity frame access
**Query Logic**:
```sparql
SELECT ?frame_uri WHERE {
    GRAPH <{graph_uri}> {
        <{entity_uri}> <Edge_hasEntityKGFrame_predicate> ?edge .
        ?edge <edgeDestination> ?frame_uri .
        FILTER(?frame_uri IN (<frame_uri_1>, <frame_uri_2>, ...))
    }
}
```

**Validation Logic**:
- If any frame URI is NOT found in results ‚Üí Return 400 error
- If all frame URIs are validated ‚Üí Proceed to Phase 2
- Only applies to top-level frames (not sub-frames)

#### Phase E0C: Phase 2 SPARQL Queries - Frame Graph Retrieval
**Purpose**: Retrieve complete frame graphs using frameGraphURI grouping
**Approach**: One query per frame URI (N separate queries)
**Query Logic**:
```sparql
SELECT ?subject ?predicate ?object WHERE {
    GRAPH <{graph_uri}> {
        ?subject ?predicate ?object .
        ?subject <frameGraphURI> <{frame_uri}> .
    }
}
UNION
SELECT ?subject ?predicate ?object WHERE {
    GRAPH <{graph_uri}> {
        ?subject ?predicate ?object .
        ?subject <URI> <{frame_uri}> .
    }
}
```

**Per-Frame Processing**:
- Execute query for each validated frame URI
- Convert SPARQL results to VitalSigns GraphObjects
- Convert GraphObjects to JsonLD document
- Handle empty results (frame URI exists but no graph data)

#### Phase E0D: Response Structure
**Format**: Dictionary with frame URI as key, JsonLD document as value
**Structure**:
```python
{
    "frame_uri_1": JsonLdDocument(...),  # Complete frame graph
    "frame_uri_2": JsonLdDocument(...),  # Complete frame graph  
    "frame_uri_3": {},                   # Empty if no graph data
}
```

**Error Handling**:
- Cross-entity frame access ‚Üí 400 Bad Request
- Non-existent frame URI ‚Üí Empty JsonLD document in response
- SPARQL query errors ‚Üí 500 Internal Server Error

### Query Execution Flow

#### Step 1: Input Validation
```python
if frame_uris:
    # Validate frame URI format
    # Check frame_uris is not empty list
```

#### Step 2: Phase 1 Query - Ownership Validation
```python
ownership_query = build_frame_ownership_query(entity_uri, frame_uris)
ownership_results = await backend.execute_sparql_query(space_id, ownership_query)
validated_frame_uris = extract_validated_frames(ownership_results)

if len(validated_frame_uris) != len(frame_uris):
    # Some frames don't belong to entity
    raise HTTPException(400, "One or more frame URIs do not belong to the specified entity")
```

#### Step 3: Phase 2 Queries - Frame Graph Retrieval
```python
frame_graphs = {}
for frame_uri in validated_frame_uris:
    graph_query = build_frame_graph_query(frame_uri)
    graph_results = await backend.execute_sparql_query(space_id, graph_query)
    
    if graph_results:
        graph_objects = convert_sparql_to_vitalsigns(graph_results)
        frame_graphs[frame_uri] = convert_to_jsonld_document(graph_objects)
    else:
        frame_graphs[frame_uri] = {}  # Empty document
```

#### Step 4: Response Construction
```python
return FrameGraphsResponse(frame_graphs=frame_graphs)
```

### Test Integration Plan

#### Phase E1: Frame Graph Retrieval Test
**Method**: `test_frame_graph_retrieval()`
**Test Logic**:
```python
# Get frame URIs from creation result
frame_uris = [frame.URI for frame in creation_result.created_frames]

# Call enhanced endpoint
response = await self.endpoint.get_entity_frames(
    entity_uri=entity_uri,
    frame_uris=frame_uris
)

# Validate response structure
assert len(response.frame_graphs) == len(frame_uris)
for frame_uri in frame_uris:
    assert frame_uri in response.frame_graphs
    frame_graph = response.frame_graphs[frame_uri]
    # Validate frame graph contains expected objects
```

#### Phase E2: Entity Graph Retrieval Test
**Method**: `test_entity_graph_with_frames()`
**Approach**: Use existing entity retrieval with `include_entity_graph=True`

**Implementation**:
```python
# Call entity retrieval with full graph
response = await self.endpoint._get_entity_by_uri(
    space_id=space_id,
    graph_id=graph_id,
    uri=entity_uri,
    include_entity_graph=True,  # Get complete graph
    current_user=current_user
)
```

**Validation**:
- Entity object is present
- Frame objects are included in entity graph
- Edge_hasEntityKGFrame relationships connect entity to frames
- Complete frame structure (frames + slots + edges) is present
- Dual grouping URIs are correctly assigned

#### Enhanced Test Sequence
```
Phase 1.4: Frame Creation Tests  
‚îú‚îÄ‚îÄ 1. test_frame_creation_with_processor()
‚îú‚îÄ‚îÄ 2. test_dual_grouping_uri_assignment()
‚îú‚îÄ‚îÄ 3. test_edge_relationship_creation() 
‚îú‚îÄ‚îÄ 4. test_concrete_slot_types()
‚îú‚îÄ‚îÄ 5. test_frame_graph_retrieval() ‚Üê NEW (uses enhanced KGFrames endpoint)
‚îî‚îÄ‚îÄ 6. test_entity_graph_with_frames() ‚Üê NEW (uses include_entity_graph=True)
```

### Success Criteria

#### Functional Requirements
- ‚úÖ `frame_uris` parameter correctly filters frame retrieval
- ‚úÖ Phase 1 query prevents cross-entity frame access
- ‚úÖ Phase 2 queries retrieve complete frame graphs
- ‚úÖ Response structured as N JsonLD documents
- ‚úÖ Empty documents for non-existent frame data
- ‚úÖ Error responses for invalid frame ownership

#### Performance Considerations
- **Current**: N+1 queries (1 ownership + N frame graphs)
- **Future Optimization**: Could be reduced to fewer queries with SPARQL UNION
- **Acceptable**: For initial implementation, separate queries provide clarity

#### Security Requirements
- ‚úÖ Frame ownership validation prevents unauthorized access
- ‚úÖ Only top-level frames accessible (no sub-frame traversal)
- ‚úÖ Proper error messages without information leakage

### Implementation Sequence

1. **Phase E0A**: Add `frame_uris` parameter to endpoint
2. **Phase E0B**: Implement Phase 1 ownership validation query
3. **Phase E0C**: Implement Phase 2 frame graph retrieval queries  
4. **Phase E0D**: Structure response format and error handling
5. **Phase E1**: Add frame graph retrieval test case
6. **Phase E2**: Add entity graph retrieval test case
7. **Phase E3**: Integrate tests into comprehensive suite

### Expected Results

#### Phase E1 - Frame Graph Retrieval
```
KGFrames Response (with frame_uris parameter):
‚îú‚îÄ‚îÄ KGFrame objects (specified by frame_uris)
‚îú‚îÄ‚îÄ KGSlot objects (associated with frames)
‚îú‚îÄ‚îÄ Edge_hasKGSlot objects (frame ‚Üí slot relationships)
‚îî‚îÄ‚îÄ Complete frame graph structure
```

#### Phase E2 - Entity Graph Retrieval
```
Entity Graph Response (include_entity_graph=True):
‚îú‚îÄ‚îÄ KGEntity (main entity)
‚îú‚îÄ‚îÄ Edge_hasEntityKGFrame (entity ‚Üí frame relationships)
‚îú‚îÄ‚îÄ KGFrame objects (all frames linked to entity)
‚îú‚îÄ‚îÄ KGSlot objects (all slots in frame graphs)
‚îî‚îÄ‚îÄ Edge_hasKGSlot objects (frame ‚Üí slot relationships)
```

This enhancement provides secure, efficient, and well-structured frame graph retrieval with proper validation and comprehensive testing coverage, completing the frame creation validation infrastructure.

---

## üéâ MAJOR MILESTONE: Frame Deletion Issues Completely Resolved

### Summary of Critical Fixes Applied (January 7, 2026)

After extensive debugging and systematic problem-solving, all frame deletion issues have been successfully resolved, resulting in **100% test success rate** for the comprehensive KGFrames functionality.

### üîß Key Issues Identified and Fixed

#### Issue 1: SPARQL Syntax Error - Double Angle Brackets
**Problem**: Persistent SPARQL syntax error `" "<" "< ""` at various line positions preventing frame creation from completing successfully.

**Root Cause**: The Fuseki dataset manager's `_format_term` method was incorrectly treating all string values as URIs, wrapping string literals like `"Test Double Slot"` in angle brackets `<Test Double Slot>` instead of quotes.

**Solution Applied**:
```python
# Fixed in: /vitalgraph/db/fuseki_postgresql/fuseki_dataset_manager.py
elif isinstance(term, str):
    # Check if string looks like a URI, otherwise treat as literal
    if term.startswith(('http://', 'https://', 'urn:', 'file:')):
        # Clean URI formatting - strip any existing angle brackets
        clean_uri = term.strip('<>')
        return f"<{clean_uri}>"
    else:
        # Treat as string literal - escape quotes and wrap in quotes
        escaped_literal = term.replace('"', '\\"').replace('\n', '\\n').replace('\r', '\\r')
        return f'"{escaped_literal}"'
```

**Result**: SPARQL queries now generate correct syntax with proper distinction between URIs (angle brackets) and string literals (quotes).

#### Issue 2: Frame Ownership Validation Failure
**Problem**: Frame deletion processor could not find entity-frame relationships during ownership validation, returning empty results and preventing frame deletion.

**Root Cause**: The ownership validation SPARQL query used incorrect property URIs that didn't match VitalSigns' actual property mappings:
- **Expected**: `edgeSource` and `edgeDestination`  
- **Actual VitalSigns properties**: `hasEdgeSource` and `hasEdgeDestination`

**Debug Process**:
1. Created debug script to examine actual VitalSigns triple generation
2. Discovered VitalSigns uses `http://vital.ai/ontology/vital-core#hasEdgeSource` and `http://vital.ai/ontology/vital-core#hasEdgeDestination`
3. Updated ownership validation query to match actual property URIs

**Solution Applied**:
```python
# Fixed in: /vitalgraph/kg_impl/kgentity_frame_delete_impl.py
ownership_query = f"""
SELECT DISTINCT ?frame_uri WHERE {{
    GRAPH <{graph_id}> {{
        ?edge a <{self.haley_prefix}Edge_hasEntityKGFrame> .
        ?edge <{self.vital_prefix}hasEdgeSource> <{entity_uri}> .
        ?edge <{self.vital_prefix}hasEdgeDestination> ?frame_uri .
        FILTER(?frame_uri IN ({frame_uris_filter}))
    }}
}}
"""
```

**Result**: Frame ownership validation now successfully finds entity-frame relationships, enabling proper frame deletion functionality.

### üéØ Final Test Results - Complete Success

**All 9 comprehensive frame tests now passing (100% success rate)**:
```
‚úÖ KGEntities comprehensive tests completed successfully!
üìä Complete CRUD Cycle Results:
   - Space creation: ‚úÖ Success
   - Entity creation (CREATE): ‚úÖ Success
   - Entity retrieval (READ): ‚úÖ Success
   - Frame creation (CREATE): ‚úÖ Success
   - Entity updates (UPDATE): ‚úÖ Success
   - Entity deletion (DELETE): ‚úÖ Success
   - Space cleanup: ‚úÖ Success
üéØ Full CRUD cycle with enhanced KGFrames endpoint validated!
```

### üèÜ Technical Achievements

**1. SPARQL Generation Robustness**
- Proper URI vs literal distinction in all SPARQL queries
- Correct N-Triples formatting for dual-write backend
- Elimination of syntax errors in complex frame graph operations

**2. VitalSigns Integration Accuracy**
- Correct property URI mappings for all edge relationships
- Proper handling of `Edge_hasEntityKGFrame` objects
- Accurate SPARQL query generation matching VitalSigns ontology

**3. Frame Deletion Functionality**
- Two-phase SPARQL architecture (ownership validation + deletion)
- Security validation preventing cross-entity frame access
- Complete frame graph deletion using `frameGraphURI` grouping
- Proper cleanup of entity-frame relationships

**4. Dual-Write Backend Consistency**
- Successful frame storage in both PostgreSQL and Fuseki
- Consistent query results across both storage systems
- Proper transaction management and error handling

### üîÑ Architectural Impact

The successful resolution of these issues completes the KGFrames DELETE functionality, which enables:

1. **Full CRUD Operations**: Complete Create, Read, Update, Delete cycle for frames
2. **UPDATE/UPSERT Support**: Frame update operations can now properly delete existing frames before creating new ones
3. **Production Readiness**: All frame operations are now stable and fully tested
4. **Security Compliance**: Proper ownership validation prevents unauthorized frame access

### üìã Next Phase Readiness

With frame deletion fully operational, the system is now ready for:
- **Enhanced Frame Operations**: UPDATE and UPSERT test cases
- **Advanced Frame Workflows**: Complex frame manipulation scenarios
- **Production Deployment**: Stable frame management capabilities
- **Performance Optimization**: Focus on query efficiency and scalability

This milestone represents a critical breakthrough in the VitalGraph frame management system, providing a solid foundation for advanced knowledge graph operations.

---

## üîß Phase G: Frame UPDATE/UPSERT Implementation Plan

### Overview: PostgreSQL-First Atomic Operations

Based on investigation of existing KGEntity UPDATE patterns, the frame UPDATE/UPSERT implementation will use a **PostgreSQL-first dual-write coordination** approach with a new `update_quads` function that handles atomic delete+insert operations.

### üéØ Key Architecture Decision: Simplified Transaction Management

**Critical Insight**: PostgreSQL transaction provides the atomicity guarantee. Once PostgreSQL transaction commits successfully, Fuseki operations can be performed separately without complex transaction coordination.

**Benefits**:
- **Single Transaction Scope**: Only PostgreSQL needs transaction management
- **No Dual Coordination**: No complex two-phase commit protocols  
- **Clear Failure Modes**: PostgreSQL failure = rollback, Fuseki failure = sync issue
- **PostgreSQL as Source of Truth**: Transaction guarantees consistency in primary store
- **Fuseki as Derived State**: Can be rebuilt from PostgreSQL if sync fails

### üîß Phase G2C: Atomic `update_quads` Function Implementation

#### Core Function Signature
```python
async def update_quads(self, space_id: str, graph_id: str, 
                      delete_quads: List[Tuple], insert_quads: List[Tuple]) -> bool:
    """
    Atomically update quads by deleting old ones and inserting new ones.
    
    Implementation Strategy:
    1. Execute DELETE and INSERT within single PostgreSQL transaction
    2. Once PostgreSQL commits, synchronize Fuseki separately
    3. PostgreSQL transaction provides atomicity guarantee
    
    Args:
        space_id: Space identifier
        graph_id: Graph identifier (full URI)
        delete_quads: List of (subject, predicate, object, graph) tuples to delete
        insert_quads: List of (subject, predicate, object, graph) tuples to insert
        
    Returns:
        bool: True if operation succeeded, False otherwise
    """
```

#### PostgreSQL Implementation (Atomic Core)
```python
async def update_quads(self, space_id: str, graph_id: str, 
                      delete_quads: List[Tuple], insert_quads: List[Tuple]) -> bool:
    """PostgreSQL implementation with transaction-secured atomic operations."""
    
    async with self.connection_pool.acquire() as conn:
        async with conn.transaction():  # This provides atomicity guarantee
            try:
                # Step 1: Delete old quads within transaction
                if delete_quads:
                    await self._delete_quads_in_transaction(conn, space_id, delete_quads)
                
                # Step 2: Insert new quads within same transaction  
                if insert_quads:
                    await self._insert_quads_in_transaction(conn, space_id, insert_quads)
                
                # Transaction commits here - PostgreSQL state is now consistent
                
            except Exception as e:
                # Transaction rolls back automatically
                raise
    
    # Step 3: Update Fuseki separately (PostgreSQL is already consistent)
    try:
        await self._sync_fuseki_operations(space_id, graph_id, delete_quads, insert_quads)
        return True
    except Exception as e:
        # PostgreSQL is consistent, Fuseki sync failed
        # This is a sync issue, not a data consistency issue
        return True  # PostgreSQL succeeded, which is our source of truth
```

#### Separate Fuseki Synchronization
```python
async def _sync_fuseki_operations(self, space_id: str, graph_id: str,
                                 delete_quads: List[Tuple], insert_quads: List[Tuple]):
    """
    Synchronize Fuseki with PostgreSQL changes using separate operations.
    No transaction coordination needed since PostgreSQL is already committed.
    """
    
    # Delete operations first
    if delete_quads:
        for subject, predicate, obj, graph in delete_quads:
            delete_query = f"""
            DELETE DATA {{
                GRAPH <{graph_id}> {{
                    <{subject}> <{predicate}> {self._format_object(obj)} .
                }}
            }}
            """
            await self.execute_sparql_update(space_id, delete_query)
    
    # Insert operations second  
    if insert_quads:
        for subject, predicate, obj, graph in insert_quads:
            insert_query = f"""
            INSERT DATA {{
                GRAPH <{graph_id}> {{
                    <{subject}> <{predicate}> {self._format_object(obj)} .
                }}
            }}
            """
            await self.execute_sparql_update(space_id, insert_query)
```

### üéØ Phase G3: Frame UPDATE/UPSERT Implementation

#### UPDATE Mode Implementation
```python
async def update_frames_atomic(self, space_id: str, graph_id: str, 
                              entity_uri: str, frame_objects: List[GraphObject]) -> FrameCreateResponse:
    """UPDATE frames using PostgreSQL-first atomic update_quads."""
    
    # Step 1: Validate frame ownership (prevent unauthorized updates)
    frame_uris = self._extract_frame_uris(frame_objects)
    delete_processor = KGEntityFrameDeleteProcessor(self.backend, self.logger)
    validated_uris = await delete_processor.validate_frame_ownership(
        space_id, graph_id, entity_uri, frame_uris
    )
    
    if len(validated_uris) != len(frame_uris):
        missing_frames = set(frame_uris) - set(validated_uris)
        raise HTTPException(status_code=404, detail=f"UPDATE failed: Frames not found: {missing_frames}")
    
    # Step 2: Build quad sets for atomic replacement
    delete_quads = await self._build_frame_delete_quads(space_id, graph_id, frame_uris)
    insert_quads = self._build_frame_insert_quads(graph_id, frame_objects)
    
    # Step 3: Execute atomic update (PostgreSQL transaction + Fuseki sync)
    success = await self.backend.update_quads(space_id, graph_id, delete_quads, insert_quads)
    
    if not success:
        raise HTTPException(status_code=500, detail="UPDATE failed: Atomic frame replacement failed")
    
    return FrameCreateResponse(
        success=True,
        created_uris=frame_uris,
        message=f"Successfully updated {len(frame_uris)} frames atomically"
    )
```

#### UPSERT Mode Implementation
```python
async def upsert_frames_atomic(self, space_id: str, graph_id: str,
                              entity_uri: str, frame_objects: List[GraphObject]) -> FrameCreateResponse:
    """UPSERT frames using conditional atomic update_quads function."""
    
    # Step 1: Extract frame URIs
    frame_uris = self._extract_frame_uris(frame_objects)
    
    # Step 2: Identify existing frames (no failure for missing ones)
    delete_processor = KGEntityFrameDeleteProcessor(self.backend, self.logger)
    existing_frame_uris = await delete_processor.validate_frame_ownership(
        space_id, graph_id, entity_uri, frame_uris
    )
    
    # Step 3: Build delete quads (only for existing frames)
    delete_quads = []
    if existing_frame_uris:
        delete_quads = await self._build_frame_delete_quads(space_id, graph_id, existing_frame_uris)
    
    # Step 4: Build insert quads (for all frames - existing and new)
    insert_quads = self._build_frame_insert_quads(graph_id, frame_objects)
    
    # Step 5: Execute atomic update
    success = await self.backend.update_quads(space_id, graph_id, delete_quads, insert_quads)
    
    if not success:
        raise HTTPException(status_code=500, detail="UPSERT failed: Atomic frame operation failed")
    
    return FrameCreateResponse(
        success=True,
        created_uris=frame_uris,
        message=f"Successfully upserted {len(frame_uris)} frames atomically"
    )
```

### üîß Helper Functions for Quad Building

#### Frame Delete Quads Builder
```python
async def _build_frame_delete_quads(self, space_id: str, graph_id: str, 
                                   frame_uris: List[str]) -> List[Tuple]:
    """Build delete quads for complete frame graphs using frameGraphURI grouping."""
    delete_quads = []
    
    for frame_uri in frame_uris:
        # Query to find all subjects belonging to this frame graph
        query = f"""
        SELECT ?s ?p ?o WHERE {{
            GRAPH <{graph_id}> {{
                ?s <http://vital.ai/ontology/haley-ai-kg#frameGraphURI> <{frame_uri}> .
                ?s ?p ?o .
            }}
        }}
        """
        
        results = await self.backend.execute_sparql_query(space_id, query)
        
        for result in results:
            subject = result['s']['value']
            predicate = result['p']['value'] 
            obj = result['o']['value']
            delete_quads.append((subject, predicate, obj, graph_id))
    
    return delete_quads
```

#### Frame Insert Quads Builder
```python
def _build_frame_insert_quads(self, graph_id: str, frame_objects: List[GraphObject]) -> List[Tuple]:
    """Build insert quads from VitalSigns frame objects."""
    insert_quads = []
    
    # Convert GraphObjects to RDF triples - See `endpoints/fuseki_psql_triples_endpoint_plan.md`
    triples = GraphObject.to_triples_list(frame_objects)
    
    for subject, predicate, obj in triples:
        insert_quads.append((str(subject), str(predicate), str(obj), graph_id))
    
    return insert_quads
```

### üìã Implementation Phases

#### Phase G2C: Backend Implementation
- **Status**: üîÑ IN PROGRESS
- **Tasks**:
  - Implement `update_quads` function in FusekiPostgreSQLBackendAdapter
  - Add `_delete_quads_in_transaction` and `_insert_quads_in_transaction` methods
  - Implement `_sync_fuseki_operations` with retry logic
  - Add comprehensive error handling and logging

#### Phase G3A: UPDATE Mode Enhancement
- **Status**: üìã PLANNED
- **Tasks**:
  - Enhance KGEntityFrameCreateProcessor to support UPDATE mode
  - Add frame ownership validation for UPDATE operations
  - Implement atomic frame replacement using `update_quads`
  - Add UPDATE-specific error handling and validation

#### Phase G3B: UPSERT Mode Enhancement  
- **Status**: üìã PLANNED
- **Tasks**:
  - Enhance KGEntityFrameCreateProcessor to support UPSERT mode
  - Implement conditional frame deletion for existing frames only
  - Add UPSERT-specific logic for mixed create/update scenarios
  - Ensure proper response messaging for UPSERT operations

#### Phase G4: Comprehensive Testing
- **Status**: üìã PLANNED
- **Tasks**:
  - Create UPDATE test cases with frame ownership validation
  - Create UPSERT test cases with mixed existing/new frame scenarios
  - Add atomic operation validation tests
  - Add transaction rollback testing
  - Add Fuseki sync failure recovery testing

#### Phase G5: Integration and Validation
- **Status**: üìã PLANNED
- **Tasks**:
  - Integrate UPDATE/UPSERT tests into comprehensive test suite
  - Validate complete atomic frame lifecycle (CREATE ‚Üí UPDATE ‚Üí DELETE)
  - Performance testing for large frame operations
  - End-to-end validation of dual-write consistency

### üéØ Expected Benefits

#### Atomic Frame Operations
- **Complete Replacement**: Frames are atomically replaced without intermediate states
- **Rollback Safety**: Any failure rolls back the entire operation automatically
- **Consistency Guarantee**: PostgreSQL transaction ensures data consistency

#### Performance Optimization
- **Reduced Network Calls**: One `update_quads` call instead of separate delete/insert
- **Better Concurrency**: Shorter transaction windows reduce lock contention
- **Efficient Operations**: Batch processing of quad operations

#### Architectural Consistency
- **Clean Interface**: Simple function signature without transaction parameters
- **Reusable Pattern**: Can be used for entity updates and other atomic operations
- **Backend Agnostic**: Works with any backend that implements `update_quads`

This approach provides true atomicity at the database level while maintaining a clean, simple interface for frame UPDATE and UPSERT operations, leveraging the proven PostgreSQL-first dual-write coordination pattern already established in the VitalGraph architecture.

### üß™ Phase G2D: Dedicated Testing Infrastructure

#### Test Module: `test_update_quads.py`
**Location**: `/Users/hadfield/Local/vital-git/vital-graph/test_scripts/fuseki_postgresql/test_update_quads.py`

**Purpose**: Comprehensive validation of the atomic `update_quads` functionality with focus on:
- PostgreSQL transaction management and rollback scenarios
- Fuseki synchronization and failure recovery
- Atomic operation validation (no intermediate states)
- Performance benchmarking for large quad operations

**Test Cases**:
```python
async def test_basic_update_quads():
    """Test basic atomic update with simple quad replacement."""
    
async def test_update_quads_transaction_rollback():
    """Test PostgreSQL transaction rollback on failure."""
    
async def test_update_quads_fuseki_sync_failure():
    """Test Fuseki sync failure handling after PostgreSQL success."""
    
async def test_update_quads_large_operations():
    """Test performance with large quad sets (1000+ quads)."""
    
async def test_update_quads_empty_sets():
    """Test edge cases with empty delete/insert sets."""
    
async def test_update_quads_concurrent_operations():
    """Test concurrent update_quads operations for race conditions."""
```

#### Integration with Existing Test Suite
The `test_update_quads.py` module will be integrated into the comprehensive test orchestrator as **Phase 1.6: Atomic Operations Validation**, positioned after frame deletion tests and before any UPDATE/UPSERT frame operations.

### üîÑ Phase H: Legacy UPDATE Implementation Revision

#### Critical Architectural Impact
The introduction of atomic `update_quads` functionality requires revision of existing UPDATE implementations that currently use separate delete/insert operations. This ensures consistency across the entire VitalGraph architecture.

#### Phase H1: KGEntity UPDATE Revision
**Current Issue**: `KGEntityUpdateProcessor.update_entity()` uses high-level `delete_object()` followed by `store_objects()`, which lacks true atomicity.

**Required Changes**:
```python
# Current implementation (lines 55-68 in kgentity_update_impl.py)
delete_result = await backend.delete_object(space_id, graph_id, entity_uri)
store_result = await backend.store_objects(space_id, graph_id, updated_objects)

# New atomic implementation using update_quads
delete_quads = await self._build_entity_delete_quads(space_id, graph_id, entity_uri)
insert_quads = self._build_entity_insert_quads(graph_id, updated_objects)
success = await backend.update_quads(space_id, graph_id, delete_quads, insert_quads)
```

**Benefits**:
- True atomicity for entity updates
- Consistent transaction management across all UPDATE operations
- Better performance through batch quad operations
- Unified error handling and rollback behavior

#### Phase H2: KGTypes UPDATE Revision
**Current Issue**: KGTypes UPDATE operations for GraphObject properties use separate quad removal and insertion, lacking atomicity.

**Required Changes**:
- Identify all KGTypes UPDATE methods that modify GraphObject properties
- Replace separate `remove_rdf_quads_batch()` and `add_rdf_quads_batch()` calls with atomic `update_quads()`
- Ensure consistent transaction behavior across KGTypes operations

**Files to Modify**:
- `vitalgraph/endpoint/impl/kgtype_impl.py` - UPDATE methods
- Any KGTypes processors that handle GraphObject updates

### üìã Updated Implementation Phases

#### Phase G2C: Backend Implementation (COMPLETED)
- **Status**: ‚úÖ COMPLETED
- **Tasks**:
  - ‚úÖ Implement `update_quads` function in FusekiPostgreSQLBackendAdapter
  - ‚úÖ Add `_delete_quads_in_transaction` and `_insert_quads_in_transaction` methods
  - ‚úÖ Implement `_sync_fuseki_operations` with retry logic
  - ‚úÖ Add comprehensive error handling and logging

#### Phase G2D: Dedicated Testing Infrastructure (COMPLETED)
- **Status**: ‚úÖ COMPLETED
- **Tasks**:
  - ‚úÖ Create `test_update_quads.py` test module in `/test_scripts/fuseki_postgresql/`
  - ‚úÖ Implement comprehensive test cases for atomic operations (6/6 tests passing)
  - ‚úÖ Add transaction rollback and Fuseki sync failure testing
  - ‚úÖ Integrate into comprehensive test orchestrator as Phase 1.6
  - ‚úÖ Performance benchmarking for large quad operations (300 quads tested)

#### Phase G2E: VitalSigns Property Access Issues (COMPLETED)
- **Status**: ‚úÖ COMPLETED
- **Tasks**:
  - ‚úÖ Fix VitalSigns `CombinedProperty` casting issues in SPARQL conversion layer
  - ‚úÖ Replace all `.startswith()` calls with proper URI validation using `validate_rfc3986()`
  - ‚úÖ Cast all SPARQL binding values to strings before RDFLib processing
  - ‚úÖ Achieve 100% test success rate for atomic `update_quads` functionality

#### Phase G3A: Frame UPDATE Mode Enhancement
- **Status**: üìã PLANNED
- **Dependencies**: G2C, G2D
- **Tasks**:
  - Enhance KGEntityFrameCreateProcessor to support UPDATE mode
  - Add frame ownership validation for UPDATE operations
  - Implement atomic frame replacement using `update_quads`
  - Add UPDATE-specific error handling and validation

#### Phase G3B: Frame UPSERT Mode Enhancement  
- **Status**: üìã PLANNED
- **Dependencies**: G3A
- **Tasks**:
  - Enhance KGEntityFrameCreateProcessor to support UPSERT mode
  - Implement conditional frame deletion for existing frames only
  - Add UPSERT-specific logic for mixed create/update scenarios
  - Ensure proper response messaging for UPSERT operations

#### Phase G4: Frame UPDATE/UPSERT Testing
- **Status**: üìã PLANNED
- **Dependencies**: G3A, G3B
- **Tasks**:
  - Create UPDATE test cases with frame ownership validation
  - Create UPSERT test cases with mixed existing/new frame scenarios
  - Add atomic operation validation tests
  - Add transaction rollback testing for frame operations
  - Add Fuseki sync failure recovery testing for frames

#### Phase G5: Frame Integration and Validation
- **Status**: üìã PLANNED
- **Dependencies**: G4
- **Tasks**:
  - Integrate UPDATE/UPSERT tests into comprehensive test suite
  - Validate complete atomic frame lifecycle (CREATE ‚Üí UPDATE ‚Üí DELETE)
  - Performance testing for large frame operations
  - End-to-end validation of dual-write consistency

#### Phase H1: KGEntity UPDATE Revision (NEW)
- **Status**: üìã PLANNED
- **Dependencies**: G2C, G2D
- **Tasks**:
  - Revise `KGEntityUpdateProcessor.update_entity()` to use `update_quads`
  - Replace high-level delete/insert with atomic quad operations
  - Add helper methods for entity quad building
  - Update existing entity UPDATE tests to validate atomicity
  - Performance comparison between old and new implementations
