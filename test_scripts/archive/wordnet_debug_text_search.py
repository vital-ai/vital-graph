#!/usr/bin/env python3
"""
Debug the actual SQL generated by SPARQL text search queries.
Enable SQL logging to see exactly what queries are being executed.
"""

import os
import time
import logging
from rdflib import Graph, URIRef
from sqlalchemy import URL, create_engine, text
from vitalgraph.store.store import VitalGraphSQLStore

# Database configuration
PG_HOST = os.getenv("PG_HOST", "127.0.0.1")
PG_PORT = os.getenv("PG_PORT", "5432")
PG_DATABASE = os.getenv("PG_DATABASE", "vitalgraphdb")
PG_USER = os.getenv("PG_USER", "postgres")
PG_PASSWORD = os.getenv("PG_PASSWORD", "")
GRAPH_NAME = "wordnet"

def setup_sql_logging():
    """Enable detailed SQL logging to see what queries are generated."""
    # Enable SQLAlchemy logging
    logging.basicConfig()
    logger = logging.getLogger('sqlalchemy.engine')
    logger.setLevel(logging.INFO)
    
    # Also enable PostgreSQL query logging if possible
    print("SQL logging enabled - you'll see the actual queries being executed")

def test_with_sql_logging(g, query_name, sparql_query, max_time=60):
    """Test a SPARQL query with full SQL logging enabled."""
    print(f"\n{'='*80}")
    print(f"DEBUGGING: {query_name}")
    print(f"{'='*80}")
    
    print("SPARQL Query:")
    print(sparql_query)
    print("\nSQL queries will be logged below:")
    print("-" * 40)
    
    start_time = time.time()
    
    try:
        results = g.query(sparql_query)
        count = 0
        
        for row in results:
            count += 1
            if count >= 3:  # Stop after 3 results to see the pattern
                break
            
            # Check if we're taking too long
            elapsed = time.time() - start_time
            if elapsed > max_time:
                print(f"\n⚠ STOPPING after {elapsed:.1f} seconds (max_time={max_time})")
                break
        
        elapsed = time.time() - start_time
        print(f"\n{'-'*40}")
        print(f"RESULT: {count} results in {elapsed:.3f} seconds")
        
        if elapsed > 30:
            print("✗ VERY SLOW - Check the SQL queries above for inefficiencies")
        elif elapsed > 10:
            print("⚠ SLOW - Could be optimized")
        else:
            print("✓ GOOD - Reasonable performance")
            
        return elapsed, count
        
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"\n{'-'*40}")
        print(f"ERROR after {elapsed:.3f} seconds: {e}")
        return elapsed, 0

def analyze_table_structure(engine, interned_id):
    """Analyze the table structure and indexes to understand the query patterns."""
    print(f"\n{'='*80}")
    print("TABLE STRUCTURE ANALYSIS")
    print(f"{'='*80}")
    
    tables = [
        f"{interned_id}_asserted_statements",
        f"{interned_id}_literal_statements", 
        f"{interned_id}_type_statements"
    ]
    
    with engine.connect() as connection:
        for table in tables:
            print(f"\n--- {table} ---")
            
            # Get row count
            result = connection.execute(text(f"SELECT COUNT(*) FROM {table}"))
            count = result.scalar()
            print(f"Rows: {count:,}")
            
            # Get sample data to understand structure
            result = connection.execute(text(f"""
                SELECT subject, predicate, object 
                FROM {table} 
                WHERE object IS NOT NULL 
                LIMIT 3
            """))
            
            print("Sample data:")
            for row in result:
                subject = str(row[0])[:50] + "..." if len(str(row[0])) > 50 else str(row[0])
                predicate = str(row[1])[:50] + "..." if len(str(row[1])) > 50 else str(row[1])
                obj = str(row[2])[:50] + "..." if len(str(row[2])) > 50 else str(row[2])
                print(f"  {subject} | {predicate} | {obj}")

def main():
    # Build database URL
    db_url = URL.create(
        drivername="postgresql+psycopg",
        username=PG_USER,
        password=PG_PASSWORD or None,
        host=PG_HOST,
        port=PG_PORT,
        database=PG_DATABASE,
    )

    # Enable SQL logging
    setup_sql_logging()
    
    store = VitalGraphSQLStore()
    graph_iri = URIRef(f"http://vital.ai/graph/{GRAPH_NAME}")
    g = Graph(store=store, identifier=graph_iri)
    g.open(db_url)
    
    print("Connected to WordNet graph")
    
    # Analyze table structure first
    engine = create_engine(db_url)
    interned_id = store._interned_id
    analyze_table_structure(engine, interned_id)
    
    # Test a simple query first
    simple_query = """
PREFIX vital-core: <http://vital.ai/ontology/vital-core#>
PREFIX haley-ai-kg: <http://vital.ai/ontology/haley-ai-kg#>

SELECT ?entity ?entityName WHERE {
  ?entity a haley-ai-kg:KGEntity .
  ?entity vital-core:hasName ?entityName .
}
LIMIT 3
"""
    
    test_with_sql_logging(g, "Simple query (no text search)", simple_query, max_time=30)
    
    # Now test the problematic text search query
    text_search_query = """
PREFIX vital-core: <http://vital.ai/ontology/vital-core#>
PREFIX haley-ai-kg: <http://vital.ai/ontology/haley-ai-kg#>

SELECT ?entity ?entityName WHERE {
  ?entity a haley-ai-kg:KGEntity .
  ?entity vital-core:hasName ?entityName .
  FILTER(CONTAINS(LCASE(STR(?entityName)), "happy"))
}
LIMIT 3
"""
    
    test_with_sql_logging(g, "Text search: CONTAINS 'happy'", text_search_query, max_time=60)
    
    print(f"\n{'='*80}")
    print("DEBUGGING COMPLETE")
    print(f"{'='*80}")
    print("\nLook at the SQL queries above to identify:")
    print("1. Are the queries using JOINs across multiple tables?")
    print("2. Are text search operations using ILIKE, LIKE, or other operators?")
    print("3. Are there any sequential scans instead of index scans?")
    print("4. Are the queries overly complex for simple text search?")

if __name__ == "__main__":
    main()
